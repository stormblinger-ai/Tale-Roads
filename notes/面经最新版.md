#                                          面经最新版







# 1，自我介绍

优势，亮点，具体情况

和应聘岗位相关经历

为什么你可以胜任该工作

求职动机

学校和专业，满足岗位要求的三个优势和潜力或亮点，表现出自己对岗位的理解和职业规划

面试官你好，我叫，，，来自，，，毕业于，，，这一次我应聘的是，，，从事相关工作一直是我，为此我很早就结合岗位的要求进行了准备，包括对，，，熟悉，在几年的时间里，我也是，，，，希望能够加入公司从事相关工作

# 2，技能介绍

编程语言，编程工具（软硬件），操作系统，计算机网络，数据库，数据结构与算法，软件工程

 

# 3，项目

包括你们公司的主要业务是什么，你在上一家公司主要是做什么内容，你在这家公司都做过哪些项目，这些项目都使用了什么技术，在最近的项目中，你是担任什么角色，核心开发还是辅助开发 ，你最近做的这个项目上还有哪些技术上的一些优化点，项目的压测方案

1. LDAP集成，用户模块crud

2. 企查查接口

3. 主管应用

4. <font color = 'blue'>系统需求设计</font>

5. LDAP集成

   1，LDAP是什么

   LDAP是一种网络协议，主要是用来集中管理企业的组织结构和人员账号信息的系统，除了标准的LDAP服务器外，还有Windows服务器提供的AD域；LDAP是轻量化目录访问协议，LDAP目录服务在TCP/IP上的实现（RFC1777 v2版和 RFC2251 V3版），LDAP是一种特殊的数据库系统，专门针对读取，浏览和搜索操作进行了特定的优化。

   LDAP的信息是以树型结构存储的，具体存储在条目entry的数据结构中，条目相当于关系数据库中表的记录；条目是具有区别名DN的属性attribute，DN是用来引用条目的，DN相当于关系数据库表中的关键字primary key，属性由类型type和一个或多个值values组成，相当于关系数据库中的字段field由字段名和数据类型组成，只是为了方便检索的需要，LDAP中的type可以由多个value；

   LDAP优点：

   -优异的数据读取性能

   -跨平台及语言无关性

   -灵活的推拉  数据复制特性

   -安全，细致的权限控制

   LDAP用于AD，微软活动目录，Windows2000操作系统的中心组件

   LDAPtemplate代码示例

   1，初始化配置

   2，新增adduser

   获取用户传来的属性集

   把新条目绑定到LDAP

   参数说明

   3，修改updateuser

   获取用户传来的属性集

   绑定

   4，删除

   删除，dn后为TRUE表示同时删除器孩子结点

   5，主键查询

   6，数量统计

   7，分页查询及排序

   常见问题

   1，suncertpathbuilderexception异常

   安装插件报错，因为LDAPtemplate采用LDAPcontext的方式对LDAP服务器进行访问，而服务器采用SSL协议，JRE未导入SSL相应证书

   2，LDAPtemplate读取的条目属性不完整

   出现该问题一般是因为LDAP服务器需要进行授权读取，确认net.sf.ldaptemplate.support.ldapcontextsource的authenticatedReadonly属性设置为true

   3，LDAP手动初始化，出现NULLException错误

    

   2，服务器webservice接口方法

   Pushuser方法

   推送人员信息，接入应用系统接收到后，解析人员信息数据，做相应的操作

   \* 推送人员映射信息

   *appid接入应用标识

   *dataStr推送人员映射数据

   *dataType人员映射数据类型-XML或json

   ****\*@return\*******\*-返回操作结果\****

    SoapUI调用示例

   ```
   XML
   <weav:addUser>
   <weav:in0>test4</weav:in0>
   <weav:in1>
   <?xml version="1.0" encoding="UTF-8"?>
   <root>
   ​	<hrmmaplist>
   ​		<hrmmap>
   ​			<loginid>zixun002</loginid>
   ​			<apploginid>zixun002t</apploginid>
   ​		</hrmmap>
   ​		<hrmmap>
   ​			<loginid>zixun004</loginid>
   ​			<apploginid>zixun004t</apploginid>
   ​		</hrmmap>
   ​	</hrmmaplist>
   </root>  
   </weav:in1>
   <weav:in2>XML</weav:in2>
   </weav:addUser>
   
   Json
   <weav:pushUser>
   <weav:in0>test4</weav:in0>
   <weav:in1>
   [{"loginid":"zixun002","lastname":"紫熏002","apploginid":"zixun002tt"},
   {"loginid":"zixun004","lastname":"紫熏004","apploginid":"zixun004tt"}]
   </weav:in1>
   <weav:in2>JSON</weav:in2>
   </weav:pushUser>
```
   
rest接口
   
| 功能说明   | 推送人员信息，接入应用系统接收到后，解析人员信息数据，做相应的操作。 |
   | ---------- | ------------------------------------------------------------ |
| 调用方式   | POST                                                         |
   |            |                                                              |
| 参数说明   |                                                              |
   | 参数名     | 说明                                                         |
| appid      | 接入应用标识                                                 |
   | dataStr    | 推送人员映射信息数据                                         |
| dataType   | 人员映射信息数据类型XML或JSON                                |
   |            |                                                              |
| 返回值说明 |                                                              |
   | 返回值说明 | 操作结果                                                     |

    

    2,系统中需要添加根据企业名称或统一信用代码查询企业信息的功能，所以整合了企查查的查询接口\****

   3，合肥兆尹资产管理系统

   1）案件分类：我们会基于案件的逾期金额、案件的逾期天数、用户历史逾期情况等数据，套用计算模型，得出案件的综合评分和评级，以此作为案件分配的依据。

   2）案件分配：不同的案件类别，对应不同的分配策略（比如低风险的案件先进入自动审核阶段，而中风险的案件直接进入人工审核阶段）；其中分配到人工催收阶段的案件，还需要考虑每个人手上跟进的在逾的案件情况，看是按照案件量平均分配，还是按照案件在逾金额平均分配，或者以其他量作为参考。

   3）案件跟进：这个主要指的是人工催收阶段的跟进过程，一般需要给催员提供案件标记、催收记录、联系在逾用户（发送短信等）等功能。

   2）外部数据接入：需要同步用户订单数据和信审数据到催收系统，并且处理成催收系统需要的数据，为催员的案件分析提供数据支持。

   1）催收内部系统：这个是指催收系统本身需要建设的功能，主要分为主管监控查询、催员任务处理、委外管理、核销管理、报表管理几大块；其中委外管理和核销管理因为所在公司业务没有涉及，所以我不另外说明，放在这里只是为了需求点的完整性；主管监控查询里面涉及到案件查询、分配、审核等功能；催员的任务处理里面涉及到的案件件查询、标识、记录反馈、联系在逾用户等功能；报表管理主要分为两大部分，一部分是催收业务数据报表的统计，另外一部分是催员绩效的统计。
4）成员管理：需要事先在成员管理模块配置好催员的角色，然后催收系统案件分配给催员的时候，催员选择模块直接调用成员管理配置好的角色。
   
   5）沟通平台：在催收跟进过程中，比如自动催收阶段，则直接调用外呼系统进行催收，而在人工催收阶段，则催员会用到发短信、邮件等方式，此时则涉及到和短信平台以及邮件平台的对接。
   
   6）客服系统：催收的部分数据需要同步到客服系统，方便客服快速了解到案件当前的催收情况，以此应对客诉问题。
   
   4，企查查申请接口
   
   -企业工商详情，套餐2元/次。开通企业户 1元/次。按周期的话，比如1000家公司，一年30个周期，只需要3万一年；
   
   -企业api接口批量操作
   
   1，了解需求，申请接口，在我的接口里面有我的key和我的秘钥（注册账号，短信验证码获取）

   2，了解请求参数以及返回数据的格式

   -接口信息

   接口名：企业工商详情  方法名： GetInfo

   接口地址：

   支持格式：json/xml

   请求方式：get

   权限验证：构造请求头，两个字段，token和 timespan

   参数名	类型	传递方式		说明

   Token	string	http header头 	验证加密值（key + timespan + secretkey组成的32位md5加密的大写字符串）
   
Timespan	 string	httpheader 头	精确到秒的Unix时间戳
   
Secretkey	 string 	不传递			秘钥，请妥善保管好
   
 
   
请求参数：
   
名称		类型	是否必填		描述
   
Key			string 	是				应用appkey（应用详情页查询）
   
searchKey		string	是				搜索关键字
   
返回参数：18个参数
   
result（44），partners（14）投资人，employees（3）人员，branches（6）分支机构, changerecords（4）, contactinfo（3）, industry（8）, area（3）, revokeinfo（4）, emergingindustylist（3）, taglist（2）, ARcontactlsit(3), originalname（2）, relatedproduct（4）, relatedorg（2）, website（2）,
   
secondarylist（3）, tertiarylist（2）,  
   
18个参数有json，有string，字段共112个
   
请求状态码：有效状态码，13个有效请求状态码，22个无效请求状态码
   
8.1 有效请求状态码 
   
 200 【有效请求】查询成功 
   
201 【有效请求】查询无结果 
   
202 【有效请求】查询参数错误，请检查 
   
205 【有效请求】等待处理中 
   
207 【有效请求】请求数据的条目数超过上限（5000） 
   
208 【有效请求】此接口不支持此公司类型查询 
   
209 【有效请求】企业数量超过上限 
   
213 【有效请求】参数长度不能小于 2 
   
215 【有效请求】不支持的查询关键字 
   
218 【有效请求】该企业暂不支持空壳扫描 
   
219 【有效请求】该企业暂不支持开户尽调 
   
105 【有效请求】接口已下线停用 
   
110 【有效请求】当前相同查询连续出错，请等 2 小时后 重试 
   
8.2 无效请求状态码  
   
101 【无效请求】当前的 KEY 无效或者还未生效中 
   
102 【无效请求】当前 KEY 已欠费 
   
103 【无效请求】当前 KEY 被暂停使用 
   
104 【无效请求】请求 KEY 异常，请联系管理员 
   
106 【无效请求】非法请求过多，请联系管理员 
   
107 【无效请求】被禁止的 IP 或者签名错误 
   
108 【无效请求】异常请求过多，请联系管理员 
   
109 【无效请求】请求超过每日系统限制 
   
111 【无效请求】接口权限未开通，请联系管理员 
   
112 【无效请求】您的账号剩余使用量已不足或已过期 
   
113 【无效请求】当前接口已被删除，请重新申请 
   
114 【无效请求】当前接口已被禁用，请联系管理员 
   
115 【无效请求】身份验证错误或者已过期 
   
116 【无效请求】请求超过每日调用总量限制 
   
117 【无效请求】当前不支持的请求参数调用量过多 
   
118 【无效请求】当前接口不支持此方式的调用 
   
119 【无效请求】您的帐号出现异常，请联系管理员 
   
120 【无效请求】系统流量异常，请稍后再试 
   
199 【无效请求】系统未知错误，请联系技术客服 
   
203 【无效请求】系统查询有异常，请联系技术人员 
   
214 【无效请求】您还未购买过该接口，请先购买 
   
223 【无效请求】当前接口的计费方式不支持此查询参
   
 
   
 
   
代码实现：
   
```
   Import requests
Import json
   Import time
From hashlib import md5

#请求头的构造：

   #获取时间戳
Def get_time_tup():
   ‘’’
:return : 13位精确到秒的时间戳
   ‘’’
Time_tup = str(int(time.time()))

Return time_tup

   #md5加密
‘’’
   :param s: 拼接的字符串
：return : md5加密再转化为大写的字符串
   ‘’’ 
Def set_md5(s):
   New_md5 = md5()
New_md5.update(s.encode(encoding=’utf-8’))
   S_md5 = new_md5.hexdigest().upper()

Return s_md5

#设置请求头
   def get_headers(key,secret_key):
‘’’
   :param key :我的key
:param secret_key: 我的秘钥
   ：return: 请求头
‘’’
   headers = dict()
token = key + get_time_tup() + secret_key
   headers[“Token”] = set_md5(Token)
headers[“Timespan”] = get_time_tup()
   return headers
#批量申请数据
   def get_data(codes,name,key,secret_key):
 “””
    param codes: 关键字的可迭代对象
 param name: 存储文档的署名
    param key:  我的key
 param secret_key:  我的秘钥
    return : 无
 “””
    with open(f”data_{name}.csv” , “w”) as f , open(f”error_data_{name}.csv” , “w”)  as  f2:
   count = 013  count_true = 0
      count_false = 0
   for code in codes:
   ​     try:
​       count+=1
   ​     url = f”http://api.qichacha.com/ECIV4/GetBasicDtailsByname?key={}”
​     s = requests.get(url = url, headers = get_headers(key, secret_key))20
   ​     t = s.text21
​     m = json.loads(t)
   ​     print(M)
​     if m[“status”] == “200”:
   ​      x = m[“result”]-      f.write(str(x[“keyno”]) + ‘~’ + str(x[“name”]) + ‘~’ + ....)
​     else:
   ​      f2.write(code)
​     except:
   ​      f2.write(code)
if __name__ ==  ‘_main_’:
   codes = data[“企业名称”]
key = 我的key
   secret_key = 我的秘钥
get_data(codes, “exmple”, key , secret_key)
   ```

   

   3,创建表格存储，分两个表，因为有一些关键字无法匹配到数据，需要将这些关键字记录下来，方便后续处理，同时统计一下匹配和未匹配的数量

   4，最后使用Python里面的pandas，读取相应的CSV文件，再与原始数据关联就行了

   -json返回示例：

   ```
 {
     "JobId": null, 
 "Result": {
     "Partners": [
  { 
    "KeyNo": "xxxxxxxxxxxxxxxxxxxxx",
  "StockName": "xxx", 
     "TagsList": [ 
 "大股东", "实际控制人", "最终受益人" 
    ],
  "FinalBenefitPercent": "39.9559%", "RelatedOrg": null
     }, { 
 "KeyNo": "xxxxxxxxxxxxxxxxxx", 
     "TagsList": [],
 "RelatedProduct": { 
    "Id": "xxxxxxxxxxxxxxxxxxx",  
 } }  ],
     "Employees": [ 
 { 
    "KeyNo": "xxxxxxxxxxxxxxxxxx", "Name": "xxx", "Job": "董事长" },  



4，夸克企典代替企查查接口

1，准备软件fiddler

2，打开，进行配置tools->fiddle options 

3，查看电脑IP地址，并记录

4，打开手机WiFi使电脑和手机处于同一个wifi下，修改wifi，勾选高级选项，代理选择手动，服务器主机名输入电脑IP地址，服务端口输入8888，保存

5，打开手机浏览器输入 “http://ipv4.fiddler:8888”,点击fiddlerroot_certficate 下载fiddler的证书

6，下载完成，打开收设置，->安全和隐私->选择下载的证书->为证书命名->点击受信用的凭证->用户->出现DO_NOT_TRUST表示证书安装成功

7，配置完成后在手机使用APP，电脑端的fiddler就能捕捉到应用的请求接口，右键该接口选择copy->just URL就可以复制该接口，在浏览器中粘贴使用即可获取数据（post请求需要使用接口测试工具打开，get无限制）

 

# 4，个人期望

 python后端开发

 

# 5，Python服务端后端开发工程师知识

 

1，岗位职责

Python服务端工程师每天做什么

主要是三点：

实现网站后台业务逻辑

为网站/客户端（安卓，iOS）提供API接口

为产品/运营等提供后台网站工具，比如后台运营系统

2，岗位特点：

后端知识又多又杂，编程语言，数据库，算法，网络，架构

工作多样，写接口，网站，工具，后端，脚本、

软技能，怼开发，产品，运营

3，知识体系

面试技巧,，Python语言，算法与数据结构，编程范式，操作系统

网络编程协议，数据库，Web框架，系统设计，面试经验

4，后端职位分析

招聘信息看什么

岗位职责（业务是否感兴趣）

职位要求（自己是否掌握，查漏补缺）

公司技术栈（公司使用到哪些技术）

面试的流程和环节

学生重基础，社招重项目

一面问基础

二面问项目

三面问设计

 社招重视项目和设计

参与过哪些项目，有没有知名项目

在项目中承担责任

有没有系统设计经验

 HR面试

5，Python后端面试技术栈

技术：。。。，软实力：学习能力，业务理解能力，沟通交流能力，心态

 

6，Python初/中级工程师技能要求

初级工程师

扎实的计算机理论基础，代码规范，风格良好，能在指导下靠谱地完成业务需求

中级工程师

扎实的计算机基础和丰富的项目经验，能独立设计和完成项目需求

熟悉常用web组件（缓存，消息队列等），具备一定的系统设计能力

软实力

具有产品意识，技术引导产品

沟通交流能力，团队协作能力

技术领导能力和影响力

面试造核弹，工作拧螺丝

工作内容和业务紧密相关

平台决定成长（业务体量）

准备面试需要有的放矢，跟职位相匹配

简历内容

 表现个人优势，突出关键信息

基本信息（姓名，学校，简历，联系方式等）

职业技能（编程语言，框架，数据库，开发工具等）

关键项目经验（担任职责，用到了哪些技术）

可有可无：

自我评价

最终还是面试官评价

内容简洁，态度真诚

简历加分项

脱颖而出

知名项目经验

技术栈比较匹配

开源项目（GitHub/技术blog/Linux/Unix geek）

Star模型（指定表格）

 情境（situation）   什么情况下发生的

任务（task）			你是如何明确你的任务的

行动（action）		采取了什么样的行动

结果（result）		结果怎么样？学到了什么

面试官一般会问：你还有什么要问我的吗

表现出兴趣：问问工作内容，技术栈，团队，项目

问自己的感兴趣的一些技术问题，和架构问题

态度真诚，力求真实，不要弄虚作假

言简意赅，突出重点

采用star模型让回答更有条理

行为面试题

讲讲你做过的最有难度的项目

你做了哪些工作？承担的职责是什么？

你遇到的困难是什么？如何解决的？

 

# 6，Python面试题

## 1，框架组件

你熟悉的所有web开发框架，他们分别有什么样的优缺点

Django功能大而全，Flask只包含基本的配置, Django的一站式解决的思路，能让开发者不用在开发之前不用花费大量时间在选择应用的基础设施。

Django有模板，表单，路由，认证，基本的数据库管理等等内建功能。与之相反，Flask只是一个内核，默认依赖于两个外部库： Jinja2 模板引擎和 Werkzeug WSGI 工具集，其他很多功能都是以扩展的形式进行嵌入使用。

Flask 比 Django 更灵活 用Flask来构建应用之前，选择组件的时候会给开发者带来更多的灵活性  ，可能有的应用场景不适合使用一个标准的ORM(Object-Relational Mapping  对象关联映射)，或者需要与不同的工作流和模板系统交互。

**一、整体设计方面**

首先，两者都是非常优秀的框架。整体来讲，两者设计的哲学是区别最大的地方。

Django提供一站式的解决方案，从模板、ORM、Session、Authentication等等都分配好了，app, admin后台生成, 使用非常方便.

Flask只提供了一些核心功能，非常简洁优雅。它是一个微框架，其他的由扩展提供，但它的blueprint使它也能够很方便的进行水平扩展。

**二、路由设计
**

Django的路由设计是采用集中处理的方法，利用正则匹配。Flask也能这么做，但更多的是使用装饰器的形式，这个有优点也有缺点，优点是读源码时看到函数就知道怎么用的，缺点是一旦源码比较长，你要查路由就不太方便了，但这也促使你去思考如何更合理的安排代码。

**三、应用模块化设计**

Django的模块化是集成在命令里的，也就是说一开始Django的目标就是为以后玩大了做准备的。每个都是一个独立的模块，为以后的复用提供了便利。

Flask通过Blueprint来提供模块化，自己对项目结构划分成不同的模块进行组织。

**四、配置
**

Django的配置主要还是靠settings.py来做，当然为了Development和Production环境分离，还有一些方法来处理配置。
 Flask的配置很灵活，有多种方法配置，不同环境的配置也非常方便。

**五、文档**

两者都提供了详尽的文档，Flask的文档风格很受我个人喜好，Django的文档也非常优秀，当时用学Django时，就是只看了Django的文档。

**六、社区**

Django社区很大，各种插件很齐全，大部分情况下你都能找到你想要的。

Flask起步晚，但社区也不小，之前有一次看在github上的star数，两个相差并不远，说明越来越多的人关注它，虽然插件没那么全，但常用的还都是有的，而且质量都比较高。

请简述一下MVC和MTV

一、MVC 是 Model-View-Controller 的缩写，其中每个单词都有其不同的含义：

M-model模型（数据存储层，和数据库打交道）

V-view视图（视图层，用户看到并与之交互的界面，如由html元素组成的网页界面，或软件的客户端界面）

C-controler 控制器（控制层，如图下所示，控制器把视图层所给的指令用来检索数据层的数据，该层编写代码产生结果并输出）

（ps:图片虽然有点糊，但是应该能够大概看出来什么是什么）

二、MTV是 Model-Template-Controller 的缩写，其中每个单词都有其不同的含义：

M-Model模型 （数据存储层，和数据库打交道。相当于mvc的m）

T-Template 模板（模板层，用来处理页面的显示。相当于mvc的v）

V-View 视图（业务逻辑层，处理具体的业务逻辑，如下图所示，它的作用是通过请求响应获取Model 层，把Model层与 Template层联通。相当于mvc的c）

 （ps:图片虽然有点糊，但是应该能够大概看出来什么是什么）

 三、mvc和mtv的区别：

MTV 是 MVC 的一种细化。他们其实就是叫法上不同而已，都有那三部分，但那些都是根据需求决定的，都不是绝对的。


Flask框架问题：

简述flask框架中的Local对象和threading.local对象的区别

 ```
有些应用使用的是greenlet协程，这种情况下无法保证协程之间数据的隔离，因为不同的协程可以在同一个线程当中。 即使使用的是线程，WSGI应用也无法保证每个http请求使用的都是不同的线程，因为后一个http请求可能使用的是之前的 http请求的线程，这样的话存储于thread local中的数据可能是之前残留的数据。

为了解决上述问题，Werkzeug开发了自己的local对象，这也是为什么我们需要Werkzeug的local对象
 ```

实现websocket需要什么组件

\- flask默认使用wsgi支持http协议，如需使用websocket需要安装gevent-websocket模块，http，websocket协议都可以支持

　　Django应用：channel
　　Tornado应用：自己有

Beautifulsoup的使用

Selenium的作用和如何使用

Scrapy框架问题：

各组件中的工作流程

如何实现记录爬虫的深度

Django框架问题：

Django开发中数据优化

2，Django数据处理的优化

Django数据层提供各种途径优化数据的访问，一个项目大量优化工作一般是放在后期来做，早期的优化是“万恶之源”，这是前人总结的经验，不无道理。如果事先理解Django的优化技巧，开发过程中稍稍留意，后期会省不少的工作量。

**一 利用标准数据库优化技术：**

传统数据库优化技术博大精深，不同的数据库有不同的优化技巧，但重心还是有规则的。在这里算是题外话，挑两点通用的说说：

索引，给关键的字段添加索引，性能能更上一层楼，如给表的关联字段，搜索频率高的字段加上索引等。Django建立实体的时候，支持给字段添加索引，具体参考Django.db.models.Field.db_index。按照经验，Django建立实体之前应该早想好表的结构，尽量想到后面的扩展性，避免后面的表的结构变得面目全非。

使用适当字段类型，本来varchar就搞定的字段，就别要text类型，小细节别不关紧要，后头数据量一上去，几亿几亿的数据，小字段很可能是大问题。


**二 了解Django的QuerySets：**

了解Django的QuerySets对象，对优化简单程序有至关重要的作用。QuerySets是有缓存的，一旦取出来，它就会在内存里呆上一段时间，尽量重用它。举个简单的例子：

了解缓存属性：
\>>> entry = Entry.objects.get(id=1)
\>>> entry.blog  # 博客实体第一次取出，是要访问数据库的
\>>> entry.blog  # 第二次再用，那它就是缓存里的实体了，不再访问数据库


但下面的例子就不一样，
\>>> entry = Entry.objects.get(id=1) >>> entry.authors.all()  # 第一次all函数会查询数据库 >>> entry.authors.all()  # 第二次all函数还会查询数据库 all，count  exists是调用函数（需要连接数据库处理结果的），注意在模板template里的代码，模板里不允许括号，但如果使用此类的调用函数，一样去连接数据库的，能用缓存的数据就别连接到数据库去处理结果。还要注意的是，自定义的实体属性，如果调用函数的，记得自己加上缓存策略。

利用好模板的with标签：
模板中多次使用的变量，要用with标签，把它看成变量的缓存行为吧。

使用QuerySets的iterator()：
通常QuerySets先调用iterator再缓存起来，当获取大量的实体列表而仅使用一次时，缓存行为会耗费宝贵的内存，这时iterator()能帮到你，iterator()只调用iterator而省去了缓存步骤，显著减少内存占用率，具体参考相关文档。


**三 数据库的工作就交给数据库本身计算，别用Python处理：**

 1 使用 filter and exclude 过滤不需要的记录，这两个是最常用语句，相当是SQL的where。 2 同一实体里使用F()表达式过滤其他字段。 3 使用annotate对数据库做聚合运算。

不要用python语言对以上类型数据过滤筛选，同样的结果，python处理复杂度要高，而且效率不高， 白白浪费内存。

使用QuerySet.extra()：
extra虽然扩展性不太好，但功能很强大，如果实体里需要需要增加额外属性，不得已时，通过extra来实现，也是个好办法。

使用原生的SQL语句：
如果发现Django的ORM已经实现不了你的需求，而extra也无济于事的时候，那就用原生SQL语句吧，用Djangoango.db.connection.queries去实现你需要的东西。


**四 如果需要就一次性取出你所需要的数据：**

单一动作（如：同一个页面）需要多次连接数据库时，最好一次性取出所有需要的数据，减少连接数据库次数。此类需求推荐使用QuerySet.select_related() 和 prefetch_related()。

相反，别取出你不需要的东西，模版templates里往往只需要实体的某几个字段而不是全部，这时QuerySet.values() 和  values_list()，对你有用，它们只取你需要的字段，返回字典dict和列表list类型的东西，在模版里够用即可，这可减少内存损耗，提高性能。

同样QuerySet.defer()和only()对提高性能也有很大的帮助，一个实体里可能有不少的字段，有些字段包含很多元数据，比如博客的正文，很多字符组成，Django获取实体时（取出实体过程中会进行一些python类型转换工作），我们可以延迟大量元数据字段的处理，只处理需要的关键字段，这时QuerySet.defer()就派上用场了，在函数里传入需要延时处理的字段即可；而only()和defer()是相反功能。

使用QuerySet.count()代替len(queryset),虽然这两个处理得出的结果是一样的，但前者性能优秀很多。同理判断记录存在时，QuerySet.exists()比if queryset实在强得太多了。

当然一样的结果，在缓存里已经存在，就别滥用count()，exists()，all()函数了。


**五 懂减少数据库的连接数：**

使用 QuerySet.update() 和 delete()，这两个函数是能批处理多条记录的，适当使用它们事半功倍；如果可以，别一条条数据去update delete处理。

对于一次性取出来的关联记录，获取外键的时候，直接取关联表的属性，而不是取关联属性，如： entry.blog.id 优于 entry.blog_id 
善于使用批量插入记录，如：
Entry.objects.bulk_create([
  Entry(headline="Python 3.0 Released"),
  Entry(headline="Python 3.1 Planned")
])
优于
Entry.objects.create(headline="Python 3.0 Released")
Entry.objects.create(headline="Python 3.1 Planned")
前者只连接一次数据库，而后者连接两次哦。

还有相似的动作需要注意的，如：多对多的关系，
my_band.members.add(me, my_friend)
优于
my_band.members.add(me)
my_band.members.add(my_friend)



消息中间件，在生产中出现消息丢失处理

数据的丢失问题，可能出现在生产者、MQ、消费者中，咱们从 RabbitMQ 和 Kafka 分别来分析一下吧。
RabbitMQ

rabbitmq-message-lose

生产者弄丢了数据

生产者将数据发送到 RabbitMQ 的时候，可能数据就在半路给搞丢了，因为网络问题啥的，都有可能。

此时可以选择用 RabbitMQ 提供的事务功能，就是生产者发送数据之前开启 RabbitMQ 事务channel.txSelect，然后发送消息，如果消息没有成功被 RabbitMQ 接收到，那么生产者会收到异常报错，此时就可以回滚事务channel.txRollback，然后重试发送消息；如果收到了消息，那么可以提交事务channel.txCommit。

 

    // 开启事务
    channel.txSelect
    try {
        // 这里发送消息
    } catch (Exception e) {
        channel.txRollback
     
        // 这里再次重发这条消息
    }
     
    // 提交事务
    channel.txCommit

但是问题是，RabbitMQ 事务机制（同步）一搞，基本上吞吐量会下来，因为太耗性能。

所以一般来说，如果你要确保说写 RabbitMQ 的消息别丢，可以开启 confirm 模式，在生产者那里设置开启 confirm 模式之后，你每次写的消息都会分配一个唯一的 id，然后如果写入了 RabbitMQ 中，RabbitMQ 会给你回传一个 ack 消息，告诉你说这个消息 ok 了。如果 RabbitMQ 没能处理这个消息，会回调你的一个 nack 接口，告诉你这个消息接收失败，你可以重试。而且你可以结合这个机制自己在内存里维护每个消息 id 的状态，如果超过一定时间还没接收到这个消息的回调，那么你可以重发。

事务机制和 cnofirm 机制最大的不同在于，事务机制是同步的，你提交一个事务之后会阻塞在那儿，但是 confirm 机制是异步的，你发送个消息之后就可以发送下一个消息，然后那个消息 RabbitMQ 接收了之后会异步回调你的一个接口通知你这个消息接收到了。

所以一般在生产者这块避免数据丢失，都是用 confirm 机制的。

RabbitMQ 弄丢了数据

就是 RabbitMQ 自己弄丢了数据，这个你必须开启 RabbitMQ 的持久化，就是消息写入之后会持久化到磁盘，哪怕是 RabbitMQ 自己挂了，恢复之后会自动读取之前存储的数据，一般数据不会丢。除非极其罕见的是，RabbitMQ 还没持久化，自己就挂了，可能导致少量数据丢失，但是这个概率较小。

设置持久化有两个步骤：

    创建 queue 的时候将其设置为持久化
    
    这样就可以保证 RabbitMQ 持久化 queue 的元数据，但是它是不会持久化 queue 里的数据的。
    第二个是发送消息的时候将消息的 deliveryMode 设置为 2
    
    就是将消息设置为持久化的，此时 RabbitMQ 就会将消息持久化到磁盘上去。

必须要同时设置这两个持久化才行，RabbitMQ 哪怕是挂了，再次重启，也会从磁盘上重启恢复 queue，恢复这个 queue 里的数据。

注意，哪怕是你给 RabbitMQ 开启了持久化机制，也有一种可能，就是这个消息写到了 RabbitMQ 中，但是还没来得及持久化到磁盘上，结果不巧，此时 RabbitMQ 挂了，就会导致内存里的一点点数据丢失。

所以，持久化可以跟生产者那边的 confirm 机制配合起来，只有消息被持久化到磁盘之后，才会通知生产者 ack 了，所以哪怕是在持久化到磁盘之前，RabbitMQ 挂了，数据丢了，生产者收不到 ack，你也是可以自己重发的。

消费端弄丢了数据

RabbitMQ 如果丢失了数据，主要是因为你消费的时候，刚消费到，还没处理，结果进程挂了，比如重启了，那么就尴尬了，RabbitMQ 认为你都消费了，这数据就丢了。

这个时候得用 RabbitMQ 提供的 ack 机制，简单来说，就是你必须关闭 RabbitMQ 的自动 ack，可以通过一个 api 来调用就行，然后每次你自己代码里确保处理完的时候，再在程序里 ack 一把。这样的话，如果你还没处理完，不就没有 ack 了？那 RabbitMQ 就认为你还没处理完，这个时候 RabbitMQ 会把这个消费分配给别的 consumer 去处理，消息是不会丢的。

rabbitmq-message-lose-solution
Kafka

消费端弄丢了数据

唯一可能导致消费者弄丢数据的情况，就是说，你消费到了这个消息，然后消费者那边自动提交了 offset，让 Kafka 以为你已经消费好了这个消息，但其实你才刚准备处理这个消息，你还没处理，你自己就挂了，此时这条消息就丢咯。

这不是跟 RabbitMQ 差不多吗，大家都知道 Kafka 会自动提交 offset，那么只要关闭自动提交 offset，在处理完之后自己手动提交 offset，就可以保证数据不会丢。但是此时确实还是可能会有重复消费，比如你刚处理完，还没提交 offset，结果自己挂了，此时肯定会重复消费一次，自己保证幂等性就好了。

生产环境碰到的一个问题，就是说我们的 Kafka 消费者消费到了数据之后是写到一个内存的 queue 里先缓冲一下，结果有的时候，你刚把消息写入内存 queue，然后消费者会自动提交 offset。然后此时我们重启了系统，就会导致内存 queue 里还没来得及处理的数据就丢失了。

Kafka 弄丢了数据

这块比较常见的一个场景，就是 Kafka 某个 broker 宕机，然后重新选举 partition 的 leader。大家想想，要是此时其他的 follower 刚好还有些数据没有同步，结果此时 leader 挂了，然后选举某个 follower 成 leader 之后，不就少了一些数据？这就丢了一些数据啊。

生产环境也遇到过，我们也是，之前 Kafka 的 leader 机器宕机了，将 follower 切换为 leader 之后，就会发现说这个数据就丢了。

所以此时一般是要求起码设置如下 4 个参数：

    给 topic 设置 replication.factor 参数：这个值必须大于 1，要求每个 partition 必须有至少 2 个副本。
    在 Kafka 服务端设置 min.insync.replicas 参数：这个值必须大于 1，这个是要求一个 leader 至少感知到有至少一个 follower 还跟自己保持联系，没掉队，这样才能确保 leader 挂了还有一个 follower 吧。
    在 producer 端设置 acks=all：这个是要求每条数据，必须是写入所有 replica 之后，才能认为是写成功了。
    在 producer 端设置 retries=MAX（很大很大很大的一个值，无限次重试的意思）：这个是要求一旦写入失败，就无限重试，卡在这里了。

我们生产环境就是按照上述要求配置的，这样配置之后，至少在 Kafka broker 端就可以保证在 leader 所在 broker 发生故障，进行 leader 切换时，数据不会丢失。

生产者会不会弄丢数据？

如果按照上述的思路设置了 acks=all，一定不会丢，要求是，你的 leader 接收到消息，所有的 follower 都同步到了消息之后，才认为本次写成功了。如果没满足这个条件，生产者会自动不断的重试，重试无限次。为什么使用消息队列

先说一下消息队列常见的使用场景吧，其实场景有很多，但是比较核心的有 3 个：**解耦**、**异步**、**削峰**。

解耦

看这么个场景。A 系统发送数据到 BCD 三个系统，通过接口调用发送。如果 E 系统也要这个数据呢？那如果 C 系统现在不需要了呢？A 系统负责人几乎崩溃......

![img](https:////upload-images.jianshu.io/upload_images/10089464-bdc3230f73c148b6.png?imageMogr2/auto-orient/strip|imageView2/2/w/697/format/webp)

mq-1

在这个场景中，A 系统跟其它各种乱七八糟的系统严重耦合，A 系统产生一条比较关键的数据，很多系统都需要 A 系统将这个数据发送过来。A 系统要时时刻刻考虑 BCDE 四个系统如果挂了该咋办？要不要重发，要不要把消息存起来？头发都白了啊！

如果使用 MQ，A 系统产生一条数据，发送到 MQ 里面去，哪个系统需要数据自己去 MQ 里面消费。如果新系统需要数据，直接从 MQ 里消费即可；如果某个系统不需要这条数据了，就取消对 MQ 消息的消费即可。这样下来，A 系统压根儿不需要去考虑要给谁发送数据，不需要维护这个代码，也不需要考虑人家是否调用成功、失败超时等情况。

![img](https:////upload-images.jianshu.io/upload_images/10089464-57eb773f3dd4b53d.png?imageMogr2/auto-orient/strip|imageView2/2/w/666/format/webp)

mq-2

**总结**：通过一个 MQ，Pub/Sub 发布订阅消息这么一个模型，A 系统就跟其它系统彻底解耦了。

**面试技巧**：你需要去考虑一下你负责的系统中是否有类似的场景，就是一个系统或者一个模块，调用了多个系统或者模块，互相之间的调用很复杂，维护起来很麻烦。但是其实这个调用是不需要直接同步调用接口的，如果用 MQ 给它异步化解耦，也是可以的，你就需要去考虑在你的项目里，是不是可以运用这个 MQ 去进行系统的解耦。在简历中体现出来这块东西，用 MQ 作解耦。

异步

再来看一个场景，A 系统接收一个请求，需要在自己本地写库，还需要在 BCD 三个系统写库，自己本地写库要 3ms，BCD 三个系统分别写库要 300ms、450ms、200ms。最终请求总延时是 3 + 300 + 450 + 200 = 953ms，接近 1s，用户感觉搞个什么东西，慢死了慢死了。用户通过浏览器发起请求，等待个 1s，这几乎是不可接受的。

![img](https:////upload-images.jianshu.io/upload_images/10089464-c6a24c852cae87ba.png?imageMogr2/auto-orient/strip|imageView2/2/w/572/format/webp)

mq-3

一般互联网类的企业，对于用户直接的操作，一般要求是每个请求都必须在 200 ms 以内完成，对用户几乎是无感知的。

如果**使用 MQ**，那么 A 系统连续发送 3 条消息到 MQ 队列中，假如耗时 5ms，A 系统从接受一个请求到返回响应给用户，总时长是 3 + 5 = 8ms，对于用户而言，其实感觉上就是点个按钮，8ms 以后就直接返回了，爽！网站做得真好，真快！

![img](https:////upload-images.jianshu.io/upload_images/10089464-11b4f57351c0f69d.png?imageMogr2/auto-orient/strip|imageView2/2/w/575/format/webp)

mq-4

削峰

每天 0:00 到 12:00，A 系统风平浪静，每秒并发请求数量就 50 个。结果每次一到 12:00 ~ 13:00 ，每秒并发请求数量突然会暴增到 5k+ 条。但是系统是直接基于 MySQL 的，大量的请求涌入 MySQL，每秒钟对 MySQL 执行约 5k 条 SQL。

一般的 MySQL，扛到每秒 2k 个请求就差不多了，如果每秒请求到 5k 的话，可能就直接把 MySQL 给打死了，导致系统崩溃，用户也就没法再使用系统了。

但是高峰期一过，到了下午的时候，就成了低峰期，可能也就 1w 的用户同时在网站上操作，每秒中的请求数量可能也就 50 个请求，对整个系统几乎没有任何的压力。

[图片上传失败...(image-6444f7-1548645188187)]

如果使用 MQ，每秒 5k 个请求写入 MQ，A 系统每秒钟最多处理 2k 个请求，因为 MySQL 每秒钟最多处理 2k 个。A 系统从 MQ 中慢慢拉取请求，每秒钟就拉取 2k 个请求，不要超过自己每秒能处理的最大请求数量就 ok，这样下来，哪怕是高峰期的时候，A 系统也绝对不会挂掉。而 MQ 每秒钟 5k 个请求进来，就 2k 个请求出去，结果就导致在中午高峰期（1 个小时），可能有几十万甚至几百万的请求积压在 MQ 中。

![img](https:////upload-images.jianshu.io/upload_images/10089464-98c85c19e8f2f63d.png?imageMogr2/auto-orient/strip|imageView2/2/w/443/format/webp)

mq-6

这个短暂的高峰期积压是 ok 的，因为高峰期过了之后，每秒钟就 50 个请求进 MQ，但是 A 系统依然会按照每秒 2k 个请求的速度在处理。所以说，只要高峰期一过，A 系统就会快速将积压的消息给解决掉。

消息队列有什么优缺点

优点上面已经说了，就是**在特殊场景下有其对应的好处**，**解耦**、**异步**、**削峰**。

缺点有以下几个：

- 系统可用性降低

   系统引入的外部依赖越多，越容易挂掉。本来你就是 A 系统调用 BCD 三个系统的接口就好了，人 ABCD 四个系统好好的，没啥问题，你偏加个 MQ 进来，万一 MQ 挂了咋整，MQ 一挂，整套系统崩溃的，你不就完了？如何保证消息队列的高可用，可以[点击这里查看]()。

- 系统复杂度提高

   硬生生加个 MQ 进来，你怎么[保证消息没有重复消费]？怎么[处理消息丢失的情况]？怎么保证消息传递的顺序性？头大头大，问题一大堆，痛苦不已。

- 一致性问题

   A 系统处理完了直接返回成功了，人都以为你这个请求就成功了；但是问题是，要是 BCD 三个系统那里，BD 两个系统写库成功了，结果 C 系统写库失败了，咋整？你这数据就不一致了。

所以消息队列实际是一种非常复杂的架构，你引入它有很多好处，但是也得针对它带来的坏处做各种额外的技术方案和架构来规避掉，做好之后，你会发现，妈呀，系统复杂度提升了一个数量级，也许是复杂了 10 倍。但是关键时刻，用，还是得用的。

Kafka、ActiveMQ、RabbitMQ、RocketMQ 有什么优缺点？

| 特性                     | ActiveMQ                              | RabbitMQ                                           | RocketMQ                                                     | Kafka                                                        |
| ------------------------ | ------------------------------------- | -------------------------------------------------- | ------------------------------------------------------------ | ------------------------------------------------------------ |
| 单机吞吐量               | 万级，比 RocketMQ、Kafka 低一个数量级 | 同 ActiveMQ                                        | 10 万级，支撑高吞吐                                          | 10 万级，高吞吐，一般配合大数据类的系统来进行实时数据计算、日志采集等场景 |
| topic 数量对吞吐量的影响 |                                       |                                                    | topic 可以达到几百/几千的级别，吞吐量会有较小幅度的下降，这是 RocketMQ 的一大优势，在同等机器下，可以支撑大量的 topic | topic 从几十到几百个时候，吞吐量会大幅度下降，在同等机器下，Kafka 尽量保证 topic 数量不要过多，如果要支撑大规模的 topic，需要增加更多的机器资源 |
| 时效性                   | ms 级                                 | 微秒级，这是 RabbitMQ 的一大特点，延迟最低         | ms 级                                                        | 延迟在 ms 级以内                                             |
| 可用性                   | 高，基于主从架构实现高可用            | 同 ActiveMQ                                        | 非常高，分布式架构                                           | 非常高，分布式，一个数据多个副本，少数机器宕机，不会丢失数据，不会导致不可用 |
| 消息可靠性               | 有较低的概率丢失数据                  | 基本不丢                                           | 经过参数优化配置，可以做到 0 丢失                            | 同 RocketMQ                                                  |
| 功能支持                 | MQ 领域的功能极其完备                 | 基于 erlang 开发，并发能力很强，性能极好，延时很低 | MQ 功能较为完善，还是分布式的，扩展性好                      | 功能较为简单，主要支持简单的 MQ 功能，在大数据领域的实时计算以及日志采集被大规模使用 |

综上，各种对比之后，有如下建议：

一般的业务系统要引入 MQ，最早大家都用 ActiveMQ，但是现在确实大家用的不多了，没经过大规模吞吐量场景的验证，社区也不是很活跃，所以大家还是算了吧，我个人不推荐用这个了；

后来大家开始用 RabbitMQ，但是确实 erlang 语言阻止了大量的 Java 工程师去深入研究和掌控它，对公司而言，几乎处于不可控的状态，但是确实人家是开源的，比较稳定的支持，活跃度也高；

不过现在确实越来越多的公司，会去用 RocketMQ，确实很不错（阿里出品），但社区可能有突然黄掉的风险，对自己公司技术实力有绝对自信的，推荐用 RocketMQ，否则回去老老实实用 RabbitMQ 吧，人家有活跃的开源社区，绝对不会黄。

所以**中小型公司**，技术实力较为一般，技术挑战不是特别高，用 RabbitMQ 是不错的选择；**大型公司**，基础架构研发实力较强，用 RocketMQ 是很好的选择。如果是**大数据领域**的实时计算、日志采集等场景，用 Kafka 是业内标准的，绝对没问题，社区活跃度很高，绝对不会黄，何况几乎是全世界这个领域的事实性规范

RabbitMQ 的高可用性

RabbitMQ 是比较有代表性的，因为是**基于主从**（非分布式）做高可用性的，我们就以 RabbitMQ 为例子讲解第一种 MQ 的高可用性怎么实现。

RabbitMQ 有三种模式：单机模式、普通集群模式、镜像集群模式。

单机模式

单机模式，就是 Demo 级别的，一般就是你本地启动了玩玩儿的😄，没人生产用单机模式。

普通集群模式（无高可用性）

普通集群模式，意思就是在多台机器上启动多个 RabbitMQ 实例，每个机器启动一个。你**创建的 queue，只会放在一个 RabbitMQ 实例上**，但是每个实例都同步 queue 的元数据（元数据可以认为是 queue 的一些配置信息，通过元数据，可以找到 queue 所在实例）。你消费的时候，实际上如果连接到了另外一个实例，那么那个实例会从 queue 所在实例上拉取数据过来。

![img](https:////upload-images.jianshu.io/upload_images/10089464-5a7fda39c0c6c819.png?imageMogr2/auto-orient/strip|imageView2/2/w/883/format/webp)

mq-7

这种方式确实很麻烦，也不怎么好，**没做到所谓的分布式**，就是个普通集群。因为这导致你要么消费者每次随机连接一个实例然后拉取数据，要么固定连接那个 queue 所在实例消费数据，前者有**数据拉取的开销**，后者导致**单实例性能瓶颈**。

而且如果那个放 queue 的实例宕机了，会导致接下来其他实例就无法从那个实例拉取，如果你**开启了消息持久化**，让 RabbitMQ 落地存储消息的话，**消息不一定会丢**，得等这个实例恢复了，然后才可以继续从这个 queue 拉取数据。

所以这个事儿就比较尴尬了，这就**没有什么所谓的高可用性**，**这方案主要是提高吞吐量的**，就是说让集群中多个节点来服务某个 queue 的读写操作。

镜像集群模式（高可用性）

这种模式，才是所谓的 RabbitMQ 的高可用模式。跟普通集群模式不一样的是，在镜像集群模式下，你创建的 queue，无论元数据还是 queue 里的消息都会**存在于多个实例上**，就是说，每个 RabbitMQ 节点都有这个 queue 的一个**完整镜像**，包含 queue 的全部数据的意思。然后每次你写消息到 queue 的时候，都会自动把**消息同步**到多个实例的 queue 上。

![img](https:////upload-images.jianshu.io/upload_images/10089464-ed6dae99b4e1f92d.png?imageMogr2/auto-orient/strip|imageView2/2/w/630/format/webp)

mq-8

那么**如何开启这个镜像集群模式**呢？其实很简单，RabbitMQ 有很好的管理控制台，就是在后台新增一个策略，这个策略是**镜像集群模式的策略**，指定的时候是可以要求数据同步到所有节点的，也可以要求同步到指定数量的节点，再次创建 queue 的时候，应用这个策略，就会自动将数据同步到其他的节点上去了。

这样的话，好处在于，你任何一个机器宕机了，没事儿，其它机器（节点）还包含了这个 queue 的完整数据，别的 consumer 都可以到其它节点上去消费数据。坏处在于，第一，这个性能开销也太大了吧，消息需要同步到所有机器上，导致网络带宽压力和消耗很重！第二，这么玩儿，不是分布式的，就**没有扩展性可言**了，如果某个 queue 负载很重，你加机器，新增的机器也包含了这个 queue 的所有数据，并**没有办法线性扩展**你的 queue。你想，如果这个 queue 的数据量很大，大到这个机器上的容量无法容纳了，此时该怎么办呢？

Kafka 的高可用性

Kafka 一个最基本的架构认识：由多个 broker 组成，每个 broker 是一个节点；你创建一个 topic，这个 topic 可以划分为多个 partition，每个 partition 可以存在于不同的 broker 上，每个 partition 就放一部分数据。

这就是**天然的分布式消息队列**，就是说一个 topic 的数据，是**分散放在多个机器上的，每个机器就放一部分数据**。

实际上 RabbmitMQ 之类的，并不是分布式消息队列，它就是传统的消息队列，只不过提供了一些集群、HA(High Availability, 高可用性) 的机制而已，因为无论怎么玩儿，RabbitMQ 一个 queue 的数据都是放在一个节点里的，镜像集群下，也是每个节点都放这个 queue 的完整数据。

Kafka 0.8 以前，是没有 HA 机制的，就是任何一个 broker 宕机了，那个 broker 上的 partition 就废了，没法写也没法读，没有什么高可用性可言。

比如说，我们假设创建了一个 topic，指定其 partition 数量是 3 个，分别在三台机器上。但是，如果第二台机器宕机了，会导致这个 topic 的 1/3 的数据就丢了，因此这个是做不到高可用的。

![img](https:////upload-images.jianshu.io/upload_images/10089464-c7557f252f406db5.png?imageMogr2/auto-orient/strip|imageView2/2/w/712/format/webp)

kafka-before

Kafka 0.8 以后，提供了 HA 机制，就是 replica（复制品） 副本机制。每个 partition 的数据都会同步到其它机器上，形成自己的多个 replica 副本。所有 replica 会选举一个 leader 出来，那么生产和消费都跟这个 leader 打交道，然后其他 replica 就是 follower。写的时候，leader 会负责把数据同步到所有 follower 上去，读的时候就直接读 leader 上的数据即可。只能读写 leader？很简单，**要是你可以随意读写每个 follower，那么就要 care 数据一致性的问题**，系统复杂度太高，很容易出问题。Kafka 会均匀地将一个 partition 的所有 replica 分布在不同的机器上，这样才可以提高容错性。

![img](https:////upload-images.jianshu.io/upload_images/10089464-9046e9d49f2f2691.png?imageMogr2/auto-orient/strip|imageView2/2/w/721/format/webp)

kafka-after

这么搞，就有所谓的**高可用性**了，因为如果某个 broker 宕机了，没事儿，那个 broker上面的 partition 在其他机器上都有副本的，如果这上面有某个 partition 的 leader，那么此时会从 follower 中**重新选举**一个新的 leader 出来，大家继续读写那个新的 leader 即可。这就有所谓的高可用性了。

**写数据**的时候，生产者就写 leader，然后 leader 将数据落地写本地磁盘，接着其他 follower 自己主动从 leader 来 pull 数据。一旦所有 follower 同步好数据了，就会发送 ack 给 leader，leader 收到所有 follower 的 ack 之后，就会返回写成功的消息给生产者。（当然，这只是其中一种模式，还可以适当调整这个行为）

**消费**的时候，只会从 leader 去读，但是只有当一个消息已经被所有 follower 都同步成功返回 ack 的时候，这个消息才会被消费者读到。

3，消息中间件面试题：如果让你写一个消息队列，该如何进行架构设计？



如果让你写一个消息队列，该如何进行架构设计？说一下你的思路。

其实聊到这个问题，一般面试官要考察两块：

- 你有没有对某一个消息队列做过较为深入的原理的了解，或者从整体了解把握住一个消息队列的架构原理。
- 看看你的设计能力，给你一个常见的系统，就是消息队列系统，看看你能不能从全局把握一下整体架构设计，给出一些关键点出来。

说实话，问类似问题的时候，大部分人基本都会蒙，因为平时从来没有思考过类似的问题，大多数人就是平时埋头用，从来不去思考背后的一些东西。类似的问题，比如，如果让你来设计一个 Spring 框架你会怎么做？如果让你来设计一个 Dubbo 框架你会怎么做？如果让你来设计一个 MyBatis 框架你会怎么做？

回答

其实回答这类问题，说白了，不求你看过那技术的源码，起码你要大概知道那个技术的基本原理、核心组成部分、基本架构构成，然后参照一些开源的技术把一个系统设计出来的思路说一下就好。

比如说这个消息队列系统，我们从以下几个角度来考虑一下：

- 首先这个 mq 得支持可伸缩性吧，就是需要的时候快速扩容，就可以增加吞吐量和容量，那怎么搞？设计个分布式的系统呗，参照一下  kafka 的设计理念，broker -> topic -> partition，每个 partition  放一个机器，就存一部分数据。如果现在资源不够了，简单啊，给 topic 增加  partition，然后做数据迁移，增加机器，不就可以存放更多数据，提供更高的吞吐量了？
- 其次你得考虑一下这个 mq 的数据要不要落地磁盘吧？那肯定要了，落磁盘才能保证别进程挂了数据就丢了。那落磁盘的时候怎么落啊？顺序写，这样就没有磁盘随机读写的寻址开销，磁盘顺序读写的性能是很高的，这就是 kafka 的思路。
- 其次你考虑一下你的 mq 的可用性啊？这个事儿，具体参考之前可用性那个环节讲解的 kafka 的高可用保障机制。多副本 -> leader & follower -> broker 挂了重新选举 leader 即可对外服务。
- 能不能支持数据 0 丢失啊？可以的，参考我们之前说的那个 kafka 数据零丢失方案。

mq 肯定是很复杂的，面试官问你这个问题，其实是个开放题，他就是看看你有没有从架构角度整体构思和设计的思维以及能力。确实这个问题可以刷掉一大批人，因为大部分人平时不思考这些东西。

4，消息中间件面试题：如何解决消息队列的延时以及过期失效问题？消息队列满了以后该怎么处理？有几百万消息持续积压几小时呢？

如何解决消息队列的延时以及过期失效问题？消息队列满了以后该怎么处理？有几百万消息持续积压几小时，说说怎么解决？

你看这问法，其实本质针对的场景，都是说，可能你的消费端出了问题，不消费了；或者消费的速度极其慢。接着就坑爹了，可能你的消息队列集群的磁盘都快写满了，都没人消费，这个时候怎么办？或者是这整个就积压了几个小时，你这个时候怎么办？或者是你积压的时间太长了，导致比如 RabbitMQ 设置了消息过期时间后就没了怎么办？

所以就这事儿，其实线上挺常见的，一般不出，一出就是大 case。一般常见于，举个例子，消费端每次消费之后要写 mysql，结果 mysql 挂了，消费端 hang 那儿了，不动了；或者是消费端出了个什么岔子，导致消费速度极其慢。
 关于这个事儿，我们一个一个来梳理吧，先假设一个场景，我们现在消费端出故障了，然后大量消息在 mq 里积压，现在出事故了，慌了。

大量消息在 mq 里积压了几个小时了还没解决

几千万条数据在 MQ 里积压了七八个小时，从下午 4 点多，积压到了晚上 11  点多。这个是我们真实遇到过的一个场景，确实是线上故障了，这个时候要不然就是修复 consumer  的问题，让它恢复消费速度，然后傻傻的等待几个小时消费完毕。这个肯定不能在面试的时候说吧。

一个消费者一秒是 1000 条，一秒 3 个消费者是 3000 条，一分钟就是 18 万条。所以如果你积压了几百万到上千万的数据，即使消费者恢复了，也需要大概 1 小时的时间才能恢复过来。

一般这个时候，只能临时紧急扩容了，具体操作步骤和思路如下：

- 先修复 consumer 的问题，确保其恢复消费速度，然后将现有 consumer 都停掉。
- 新建一个 topic，partition 是原来的 10 倍，临时建立好原先 10 倍的 queue 数量。
- 然后写一个临时的分发数据的 consumer 程序，这个程序部署上去消费积压的数据，**消费之后不做耗时的处理**，直接均匀轮询写入临时建立好的 10 倍数量的 queue。
- 接着临时征用 10 倍的机器来部署 consumer，每一批 consumer 消费一个临时 queue 的数据。这种做法相当于是临时将 queue 资源和 consumer 资源扩大 10 倍，以正常的 10 倍速度来消费数据。
- 等快速消费完积压数据之后，**得恢复原先部署的架构**，**重新**用原先的 consumer 机器来消费消息。

mq 中的消息过期失效了

假设你用的是 RabbitMQ，RabbtiMQ 是可以设置过期时间的，也就是 TTL。如果消息在 queue 中积压超过一定的时间就会被 RabbitMQ 给清理掉，这个数据就没了。那这就是第二个坑了。这就不是说数据会大量积压在 mq 里，而是**大量的数据会直接搞丢**。

这个情况下，就不是说要增加 consumer 消费积压的消息，因为实际上没啥积压，而是丢了大量的消息。我们可以采取一个方案，就是**批量重导**，这个我们之前线上也有类似的场景干过。就是大量积压的时候，我们当时就直接丢弃数据了，然后等过了高峰期以后，比如大家一起喝咖啡熬夜到晚上12点以后，用户都睡觉了。这个时候我们就开始写程序，将丢失的那批数据，写个临时程序，一点一点的查出来，然后重新灌入 mq 里面去，把白天丢的数据给他补回来。也只能是这样了。

假设 1 万个订单积压在 mq 里面，没有处理，其中 1000 个订单都丢了，你只能手动写程序把那 1000 个订单给查出来，手动发到 mq 里去再补一次。

mq 都快写满了

如果消息积压在 mq 里，你很长时间都没有处理掉，此时导致 mq 都快写满了，咋办？这个还有别的办法吗？没有，谁让你第一个方案执行的太慢了，你临时写程序，接入数据来消费，**消费一个丢弃一个，都不要了**，快速消费掉所有的消息。然后走第二个方案，到了晚上再补数据吧。

5，消息中间件面试题：如何保证消息的顺序性

如何保证消息的顺序性？



我举个例子，我们以前做过一个 mysql `binlog`  同步的系统，压力还是非常大的，日同步数据要达到上亿，就是说数据从一个 mysql 库原封不动地同步到另一个 mysql 库里面去（mysql  -> mysql）。常见的一点在于说比如大数据 team，就需要同步一个 mysql 库过来，对公司的业务系统的数据做各种复杂的操作。

你在 mysql 里增删改一条数据，对应出来了增删改 3 条 `binlog` 日志，接着这三条 `binlog` 发送到 MQ 里面，再消费出来依次执行，起码得保证人家是按照顺序来的吧？不然本来是：增加、修改、删除；你楞是换了顺序给执行成删除、修改、增加，不全错了么。

本来这个数据同步过来，应该最后这个数据被删除了；结果你搞错了这个顺序，最后这个数据保留下来了，数据同步就出错了。

先看看顺序会错乱的俩场景：

- **RabbitMQ**：一个 queue，多个 consumer。比如，生产者向 RabbitMQ  里发送了三条数据，顺序依次是 data1/data2/data3，压入的是 RabbitMQ 的一个内存队列。有三个消费者分别从 MQ  中消费这三条数据中的一条，结果消费者2先执行完操作，把 data2 存入数据库，然后是 data1/data3。这不明显乱了。

![img](https://upload-images.jianshu.io/upload_images/10089464-7336df74c883c12c.png?imageMogr2/auto-orient/strip|imageView2/2/w/368/format/webp)

rabbitmq-order-01

- **Kafka**：比如说我们建了一个 topic，有三个 partition。生产者在写的时候，其实可以指定一个  key，比如说我们指定了某个订单 id 作为 key，那么这个订单相关的数据，一定会被分发到同一个 partition 中去，而且这个  partition 中的数据一定是有顺序的。
  消费者从 partition 中取出来数据的时候，也一定是有顺序的。到这里，顺序还是 ok 的，没有错乱。接着，我们在消费者里可能会搞**多个线程来并发处理消息**。因为如果消费者是单线程消费处理，而处理比较耗时的话，比如处理一条消息耗时几十 ms，那么 1 秒钟只能处理几十条消息，这吞吐量太低了。而多个线程并发跑的话，顺序可能就乱掉了。

![img](https://upload-images.jianshu.io/upload_images/10089464-54994792a5c143bc.png?imageMogr2/auto-orient/strip|imageView2/2/w/517/format/webp)

kafka-order-01

  Python web框架

常用框架对比，

RESTful的准则

六大原则
1. C-S架构

数据的存储在Server端，Client端只需使用就行。两端彻底分离的好处使client端代码的可移植性变强，Server端的拓展性变强。两端单独开发，互不干扰。
2. 无状态

http请求本身就是无状态的，基于C-S架构，客户端的每一次请求带有充分的信息能够让服务端识别。请求所需的一些信息都包含在URL的查询参数、header、body，服务端能够根据请求的各种参数，无需保存客户端的状态，将响应正确返回给客户端。无状态的特征大大提高的服务端的健壮性和可拓展性。

当然这总无状态性的约束也是有缺点的，客户端的每一次请求都必须带上相同重复的信息确定自己的身份和状态（这也是必须的），造成传输数据的冗余性，但这种确定对于性能和使用来说，几乎是忽略不计的。
3.统一的接口

这个才是REST架构的核心，统一的接口对于RESTful服务非常重要。客户端只需要关注实现接口就可以，接口的可读性加强，使用人员方便调用。
4.一致的数据格式

服务端返回的数据格式要么是XML，要么是Json（获取数据），或者直接返回状态码，有兴趣的可以看看博客园的开放平台的操作数据的api，post、put、patch都是返回的一个状态码 。

自我描述的信息，每项数据应该是可以自我描述的，方便代码去处理和解析其中的内容。比如通过HTTP返回的数据里面有 [MIME type ]信息，我们从MIME type里面可以知道数据的具体格式，是图片，视频还是JSON，客户端通过body内容、查询串参数、请求头和URI（资源名称）来传送状态。服务端通过body内容，响应码和响应头传送状态给客户端。这项技术被称为超媒体（或超文本链接）。

除了上述内容外，HATEOS也意味着，必要的时候链接也可被包含在返回的body（或头部）中，以提供URI来检索对象本身或关联对象。下文将对此进行更详细的阐述。

如请求一条微博信息，服务端响应信息应该包含这条微博相关的其他URL，客户端可以进一步利用这些URL发起请求获取感兴趣的信息，再如分页可以从第一页的返回数据中获取下一页的URT也是基于这个原理。
4.系统分层

客户端通常无法表明自己是直接还是间接与端服务器进行连接，分层时同样要考虑安全策略。
5.可缓存

在万维网上，客户端可以缓存页面的响应内容。因此响应都应隐式或显式的定义为可缓存的，若不可缓存则要避免客户端在多次请求后用旧数据或脏数据来响应。管理得当的缓存会部分地或完全地除去客户端和服务端之间的交互，进一步改善性能和延展性。
6.按需编码、可定制代码（可选）

服务端可选择临时给客户端下发一些功能代码让客户端来执行，从而定制和扩展客户端的某些功能。比如服务端可以返回一些 Javascript 代码让客户端执行，去实现某些特定的功能。提示：REST架构中的设计准则中，只有按需编码为可选项。如果某个服务违反了其他任意一项准则，严格意思上不能称之为RESTful风格。


，restful API接口设计

一、RESTful的诞生背景

近年来移动互联网的发展，前端设备层出不穷（手机、平板、桌面电脑、其他专用设备…），因此，必须有一种统一的机制，方便不同的前端设备与后端进行通信,于是RESTful诞生了，它可以通过一套统一的接口为 Web，iOS和Android提供服务。
在这里插入图片描述
二、什么是RESTful？

RESTful,简称REST。
1、英文：Representational State Transfer。
2、直译：表现层状态转化。
3、本质：用URL定位资源，用HTTP动词（GET,POST,DELETE,DETC）描述操作。
4、特点：RESTful是一种软件架构风格、设计风格，而不是标准，只是提供了一组设计原则和约束条件。它主要用于客户端和服务器交互类的软件。 基于这个风格设计的软件可以更简洁，更有层次，更易于实现缓存等机制。
5、设计原则和约束条件：
5.1、网络上的所有事物都可以被抽象为资源(resource)；
5.2、每一个资源都有唯一的资源标识(resource identifier)，对资源的操作不会改变这些标识；
5.3、所有的操作都是无状态的。
凡事满足这些约束条件和原则的应用程序或设计就是 RESTful。
6、通俗解释：服务器上每一种资源，比如一个文件，一张图片，一部电影，都有对应的唯一的url地址（URI：统一资源标识符），如果我们的客户端需要对服务器上的这个资源进行操作，就需要通过http协议（GET、POST、PUT、PATCH、DELETE）执行相应的动作来操作它，比如进行获取，更新，删除。
7、补充：
7.1、 在 RPC 样式的架构中，关注点在于方法，而在 REST 样式的架构中，关注点在于资源 —— 将使用标准方法检索并操作信息片段（使用表示的形式）。资源表示形式在表示形式中使用超链接互联。
7.2、关于http接口、api接口、RPC接口、RMI、webservice、Restful等概念。
三、Restful API接口设计规范

REST描述的是在网络中client和server的一种交互形式；REST本身不实用，实用的是如何设计 RESTful API（REST风格的网络接口）。
下面是根据Restful思想设计的通用规范：
3.1、协议

包含 http 和 https，使用 https 可以确保交互数据的传输安全。
3.2、路径规则|域名

路径又称 “终点”（endpoint），表示 API 的具体网址。
在 RESTful 架构中，每个网址代表一种资源（resource），所以网址中不能有动词，只能有名词，而且所用的名词往往与数据库的表格名对应。一般来说，数据库中的表都是同种记录的 “集合”（collection），所以 API 中的名词也应该使用复数。
包含两种形式：
a、主域名：https://api.example.com
b、子目录：https://example.org/api/
3.3、版本控制

版本号：v {n} n 代表版本号，分为整形和浮点型
整型：大功能版本发布形式；具有当前版本状态下的所有 API 接口，例如：v1,v2。
浮点型：为小版本号，只具备补充 api 的功能，其他 api 都默认调用对应大版本号的 api 例如：v1.1 v2.2。
放入位置：
1、将版本号放入URL中（方便直观）。
2、将版本号放在请求头。
3.4、请求类型

GET（SELECT）：从服务器取出资源（一项或多项）。
POST（CREATE）：在服务器新建一个资源。
PUT（UPDATE）：在服务器更新资源（客户端提供改变后的完整资源）。
PATCH（UPDATE）：在服务器更新资源（客户端提供改变的属性）。
DELETE（DELETE）：从服务器删除资源。
3.5、传入参数
3.5.1、地址栏参数

主要用于过滤查询
a、restful 地址栏参数 /api/v1/product/122 122 为产品编号，获取产品为 122 的信息
b、get 方式的查询字串，此种方式主要用于过滤查询，如下：

    ?limit=10：指定返回记录的数量
    ?offset=10：指定返回记录的开始位置。
    ?page=2&per_page=100：指定第几页，以及每页的记录数。
    ?sortby=name&order=asc：指定返回结果按照哪个属性排序，以及排序顺序（sequence、order）。
    ?producy_type=1：指定筛选条件

3.5.2、请求body数据

主要用于提交新建数据
3.5.3、请求头

用于存放请求格式信息、版本号、token 密钥、语言等信息

{
    Accept: 'application/json',     //json格式
    version: 'v1.0'                       //版本号
    Authorization: 'Bearer {access_token}',   //认证token
    language: 'zh'                      //语言
}

    1
    2
    3
    4
    5
    6

3.6、返回格式

默认返回格式：

{
    code: 0,                         //状态码
    msg: 'ok',                       //提示信息
    data: {}                          //主体数据
}

使用 json 格式作为响应格式，状态码分为两种：
a、statusCode: 系统状态码，用于处理响应状态，与 http 状态码保持一致，如：200 表示请求成功，500 表示服务器错误。
b、code：业务状态码，用于处理业务状态，一般 0 标识正常，可根据需求自行设计错误码对照表，参考
在这里插入图片描述
四、非 Restful Api 的需求

我们一般以 Restful Api 作为接口规范，但是由于实际业务开展过程中，可能会出现各种的 api 不是简单的 restful 规范能实现的，因此，需要有一些 api 突破 restful 规范原则。特别是移动互联网的 api 设计，更需要有一些特定的 api 来优化数据请求的交互。
4.1、单例型：

客户端根据需求分别请求对应 Api 接口，在客户端完成组装。
这种模式服务端相对简单，接口复用率高。
每个接口作用单一，如一个 App 首页，可能有轮播图、分类、推荐商品，则需要分别请求：

/api/v1/banners: 轮播
/api/v1/categories: 分类
/api/v1/product?is_recommend=1: 商品



开发过程中可根据实际需要结合使用。
4.2、组合型：

服务端组装数据，然后返回。
把当前页面中需要用到的所有数据通过一个接口一次性返回全部数据，如：

api/v1/get-home-data 返回首页用到的所有数据



这类 API 有一个非常不好的地址，只要业务需求变动，这个 api 就需要跟着变更。
4.3、自定义组合API

把当前用户需要在第一时间内容加载的多个接口合并成一个请求发送到服务端，服务端根据请求内容，一次性把所有数据合并返回,相比于页面级API，具备更高的灵活性，同时又能很容易的实现页面级的API功能。

data:[
    {url:'api1',type:'get',data:{...}},
    {url:'api2',type:'get',data:{...}},
    {url:'api3',type:'get',data:{...}},
    {url:'api4',type:'get',data:{...}}
]



总结：
简单来说就是url地址中只包含名词表示资源，使用http动词表示动作进行操作资源.
下面几个例子：左边是错误的设计，而右边是正确的

GET /blog/getArticles --> GET /blog/Articles  获取所有文章
GET /blog/addArticles --> POST /blog/Articles  添加一篇文章
GET /blog/editArticles --> PUT /blog/Articles  修改一篇文章 
GET /rest/api/deleteArticles?id=1 --> DELETE /blog/Articles/1  删除一篇文章



WSGI原理

[     uwsgi和wsgi的区别      ](https://www.cnblogs.com/2193657219qq/p/13911265.html)             

uwsgi
 wsgi：一种实现python解析的通用接口标准/协议，是一种通用的接口标准或者接口协议，实现了python web程序与服务器之间交互的通用性。
 利用它，web.py或bottle或者django等等的python web开发框架，就可以轻松地部署在不同的web server上了；
 uwsgi:同WSGI一样是一种通信协议
 uwsgi协议是一个uWSGI服务器自有的协议，它用于定义传输信息的类型，它与WSGI相比是两样东西。
 uWSGI :一种python web server或称为Server/Gateway
 uWSGI类似tornadoweb或者flup，是一种python web server，uWSGI是实现了uwsgi和WSGI两种协议的Web服务器，负责响应python 的web请求。
 因为apache、nginx等，它们自己都没有解析动态语言如php的功能，而是分派给其他模块来做，比如apache就可以说内置了php模块，让人感觉好像apache就支持php一样。
 uWSGI实现了wsgi协议、uwsgi协议、http等协议。 Nginx中HttpUwsgiModule的作用是与uWSGI服务器进行交换。

WSGI
 WSGI WSGI是一种WEB服务器网关接口。 是一个Web服务器（如nginx）与应用服务器（如uWSGI）通信的一种规范（协议）。
 在生产环境中使用WSGI作为python web的服务器。Python  Web服务器网关接口，是Python应用程序或框架和Web服务器之间的一种接口，被广泛接受。WSGI没有官方的实现,  因为WSGI更像一个协议，只要遵照这些协议，WSGI应用(Application)都可以在任何服务器(Server)上运行。
 uWSGI uWSGI实现了WSGI的所有接口，是一个快速、自我修复、开发人员和系统管理员友好的服务器。uWSGI代码完全用C编写，效率高、性能稳定。
 uwsgi是一种线路协议而不是通信协议，在此常用于在uWSGI服务器与其他网络服务器的数据通信。uwsgi协议是一个uWSGI服务器自有的协议，它用于定义传输信息的类型。
 作用 Django 是一个 Web 框架，框架的作用在于处理 request 和 reponse，其他的不是框架所关心的内容。所以怎么部署 Django 不是 Django 所需要关心的。
 Django 所提供的是一个开发服务器，这个开发服务器，没有经过安全测试，而且使用的是 Python 自带的 simple HTTPServer 创建的，在安全性和效率上都是不行的
 而uWSGI 是一个全功能的 HTTP 服务器，他要做的就是把 HTTP 协议转化成语言支持的网络协议。比如把 HTTP 协议转化成 WSGI  协议，让 Python 可以直接使用。 uwsgi 是一种 uWSGI 的内部协议，使用二进制方式和其他应用程序进行通信。

后端系统常用组件（缓存，数据库，详细队列等）

技术选型和实现（短网址服务，Feed流系统）



## 2，Python语言基础

Python基础问题：

-浅拷贝和深拷贝

从服务器fetch到数据之后我将其存放在store中，通过props传递给界面，然后我需要对这堆数据进行修改，那涉及到的修改就一定有保存和取消，所以我们需要将这堆数据拷贝到其它地方。

1.python中的数据类型分为两种：

不可变数据类型：数值number、字符串String、元组tuple

可变数据类型：列表list、字典dict、集合set

2.定义

最直观的理解就是：
1.深拷贝，拷贝的程度深，自己新开辟了一块内存，将被拷贝内容全部拷贝过来了；
2.浅拷贝，拷贝的程度浅，只拷贝原数据的首地址，然后通过原数据的首地址，去获取内容。
两者的优缺点对比：
（1）深拷贝拷贝程度高，将原数据复制到新的内存空间中。改变拷贝后的内容不影响原数据内容。但是深拷贝耗时长，且占用内存空间。
（2）浅拷贝拷贝程度低，只复制原数据的地址。其实是将副本的地址指向原数据地址。修改副本内容，是通过当前地址指向原数据地址，去修改。所以修改副本内容会影响到原数据内容。但是浅拷贝耗时短，占用内存空间少。

 

- **赋值：**其实就是对象的引用（相当于取别名）。
- **浅拷贝(copy)：**拷贝父对象，不会拷贝对象内部的子对象，会引用子对象。
- **深拷贝(deepcopy)：** copy 模块的 deepcopy 方法，完全拷贝了父对象及其子对象。

3.浅拷贝

改变原始对象中为可变类型的元素的值，会同时影响拷贝对象。
改变原始对象中为不可变类型的元素的值，不会响拷贝对象。

4.深拷贝

深拷贝，除了顶层拷贝，还对子元素也进行了拷贝。
经过深拷贝后，原始对象和拷贝对象所有的**可变元素**地址都没有相同的了。

-列表和元祖的区别

一.元组的概念：

前面我们学过数字类型，字符串类型，它们都代表一个元素！

1.元组是包含多个元素的类型，元素之间用逗号分隔。

t1 = 123,456,"hello"

2.元组可以使空的，t2 = ()

3.元组包含一个元素: t3 = 123,

4.元组外层可以使用括号，也可以不使用

二、元组的三大特点:

1.元组中元素可以使不同类型。元组也可以作为另一个元组的元素，只是此时作为元素的元组要加上括号来区分，从而避免歧义！

t3 = 123456,("hello","da")

print t3

2.元组中各元素存在先后关系，可以通过索引访问元组中元素。

如:t3[0]

3.元组定义后不能更改，也不能删除。

例如：t3[0] = 456 会报错！

4.与字符串相似：

1).与字符串类型相似，可以通过索引区间来访问元组中部分元素。t[1:]

2).与字符串一样，元组之间可以使用+号和*号进行运算。

5.python中的tuple元组是不能修改的，那么一个不能修改的tuple元组类型有什么意义呢？

因为元组中的元素不可改变，那么程序运行也就更加安全，如果不考虑代码的安全性，只考虑代码的灵活性那么可以使用列表类型来代替元组类型。

三、列表的概念

列表与元组类型类似，都表示一系列元素；

1.列表(list)是有序的元素集合

2.列表元素可以通过索引访问单个元素

a = [0,1,2,3,4,5,6,7,8,9]

print(a)

3.列表与元组相似

1)列表中每个元素类型可以不一样

2)访问列表中元素时可以采用索引形式

4.列表与元组不同

列表的大小没有限制可随时修改,所以在一般的程序设计中，我们用列表来代替元组

一、列表和元组的区别

    列表是动态数组，它们可变且可以重设长度（改变其内部元素的个数）。
    元组是静态数组，它们不可变，且其内部数据一旦创建便无法改变。
    元组缓存于Python运行时环境，这意味着我们每次使用元组时无须访问内核去分配内存。

这些区别结实率两者在设计哲学上的不同：

    列表可被用于保存多个互相独立对象的数据集合
    元组用于描述一个不会改变的事务的多个属性
-三元运算

三元运算符是根据条件，选择符合条件的一个值，通常是二选一，使用if …else格式，目的是简化if语句
一、三元运算
格式：结果 + if 条件 + else +结果

a = 4
print(a+1) if a>0 else print(a+2)  
#val = 1 if 条件成立 else 2   #条件成立，执行前面的代码，不成立，执行后面的代码



解析：如果条件为真，把if前面的值赋值给变量，否则把else后面的值赋值给变量
二、三元运算嵌套
格式：结果1 + if 条件1 + else + 结果2 + if 条件2 + else + 结果3

代码如下（示例）：

 x if x>y  else  a if a>b else b



解析：嵌套三元运算符执行顺序从左到右，满足if后的条件，则执行if前的代码，不满足则执行else后的代码，如果else后还嵌套三元运算符，则将else后的三元运算看作是一个整体，继续对if条件进行判断，再决定执行哪一部分


-Match和search的区别

match()和search()都只匹配一个结果，但是match()是从[字符串](https://so.csdn.net/so/search?q=字符串&spm=1001.2101.3001.7020)的开头开始匹配的，如果匹配的字符不是在开头处，那么它将会报错，匹配成功返回结果，没有返回None。而search()是从头开始匹配，匹配整一个字符串得出结果。

-Pass

Python中的pass是空语句，pass不做任何事情，表示一个占位符，一般用作占位语句。能够保证程序的代码结构正确

-闭包

一、定义

python中的闭包从表现形式上定义为：如果在一个内部函数里，对在外部作用域（但不是在全局作用域）的变量进行引用，那么内部函数就被认为闭包，这个定义是相对直白的，好理解的。

def demo():
    x=10
    def demo1(y):
        return x+y
    return demo1
c=demo()
d=c(6)
print(d)



结合这段简单的代码和定义来说明闭包：
如果在一个内部函数里：demo1(y)就是这个内部函数，
对在外部作用域（但不是在全局作用域）的变量进行引用：x就是被引用的变量，x在外部作用域demo()函数里面，但不在全局作用域里，
则这个内部函数demo1就是一个闭包。

再稍微讲究一点的解释是，闭包=函数块+定义函数时的环境，adder就是函数块，x就是环境，当然这个环境可以有很多，不止一个简单的x。

nonlocal可以改变外部变量值

二、使用闭包注意事项
1、闭包中是不能修改外部作用域的局部变量的

def foo():
    m=0
    def foo1():
        m=1
        print(m)

    print(m)
    foo1()

foo()



输出0、1

从执行结果可以看出，虽然在闭包里面也定义了一个变量m，但是其不会改变外部函数中的局部变量m。
2、以下这段代码是在python中使用闭包时一段经典的错误代码

def fun():
    a = 1
    def fun1():
        a = a + 1
        return a
    return fun1

result=fun()
data=result()
print(data)



输出：

Traceback (most recent call last):
  File "D:\Integrated_platform\vehicle_pool_for_sale_pytest\123.py", line 151, in <module>
    data=result()
  File "D:\Integrated_platform\vehicle_pool_for_sale_pytest\123.py", line 146, in fun1
    a+=1
UnboundLocalError: local variable 'a' referenced before assignment



如果在一个范围内，对一个变量进行赋值，那么这个变量就会被认为是局部变量，就像在func()这个函数范围之内，我们对a这个变量进行了重新赋值：a+=1，那么编译器就会认为a这个变量是一个局部变量，而这个赋值表达式实际上是从右向左进行的，也就是说，在进行a+1运算的时候，a并没有被定义，所以就爆出了这个UnboundLocalError错误。
解决：

def fun():
    a=1
    def fun1():
        nonlocal a
        a+=1
        return a
    return fun1

result=fun()
data=result()
print(data)



三、使用场景（后面的文章会详细讲解）
1、做数据锁定
2、实现装饰器

-单例模式的实现

python 6种方法实现单例模式

这篇文章主要介绍了python 6种方法实现单例模式，帮助大家更好的理解和使用python，感兴趣的朋友可以了解下



单例模式是一个软件的设计模式，为了保证一个类，无论调用多少次产生的实例对象，都是指向同一个内存地址，仅仅只有一个实例（只有一个对象）。

实现单例模式的手段有很多种，但总的原则是保证一个类只要实例化一个对象，下一次再实例的时候就直接返回这个对象，不再做实例化的操作。所以这里面的关键一点就是，如何判断这个类是否实例化过一个对象。

这里介绍两类方式：

- 一类是通过模块导入的方式；
- 一类是通过魔法方法判断的方式；

```
基本原理：

`-` `第一类通过模块导入的方式，借用了模块导入时的底层原理实现。`
`-` `当一个模块（py文件）被导入时，首先会执行这个模块的代码，然后将这个模块的名称空间加载到内存。`
`-` `当这个模块第二次再被导入时，不会再执行该文件，而是直接在内存中找。`
`-` `于是，如果第一次导入模块，执行文件源代码时实例化了一个类，那再次导入的时候，就不会再实例化。`
`-` `第二类主要是基于类和元类实现，在``'对象'``的魔法方法中判断是否已经实例化过一个对象`
`-` `这类方式，根据实现的手法不同，又分为不同的方法，如：``-` `通过类的绑定方法；通过元类；通过类下的__new__；通过装饰器（函数装饰器，类装饰器）实现等。
```

下面分别介绍这几种不同的实现方式，仅供参考实现思路，不做具体需求。

通过模块导入

```
cls_singleton.py

class Foo(object):
  pass
# class 类，instance 实例
instance = Foo()

test.py

import cls_singleton

obj1 = cls_singleton.instance
obj2 = cls_singleton.instance
print(obj1 is obj2)

原理：模块第二次导入从内存找的机制

```



通过类的绑定方法

```
class Student:
  _instance = None  # 记录实例化对象

  def __init__(self, name, age):
    self.name = name
    self.age = age

  @classmethod
  def get_singleton(cls, *args, **kwargs):
    if not cls._instance:
      cls._instance = cls(*args, **kwargs)
    return cls._instance

stu1 = Student.get_singleton('jack', 18)
stu2 = Student.get_singleton('jack', 18)
print(stu1 is stu2)
print(stu1.__dict__, stu2.__dict__)

原理：类的绑定方法是第二种实例化对象的方式，

第一次实例化的对象保存成类的数据属性 _instance，

第二次再实例化时，在get_singleton中判断已经有了实例对象，直接返回类的数据属性 _instance

```

> 补充：这种方式实现的单例模式有一个明显的bug；bug的根源在于如果用户不通过绑定类的方法实例化对象，而是直接通过类名加括号实例化对象，那这样不再是单利模式了。

通过魔法方法__new__

```
class Student:

  _instance = None

  def __init__(self, name, age):
    self.name = name
    self.age = age

  def __new__(cls, *args, **kwargs):
    # if cls._instance:
    #   return cls._instance            # 有实例则直接返回
    # else:
    #   cls._instance = super().__new__(cls)    # 没有实例则new一个并保存
    #   return cls._instance            # 这个返回是给是给init，再实例化一次，也没有关系

    if not cls._instance:               # 这是简化的写法，上面注释的写法更容易提现判断思路
      cls._instance = super().__new__(cls)
    return cls._instance

 


stu1 = Student('jack', 18)
stu2 = Student('jack', 18)
print(stu1 is stu2)
print(stu1.__dict__, stu2.__dict__)

原理：和方法2类似，将判断的实现方式，从类的绑定方法中转移到类的__new__中

归根结底都是 判断类有没有实例，有则直接返回，无则实例化并保存到_instance中。

```

> 补充：这种方式可以近乎完美地实现单例模式，但是依然不够完美。不完美的地方在于没有考虑到并发的极端情况下，有可能多个线程同一时刻实例化对象。关于这一点的补充内容在本文的最后一节介绍(!!!进阶必会)。

通过元类**

```
class Mymeta(type):

  def __init__(cls, name, bases, dic):
    super().__init__(name, bases, dic)
    cls._instance = None                 # 将记录类的实例对象的数据属性放在元类中自动定义了

  def __call__(cls, *args, **kwargs):            # 此call会在类被调用（即实例化时触发）
    if cls._instance:                # 判断类有没有实例化对象
      return cls._instance
    else:                        # 没有实例化对象时，控制类造空对象并初始化
      obj = cls.__new__(cls, *args, **kwargs)
      obj.__init__(*args, **kwargs)
      cls._instance = obj                # 保存对象，下一次再实例化可以直接返回而不用再造对象
      return obj


class Student(metaclass=Mymeta):
  def __init__(self, name, age):
    self.name = name
    self.age = age


stu1 = Student('jack', 18)
stu2 = Student('jack', 18)
print(stu1 is stu2)
print(stu1.__dict__, stu2.__dict__)

原理：类定义时会调用元类下的__init__，类调用(实例化对象)时会触发元类下的__call__方法

类定义时，给类新增一个空的数据属性，

第一次实例化时，实例化之后就将这个对象赋值给类的数据属性；第二次再实例化时，直接返回类的这个数据属性

和方式3的不同之处1：类的这个数据属性是放在元类中自动定义的，而不是在类中显示的定义的。

和方式3的不同之处2：类调用时触发元类__call__方法判断是否有实例化对象，而不是在类的绑定方法中判断

```

函数装饰器

```
def singleton(cls):
  _instance_dict = {}                # 采用字典，可以装饰多个类，控制多个类实现单例模式

  def inner(*args, **kwargs):
    if cls not in _instance_dict:
      _instance_dict[cls] = cls(*args, **kwargs)
    return _instance_dict.get(cls)
  return inner


@singleton
class Student:
  def __init__(self, name, age):
    self.name = name
    self.age = age

def __new__(cls, *args, **kwargs):   # 将方法3的这部分代码搬到了函数装饰器中

if not cls._instance:

cls._instance = super().__new__(cls)

return cls._instan

stu1 = Student('jack', 18)
stu2 = Student('jack', 18)
print(stu1 is stu2)
print(stu1.__dict__, stu2.__dict__)
```

类装饰器

```
class SingleTon:
  _instance_dict = {}

  def __init__(self, cls_name):
    self.cls_name = cls_name

  def __call__(self, *args, **kwargs):
    if self.cls_name not in SingleTon._instance_dict:
      SingleTon._instance_dict[self.cls_name] = self.cls_name(*args, **kwargs)
    return SingleTon._instance_dict.get(self.cls_name)


@SingleTon                     # 这个语法糖相当于Student = SingleTon(Student),即Student是SingleTon的实例对象
class Student:
  def __init__(self, name, age):
    self.name = name
    self.age = age

stu1 = Student('jack', 18)
stu2 = Student('jack', 18)
print(stu1 is stu2)
print(stu1.__dict__, stu2.__dict__)

原理：在函数装饰器的思路上，将装饰器封装成类。

程序执行到与语法糖时，会实例化一个Student对象，这个对象是SingleTon的对象。

后面使用的Student本质上使用的是SingleTon的对象。

所以使用Student('jack', 18)来实例化对象，其实是在调用SingleTon的对象，会触发其__call__的执行

所以就在__call__中，判断Student类有没有实例对象了。

```

!!!进阶必会 

本部分主要是补充介绍多线程并发情况下，多线程高并发时，如果同时有多个线程同一时刻（极端条件下）事例化对象，那么就会出现多个对象，这就不再是单例模式了。

解决这个多线程并发带来的竞争问题，第一个想到的是加互斥锁，于是我们就用互斥锁的原理来解决这个问题。

解决的关键点，无非就是将具体示例化操作的部分加一把锁，这样同时来的多个线程就需要排队。

这样一来只有第一个抢到锁的线程实例化一个对象并保存在_instance中，同一时刻抢锁的其他线程再抢到锁后，不会进入这个判断if not cls._instance，直接把保存在_instance的对象返回了。这样就实现了多线程下的单例模式。

此时还有一个问题需要解决，后面所有再事例对象时都需要再次抢锁，这会大大降低执行效率。解决这个问题也很简单，直接在抢锁前，判断下是否有单例对象了，如果有就不再往下抢锁了（代码第11行判断存在的意义）。

```
import threading


class Student:

  _instance = None              # 保存单例对象
  _lock = threading.RLock()         # 锁

  def __new__(cls, *args, **kwargs):
     
    if cls._instance:           # 如果已经有单例了就不再去抢锁，避免IO等待
      return cls._instance
     
    with cls._lock:             # 使用with语法，方便抢锁释放锁
      if not cls._instance: 
        cls._instance = super().__new__(cls, *args, **kwargs)
      return cls._instance
```



-Python变量的作用域

        一 变量作用域
                1. Local（局部变量）
                2. Enclosed(嵌套)
                3. Global(全局)
                4. Built-in(内置)
        二 变量使用规则
        三 变量的修改
                1. global修改全局变量
                2. nonlocal修改外层函数变量

一 变量作用域

python变量作用域一般有4种，如下
1. Local（局部变量）

    Local（局部变量）：暂时的存在，依赖于创建该局部作用域的函数。函数存，则局部变量存，函数亡，则局部变量亡。如下

#局部作用域
def fun():
    b = 2
    print(b)#输出2
    
fun()
print(b)#报错



>
>2
>Traceback (most recent call last):
>File "C:/Users/admin/python-learning/python学习文件/python基础/CSDN.py", line 14, in <module>
>print(b)
>NameError: name 'b' is not defined




2. Enclosed(嵌套)

    Enclosed(嵌套)：一般是在函数中嵌套函数的时候，外层函数的变量作用域。

#Enclosed(嵌套)作用域
def fun1():
    b = 2
    print("这是fun1打印的：", b)
    def fun2():
        print("这是fun2打印的：", b)
    return fun2

temp = fun1()#调用fun1
temp()#调用fun2



在这里函数fun2里面并没有定义变量b,但是它能够引用外层函数fun1定义的b变量，此时变量b的作用域就是Enclosed. 结果如下

>
>这是fun1打印的： 2
>这是fun2打印的： 2




3. Global(全局)

    Global(全局)：一般模块文件顶层声明的变量具有全局作用域，从外部来看，模
    块的全局变量就是一个模块对象的属性，仅限于单个模块文件中。

#Global(全局)作用域
a = 2
def fun1():
    print("这是fun1打印的：", a)

fun1()
print(a)



>
>这是fun1打印的： 2
>2




4. Built-in(内置)

    Built-in(内置)：解释器内置的变量，比如int, str等。

二 变量使用规则

python中变量的使用遵循上面的四种规则，即LEGB规则，如下
在这里插入图片描述

    创建变量时从上往下创建，搜索变量时从下往上搜索。
    创建变量时，下层变量会覆盖上层同名变量，但不会改变上层变量的值，除非使用gloable和nonlocal关键字声明


三 变量的修改
1. global修改全局变量

一般全局变量一经定义后几乎是不用改的，也不允许在局部修改全局变量，除非使用Global关键字声明。如下

a = "我是全局变量"
def fun1():
    a += "但是是修改后的全局变量"#修改全局变量
    print(a)
    
fun1()



>
>UnboundLocalError: local variable 'a' referenced before assignment



可以看到，当我们试图在函数fun1创建的局部作用域内改变全局变量a就会报错，但如果在修改之前使用global关键字声明时，就会正常修改外部的全局变量a,如下

a = "我是全局变量"
def fun1():
    global a    #使用global声明
    a += "，但是是修改后的全局变量"#修改全局变量
    print(a)

print(a)#fun1函数修改前
fun1()
print(a)#fun1函数修改后



>
>我是全局变量
>我是全局变量，但是是修改后的全局变量
>我是全局变量，但是是修改后的全局变量




2. nonlocal修改外层函数变量

在函数中嵌套函数时，嵌套在里面的函数创建的作用域内一般也是不允许改变外层函数变量的值的，除非是nonlocal关键字声明。如下

#不使用nonocal声明，修改外层函数变量值
def fun1():
    a = "我是fun1的变量"

    def fun2():
        a += ",但是我是修改后的"
        print(a)
    return fun2

temp = fun1()#调用fun1
temp()#调用fun2



>
>UnboundLocalError: local variable 'a' referenced before assignment



可以看到，报错和在函数内不使用global修改全局变量报的错是一样的，当使用nonlocal声明后再修改就不会报错了，如下

#使用nonocal声明，修改外层函数变量值
def fun1():
    a = "我是fun1的变量"
    print(a)#修改前
    def fun2():
        nonlocal a  #使用nonlocal声明
        a += ",但是我是修改后的"
        print(a)#修改后
    return fun2

temp = fun1()#调用fun1
temp()#调用fun2



>
>我是fun1的变量
>我是fun1的变量,但是我是修改后的



可以看到是正常修改的。
-函数调用，参数的传递方式是值传递还是引用传递

结论：python函数的参数传递是引用传递。

在Python中一切都是对象，变量总是存放对象引用。当没有变量指向对象时，这个对象便进入了垃圾收集过程。Python的“动态类型”机制，负责检查变量的对象引用适用操作。如果该对象不适用该操作，则会直接报错。一句话”变量无类型，对象有类型 “

-Python23差异常考

python2和python3分别是python的两个版本
1.print方法

python2既可以使用小括号的方式，也可以使用一个空格来分隔打印内容，比如 print ‘hi’；

python3使用print必须要用小括号包含打印内容，比如print(“hi”)
2.编码

python2中使用ASCII编码，需要更改更改字符集（添加coding:utf-8）才能正常支持中文

python3中使用utf-8，支持中文
3.除法运算

python2中 / 除法规则是整除，结果为整数，把小数部分完全忽略掉，要想真除需要转为浮点数再除

 //整数相除，与/相同，取整

python3 / 是真除，会得到小数

// 是地板除，取整
4.数据类型

python2整型有长整形和整型

python3只有整型，范围是无限大
5.python3中有f格式化，python2没有
6.range方法

python3中没有xrange方法，只有range方法

python2中range(1,10)返回列表，python3返回range可迭代对象，节约内存
7.市场差异

python2：官方通知python2 2020开始不再维护，但企业很多代码都是python2,python2有很大的用户基群故会出现历史遗留问题，
需要很长时间的迁移过度到python3
python3：最新版本，但目前市场使用量不大
8.字符串

python2中Unicode表示字符串序列，str表示字节序列

python3中str表示字符串序列，byte表示字节序列
9.input

在Python2中raw_input()和input( )，两个函数都存在，其中区别为：

1）raw_input()：将所有输入作为字符串看待，返回字符串类型

2）input()：只能接收"数字"的输入，在对待纯数字输入时具有自己的特性，它返回所输入的数字的类型（int, float ）

在Python3中raw_input()和input( )进行了整合，去除了raw_input()，仅保留了input()函数，其接收任意任性输入，将所有输入默认为字符串处理，并返回字符串类型。
10.异常处理

1）Python2中捕获异常的语法为except exc, var，Python3中捕获异常的语法为except exc as var，使用语法except (exc1, exc2) as var可以同时捕获多种类别的异常。 Python 2.6已经支持这两种语法。

2）在Python2时代，所有类型的对象都是可以被直接抛出的，在Python3时代，只有继承自BaseException的对象才可以被抛出。

3）Python2中触发异常可以用raise IOError, "file error"或raise IOError(“file error”)两种方式，Python3中触发异常只能用raise IOError("file error”)。

4）异常StandardError 被Python3废弃，统一使用Exception

5）在Python2时代，异常在代码中除了表示程序错误，还经常做一些普通控制结构应该做的事情，在Python3中可以看出，设计者让异常变的更加专一，只有在错误发生的情况才能去用异常捕获语句来处理。
11.比较符

Python2 中任意两个对象都可以比较，11 < 'test’返回True

Python3中只有同一数据类型的对象可以比较，11 < 'test’报错，需要调用正则判断

import re  
11 < int('test') if re.compile('^[0-9]+$').match('test') else 0 



12.包的定义

Python2：文件夹中必须有_ _ init _ _.py文件

Python3：不需要有_ _ init _ _.py文件
13.打开文件

Python2中使用file( … ) 或 open(…)

Python3中只能使用open(…)

-Python函数

-Python异常机制

 一、对异常的理解

​    1、什么是异常

  异常即“与正常情况不同”，何为正常？正常便是解释器在解释代码时，我们所编写的代码符合解释器定义的规则，即为正常，当解释器发现某段代码符合语法但有可能出现不正常的情况时，解释器便会发出一个事件，中断程序的正常执行。这个中断的信号便是一个异常信号。所以，总体解释就是，在解释器发现到程序出现错误的时候，则会产生一个异常，若程序没有处理，则会将该异常抛出，程序的运行也随之终止。我们可以在一个空白的.py文件中写一句int（“m”），运行后结果如下。

![img](https://www.qycn.com/uploads/allimg/2021/12/4111433108798638464.png)



  这一串字体为解释器抛出的一系列错误信息，因为int()传入的参数只支持数字字符串和数字，显然‘m’不属于数字字符串传入参数错误所以解释器报“valueError”的错误。

​    2、错误和异常的区别

  对于python错误的概述：它指的是代码运行前的语法或逻辑错误。拿常规语法错误来说，当我们编写的代码过不了语法检测时，则会直接出现语法错误，必须在程序执行前就改正，不然写的代码将毫无意义，代码是不运行的，也无法捕获得到。举个例子，在.py文件输入if a = 1 print(“hello”)，输出结果如下：

```
  Traceback (most recent call last):
  	File "E:/Test_code/test.py",line 1
    	if a = 1 print("hello")
                ^SyntaxError: invalid syntax
```

  函数 print() 被检查到有错误，是它前面缺少了一个冒号 :  ，所以解析器会复现句法错误的那行代码，并用一个小“箭头”指向行里检测到的第一个错误，所以我们可以直接找到对应的位置修改其语法。当然除了语法错误，还有很多程序奔溃的错误，如内存溢出等，这类错误往往比较隐蔽。

   相比于错误，python异常主要在程序执行过程中，程序遇见逻辑或算法问题，这时解释器如果可以处理，则没问题，如果处理不了，便直接终止程序，便将异常抛出，如第1小点的int(‘m’)例子，因为参数传入错误导致程序出错。这种因为逻辑产生的异常五花八门，还好我们的解释器都内置好了各种异常的种类，让我们知道是什么样的异常出现，好让我们“对症下药”。
   这里注意一点，上述语法错误是可识别的错误，所以解释器也会默认抛出一个SyntaxError异常信息反馈给程序员。所以本质上大部分错误都是可被输出打印的，只是因为错误代码不运行，也就没法处理，所以捕获错误的异常信息就变得没意义。



​    3、常见python异常种类

  这里贴上我们在写代码时最常见的异常类型，如果遇到其他种类的异常，当然是选择白度啦~

| 异常名称                      | 名称解析                                           |
| ----------------------------- | -------------------------------------------------- |
| BaseException                 | 所有异常的基类                                     |
| SystemExit                    | 解释器请求退出                                     |
| KeyboardInterrupt             | 用户中断执行(通常是输入^C)                         |
| Exception                     | 常规错误的基类                                     |
| StopIteration                 | 迭代器没有更多的值                                 |
| GeneratorExit                 | 生成器(generator)发生异常来通知退出                |
| StandardError                 | 所有的内建标准异常的基类                           |
| ArithmeticError               | 所有数值计算错误的基类                             |
| FloatingPointError            | 浮点计算错误                                       |
| OverflowError                 | 数值运算超出最大限制                               |
| ZeropisionError               | 除(或取模)零 (所有数据类型)                        |
| AssertionError                | 断言语句失败                                       |
| AttributeError                | 对象没有这个属性                                   |
| EOFError                      | 没有内建输入,到达EOF 标记                          |
| EnvironmentError              | 操作系统错误的基类                                 |
| IOError                       | 输入/输出操作失败                                  |
| OSError                       | 操作系统错误                                       |
| WindowsError                  | 系统调用失败                                       |
| ImportError                   | 导入模块/对象失败                                  |
| LookupError                   | 无效数据查询的基类                                 |
| IndexError                    | 序列中没有此索引(index)                            |
| KeyError                      | 映射中没有这个键                                   |
| MemoryError                   | 内存溢出错误(对于Python 解释器不是致命的)          |
| NameError                     | 未声明/初始化对象 (没有属性)                       |
| UnboundLocalError             | 访问未初始化的本地变量                             |
| ReferenceError                | 弱引用(Weak reference)试图访问已经垃圾回收了的对象 |
| RuntimeError                  | 一般的运行时错误                                   |
| NotImplementedError           | 尚未实现的方法                                     |
| SyntaxError Python            | 语法错误                                           |
| IndentationError              | 缩进错误                                           |
| TabError Tab                  | 和空格混用                                         |
| SystemError                   | 一般的解释器系统错误                               |
| TypeError                     | 对类型无效的操作                                   |
| ValueError                    | 传入无效的参数                                     |
| UnicodeError Unicode          | 相关的错误                                         |
| UnicodeDecodeError Unicode    | 解码时的错误                                       |
| UnicodeEncodeError Unicode    | 编码时错误                                         |
| UnicodeTranslateError Unicode | 转换时错误                                         |
| Warning                       | 警告的基类                                         |
| DeprecationWarning            | 关于被弃用的特征的警告                             |
| FutureWarning                 | 关于构造将来语义会有改变的警告                     |
| OverflowWarning               | 旧的关于自动提升为长整型(long)的警告               |
| PendingDeprecationWarning     | 关于特性将会被废弃的警告                           |
| RuntimeWarning                | 可疑的运行时行为(runtime behavior)的警告           |
| SyntaxWarning                 | 可疑的语法的警告                                   |
| UserWarning                   | 用户代码生成的警告                                 |

​    二、python五大异常处理机制

  我们明白了什么是异常后，那么发现异常后怎么处理，便是我们接下来要解决的问题。这里将处理异常的方式总结为五种。

​    1、默认异常处理机制

  “默认”则说明是解释器默认做出的行为，如果解释器发现异常，并且我们没有对异常进行任何预防，那么程序在执行过程中就会中断程序，调用python默认的异常处理器，并在终端输出异常信息。刚才举过的例子：int(“m”)，便是解释器因为发现参数传入异常，这种异常解释器“无能为力”，所以它最后中断了程序，并将错误信息打印输出，告诉码农朋友们：你的程序有bug！！！

​    2、try…except…处理机制

  我们把可能发生错误的语句放在try语句里，用except来处理异常。每一个try，都必须至少有一个或者多个except。举一个最简单的例子如下，在try访问number的第500个元素，很明显数组越界访问不了，这时候解释器会发出异常信号：IndexError，接着寻找后面是否有对应的异常捕获语句except  ，如果有则执行对应的except语句，待except语句执行完毕后，程序将继续往下执行。如果没有对应的except语句，即用户没有处理对应的异常，这时解释器会直接中断程序并将错误信息打印输出。

```
number = 'hello'try：	
print(number[500])	
#数组越界访问except IndexError：	
print("下标越界啦！")except NameError：	
print("未声明对象！")print("继续运行...")
```

​    输出结果如下，因为解释器发出异常信号是IndexError，所以执行下标越界语句。

```
下标越界啦！
继续运行...
```

  为了解锁更多用法，我们再将例子改一下，我们依然在try访问number的第500个元素，造成访问越界错误，这里的except用了as关键字可以获得异常对象，这样子便可获得错误的属性值来输出信息。

```
number = 'hello'try：	print(number[500])	
#数组越界访问except IndexError as e：	
print(e)except Exception as e：	#万能异常
	print(e)except：			  	 
#默认处理所有异常
	print("所有异常都可处理")print("继续运行...")
```

​    输出结果如下所示，会输出系统自带的提示错误：string index out of  range，相对于解释器因为异常自己抛出来的一堆红色刺眼的字体，这种看起来舒服多了（能够“运筹帷幄”的异常才是好异常嘛哈哈哈）。另外这里用到“万能异常”Exception，基本所有没处理的异常都可以在此执行。最后一个except表示，如果没有指定异常，则默认处理所有的异常。

string index out of range继续运行...

​    3、try…except…finally…处理机制

  finally语句块表示，无论异常发生与否，finally中的语句都要执行完毕。也就是可以很霸气的说，无论产生的异常是被except捕获到处理了，还是没被捕获到解释器将错误输出来了，都统统要执行这个finally。还是原来简单的例子加上finally语句块如下，代码如下：

```
number = 'hello'try：	
print(number[500])	
#数组越界访问，抛出IndexError异常except IndexError：	
print("下标越界啦！")finally：	
print("finally！")print("继续运行...")		
#运行
```

​    结果如下，数据越界访问异常被捕获到后，先执行except 语句块，完毕后接着执行了finally语句块。因为异常被执行，所以后面代码继续运行。

```
下标越界啦！finally！
继续运行...
```

  对try语句块进行修改，打印abc变量值，因为abc变量没定义，所以会出现不会被捕获的NameError异常信号，代码如下所示：

```
number = 'hello'try：	print(abc)	
#变量未被定义，抛出NameError异常except IndexError：	
print("下标越界啦！")finally：	
print("finally！")print("继续运行...")	#不运行
```

​    结果如下，因为NameError异常信号没法被处理，所以解释器将程序中断，并将错误信息输出，但这过程中依然会执行finally语句块的内容。因为程序被迫中断了，所以后面代码不运行。

```
finally！	#异常没被捕获，也执行了finallyTraceback (most recent call last):
	File "E:/Test_code/test.py",line 3,in <module>
   		print("abc")NameError: name 'abc' is not defined
```

  理解到这里，相信:try…finally…这种机制应该也不难理解了，因为省略了except 捕获异常机制，所以异常不可能被处理，解释器会将程序中断，并将错误信息输出，但finally语句块的内容依然会被执行。例子代码如下：

```
number = 'hello'try：	
print(abc)	#变量未被定义，抛出NameError异常finally：	
print("finally！")print("继续运行...")
```

​    运行结果：

```
finally！	#异常没被捕获，也执行了finallyTraceback (most recent call last):
	File "E:/Test_code/test.py",line 3,in <module>
   		print("abc")NameError: name 'abc' is not defined
```

​    4、assert断言处理机制

  assert语句先判断assert后面紧跟的语句是True还是False，如果是True则继续往下执行语句，如果是False则中断程序，将错误信息输出。

```
assert 1 == 1 	#为True正常运行assert 1 == 2	#为False,终止程序，错误信息输出
```

​    5、with…as处理机制

  with…as一般常用在文件处理上，我们平时在使用类似文件的流对象时，使用完毕后要调用close方法关闭，很麻烦，这里with…as语句提供了一个非常方便且人性的替代方法，即使突发情况也能正常关闭文件。举个例子代码如下，open打开文件后将返回的文件流对象赋值给fd，然后在with语句块中使用。

```
with open('e:/test.txt','r') as fd:
	fd.read()
	print(abc)	#变量未被定义，程序终止，错误信息输出print("继续运行...")
```

  正常情况下，这里的with语句块完毕之后，会自动关闭文件。但如果with语句执行中发生异常，如代码中的变量未定义异常，则会采用默认异常处理机制，程序终止，错误信息输出，后面代码不被运行，文件也会正常关闭。

​    三、python异常自定义

  说了这么多异常的使用，终于可以回到我前言所说的在实际项目中存在的问题，即错误码的返回和数值的返回是冲突的（因为错误码也是数值），这时候便可以用异常的抛出和捕获来完成错误码的传递，即try和except  。但系统发生异常时抛出的是系统本身定义好的异常类型，跟自己的错误码又有何关系？这就是我接下来要说的内容：如何定义自己的异常并且能够被except 所捕获。

​    1、异常自定义

  实际开发中，有时候系统提供的异常类型往往都不能满足开发的需求。这时候就要使用到异常的自定义啦，你可以通过创建一个新的异常类来拥有自己的异常。自己定义的异常类继承自 Exception 类，可以直接继承，或者间接继承。栗子举起来：

```
class MyException(Exception):
    '''自定义的异常类'''
    def __init__(self, error_num):	#异常类对象的初始化属性
        self.error_num = error_num    def __str__(self):				
#返回异常类对象说明信息
        err_info = ['超时错误','接收错误']
        return err_info[self.error_num]
```

  该类继承自Exception 类，并且新类的名字为MyException，这跟前面我们一直在用的IndexError这个异常类一样，都是继承自Exception  类。__init__为构造函数，当我们创建对象时便会自动调用，__str__为对象说明信息函数，当使用print输出对象的时候，只要自己定义了__str__方法，那么就会打印从在这个方法中return的数据。

   即print(MyException(0))时，便可打印“超时错误”这个字符串，print(MyException(1))时，便可打印“接收错误”这个字符串，心细的你应该可以理解，MyException(x)为临时对象（x是传入错误码参数，这里只定义了0和1），与a = MyException(x)，a为对象一个样子 。  这里有一个好玩的说法，在python中方法名如果是__xxxx__()的，那么就有特殊的功能，因此叫做“魔法”方法。
   raise 唯一的一个参数指定了要被抛出的异常。它必须是一个异常的实例或者是异常的类（也就是 Exception 的子类），那么我们刚刚定义的异常类就可以用啦，举个简单例子：



​    2、异常抛出raise

  现在我们自己定义的错误定义好了（上面的MyException），怎么能像IndexError一样让except捕获到呢？于是乎raise关键字派上用场。我们在异常机制中用try…except时，一般都是将可能产生的错误代码放到try语句块中，这时出现异常则系统便会自动将其抛出，比如IndexError，这样except就能捕获到，所以我们只要将自定义的异常在需要的时候将其抛出即可。

```
try:
    raise MyException(0)	

自己定义的错误类，将错误码为0的错误抛出except MyException as e:

    print(e) 	  			
输出的是__str__返回的内容，即“超时错误”

```

  这里我直接将自己定义的错误抛出，…as e就是把得到的错误当成对象e，这样才可以访问其属性和方法。因为自己定义的错误中可以支持多个错误码（本质还是MyException这个错误），所以便可实现传入不同错误码就可打印不同错误信息。

​    3、异常捕获

  只要我们在try中将错误raise出来，except就可以捕获到（当然，异常必须是Exception 子类才能被捕获），将前面两个例子整合起来，代码如下：

```
'''错误码：0代表超时错误，1代表接收错误'''class MyException(Exception):
    '''自定义的异常类'''
    def __init__(self, error_num):	# 异常类对象的初始化属性
        self.error_num= error_num    def __str__(self):				

返回异常类对象指定错误码的信息

        err_info = ['超时错误','接收错误']
        return err_info[self.error_num]def fun()
    raise MyException(1) 			
抛出异常对象，传入错误码1def demo_main():

    try:
        fun()
    except MyException as ex:		
这里要使用MyException进行捕获，对象为ex

        print(ex) 	   				
输出的是__str__部分返回的内容，即“接收错误”

        print(ex.error_num) 		
输出的是__init__中定义的error_num，即1demo_main()							

#此处开始运行
```

  代码从demo_main函数开始执行，进入try语句块，语句块中的fun()函数模拟代码运行失败时raise 自定义的异常，except 正常接收后通过as 关键字得到异常对象，访问该异常对象，便可正常输出自定义的异常信息和自定义的错误码。

​    四、异常使用注意事项

​    1、不要太依赖异常机制

  python  的异常机制非常方便，对于信息的传递中十分好用（这里信息的传递主要有三种，参数传递，全局变量传递，以及异常机制传递），但滥用异常机制也会带来一些负面影响。过度使用异常主要表现在两个方面：①把异常和普通错误混淆在一起，不再编写任何错误处理代码，而是以简单地引发异常来代苦所有的错误处理。②使用异常处理来代替流程控制。例子如下：

```
buf = "hello"#例1：使用异常处理来遍历arr数组的每个元素try:
    i = 0    
    while True:
        print (buf [i])
        i += 1except:
    pass#例2：使用流程控制避免下标访问异常i = 0while i < len(buf ):
    print(buf [i])
    i += 1
```

  例1中假如循环过度便会下标访问异常，这时候把错误抛出，再进行一系列处理，显然是不可取的，因为异常机制的效率比正常的流程控制效率差，显然例2中简单的业务流程就可以避开这种错误。所以不要熟悉了异常的使用方法后，遇到这种简单逻辑，便不管三七二十一引发异常后再进行解决。对于完全己知的错误和普通的错误，应该编写处理这种错误的代码，增加程序的健壮性。只有对于外部的、不能确定和预知的运行时错误才使用异常。

​    2、不要在 try 块中引入太多的代码

  在 try 块里放置大量的代码，这看上去很“简单”，代码框架很容易理解，但因为 try 块里的代码过于庞大，业务过于复杂，就会造成 try 块中出现异常的可能性大大增加，从而导致分析异常原因    的难度也大大增加。
   而且当块过于庞大时，就难免在 try 块后紧跟大量的 except 块才可以针对不同的异常提供不同的处理逻辑。在同一个 try  块后紧跟大量的 except 块则需要分析它们之间的逻辑关系，反而增加了编程复杂度。所以，可以把大块的 try  块分割成多个小块，然后分别捕获并处理异常。

​    3、不要忽略捕获到的异常

  不要忽略异常！既然己捕获到异常，那么 except 块理应做些有用的事情，及处理并修复异常。except 块整个为空，或者仅仅打印简单的异常信息都是不妥的！具体的处理方式为：
 ①处理异常。对异常进行合适的修复，然后绕过异常发生的地方继续运行；或者用别的数据进行计算，以代替期望的方法返回值；或者提示用户重新操作，总之，程序应该尽量修复异常，使程序能恢复运行。
 ②重新引发新异常。把在当前运行环境下能做的事情尽量做完，然后进行异常转译，把异常包装成当前层的异常，重新传给上层调用者。
 ③在合适的层处理异常。如果当前层不清楚如何处理异常，就不要在当前层使用 except 语句来捕获该异常，让上层调用者来负责处理该异常。

!!!! 使用traceback模块打印异常信息

![image-20221018180417175](C:\Users\stormblinger\AppData\Roaming\Typora\typora-user-images\image-20221018180417175.png)

-Python性能分析与优化，GIL常考

Python性能分析和优化，GIL全局解释器锁

一、什么是Cpython GIL

GIL Global inteperter lock

Cpython 解释器的内存管理并不是线程安全的，为了保护多个线程对python对象的访问，cpython 使用了简单的锁机制来避免多个线程同时执行字节码 【python执行是先把源文件编译成字节码然后再执行的】

二、单核CPU的线程调度方式

单核CPU上调度多个线程任务，大家相互共享一个全局锁，谁在CPU执行，谁就占有这把锁，直到这个线程因为IO操作或者Timer  Tick到期让出CPU，没有在执行的线程就安静的等待着这把锁（除了等待之外，他们应该也无事可做）。下面这个图演示了一个单核CPU的线程调度方式：

![img](https://img-blog.csdnimg.cn/1351b170a0744f9c80d93ffd051cf5ca.png?x-oss-process=image/watermark,type_d3F5LXplbmhlaQ,shadow_50,text_Q1NETiBA5p2O5L-K55qE5Y2a5a6i,size_20,color_FFFFFF,t_70,g_se,x_16)

三、GIL 影响

限制了程序的多核执行

同一个时间只能有一个线程执行字节码

CPU密集程序难以利用多核优势

IO期间会释放GIL，对IO密集程序影响不大

四、规避GIL影响

区分CPU和IO密集型程序

CPU密集可以使用多进程+进程池

IO密集使用 多线程/协程

cython扩展

五、GIL的实现

执行指定数量的字节码之后就会判断是否有全局解释器锁，如果存在则释放，不存在则等待

说白了就是 每隔一段时间释放当前的全局解释器锁，让其他的线程获取到解释器锁并执行，然后再次获取

![img](https://img-blog.csdnimg.cn/ba6205726c824767b9a78c08cff115c4.png?x-oss-process=image/watermark,type_d3F5LXplbmhlaQ,shadow_50,text_Q1NETiBA5p2O5L-K55qE5Y2a5a6i,size_20,color_FFFFFF,t_70,g_se,x_16)

 

六、为什么有了GIL还要关注线程安全

6.1 非原子操作不是线程安全的

```python
import threading
import dis #分析字节码的包
n = [0]


def foo():
    n[0] = n[0]+1
    n[0] = n[0]+1

print(dis.dis(foo))

threads = []
for i in range(5000):
    t = threading.Thread(target=foo)
    threads.append(t)

print(threads)
for t in threads:
    t.start()
print(n)
```

![img](https://img-blog.csdnimg.cn/4665103057f542588ae84e62e77dcce9.png?x-oss-process=image/watermark,type_d3F5LXplbmhlaQ,shadow_50,text_Q1NETiBA5p2O5L-K55qE5Y2a5a6i,size_20,color_FFFFFF,t_70,g_se,x_16)



输出结果：

有可能是10000也有可能小于10000，原因是在进行 n[0] = n[0] + 1  的时候还没有执行完就被其他进程抢走了GIL，GIL的切换原理是执行指定数量的字节码之后或者执行IO操作就进行切换，所以非原子操作线程并安全， 我们可以通过加锁来保证线程安全

![img](https://img-blog.csdnimg.cn/37c9c6f93d53455da119af73ddfc2b87.png?x-oss-process=image/watermark,type_d3F5LXplbmhlaQ,shadow_50,text_Q1NETiBA5p2O5L-K55qE5Y2a5a6i,size_19,color_FFFFFF,t_70,g_se,x_16)

6.2 通过加锁保证线程安全

```python
import threading
import dis #分析字节码的包
n = [0]
lock = threading.lock()

def foo():
    with lock:
        n[0] = n[0]+1
        n[0] = n[0]+1

print(dis.dis(foo))

threads = []
for i in range(5000):
    t = threading.Thread(target=foo)
    threads.append(t)

print(threads)
for t in threads:
    t.start()
print(n)
```

![img](https://img-blog.csdnimg.cn/1a0360c313434049aac8c2da7dc1745a.png?x-oss-process=image/watermark,type_d3F5LXplbmhlaQ,shadow_50,text_Q1NETiBA5p2O5L-K55qE5Y2a5a6i,size_19,color_FFFFFF,t_70,g_se,x_16)

 

七、服务端性能优化措施

1、业务代码层：业务端代码优化，数据结构和算法优化

2、数据库层（后端和数据库打交道比较多，往往数据库容易成为瓶颈）: 增加索引，满查询优化，通过批量操作减少请求服务器次数，引入NoSQL（非结构化的数据）

3、缓存层：增加redis、memcache

4、网络：通过 批量操作，通过pinpline 技术减少请求服务器次数

4、异步任务：引入asyncio celery

5、多进程 ：gevent/多进层

-Python生成器与协程

1.生成器

要理解 python 中的协程，就必须先理解生成器，关于生成器的介绍可以先看下这篇文章：[Python中的迭代器与生成器](https://zhuanlan.zhihu.com/p/116141886)

这里总结一下可迭代对象、迭代器、生成器三者间的区别

① 可迭代对象就是可以通过 for 循环去遍历取值操作，判断一个对象是否可迭代就看这个对象里面有没有实现 __iter__() 或 __getitem__() 方法，前者的优先级更高。

② 迭代器则是在可迭代对象的基础上，实现了 __next__() 方法，迭代器支持通过 next() 去取值。

③ 生成器就是在迭代器的基础上，再实现了 yield 关键字，所以生成器既支持 for 循环遍历取值，也支持使用 next() 一个一个取值。

④ 因为生成器对象实现了迭代器协议的接口 __next__() 和 __iter__() ，所以所有的生成器都是迭代器。

⑤ 可迭代对象和迭代器，是将所有的值都生成存放在内存中。而生成器由于有这个 yield 关键字，它在每次取值的时候都会在这里将新的值返回，并阻塞等待下一次调用，它是在需要元素的时候才临时生成，所以更节省时间和空间。







2.创建生成器的两种方式

```js
① 使用生成器推导式
gen1 = (x * x for x in range(10))


② 使用 yield 关键字
def test_gen():
    yield 'j'
    yield 'a'
    yield 'y'
    yield 'e'
    


if __name__ == '__main__':
    from collections.abc import Generator
    print(isinstance(gen1, Generator))    # True
    print(isinstance(test_gen(), Generator))    # True
```









3.激活生成器

生成器创建后并不会执行任何代码逻辑，要触发运行需要先激活

① 使用 next() 

② 使用 generator.send(None)   # 生成器第一次调用只能传入 None 值

```js
def test_gen():
    yield 'j'
    yield 'a'
    yield 'y'
    yield 'e'
    

if __name__ == '__main__':
    gen = test_gen()
    print(gen.send(None))
    print(next(gen))
    print(gen.send(None))
    print(next(gen))

# 打印输出
j
a
y
e
```









4.生成器中的方法

① generator.send()    # send方法可以传递值进入生成器内部，同时还可以重启生成器执行到下一个 yield 位置

```js
def gen_func():
    url = yield "http://www.jayeblog.cn"
    print(url)
    yield 1
    yield 2
    

if __name__ == "__main__":
    gen = gen_func()
    url = gen.send(None)
    print(url)
    url = "http://www.zhihu.com"
    print(gen.send(url))   # send方法可以传递值进入生成器内部，同时还可以重启生成器执行到下一个 yield 位置


# 打印输出
http://www.jayeblog.cn
http://www.zhihu.com
1
```



② generator.close()   # 关闭生成器

```js
#1. 第一次启动生成器，并输出了第一个 yield 返回信息，然后调用 close 方法关闭了生成器，接着再次调用 next 方法报错: StopIteration
def gen_func():
    yield "http://www.jayeblog.cn"
    yield "1"
    yield "2"

if __name__ == "__main__":
    gen = gen_func()
    print(next(gen))
    gen.close()
    print(next(gen))

# 打印输出
http://www.jayeblog.cn
Traceback (most recent call last):
    print(next(gen))
StopIteration





#2. 我们来试着捕获一下异常，代码执行到第一个 print(next(gen)) 的时候，其实就停在了 yield "http://www.jayeblog.cn"处，
因此会输出 yield "http://www.jayeblog.cn"。然后当调用 gen.close()时，就捕获了 GeneratorExit 异常，我们直接 pass ，按理
说第二个 print(next(gen)) 应该打印输出 "1"，但是直接在 gen.close() 的地方报错：RuntimeError: generator ignored 
GeneratorExit，这是因为生成器已经关闭了，执行生成器相关的代码逻辑(含有 yield 的语句)就会报错，而其他语句不会报错

def gen_func():
    try:
        yield "http://www.jayeblog.cn"
    except GeneratorExit:
        pass
    yield "1"
    yield "2"


if __name__ == "__main__":
    gen = gen_func()
    print(next(gen))
    gen.close()
    print(next(gen))


# 打印输出
Traceback (most recent call last):
    gen.close()
RuntimeError: generator ignored GeneratorExit
http://www.jayeblog.cn


#3. 我们这里在生成器关闭后将 yield 相关语句注销掉再看一下，能够正常打印输出

def gen_func():
    try:
        yield "http://www.jayeblog.cn"
    except GeneratorExit:
        pass
    # yield "1"
    # yield "2"


if __name__ == "__main__":
    gen = gen_func()
    print(next(gen))
    gen.close()
    # print(next(gen))


# 打印输出
http://www.jayeblog.cn


所以当生成器调用close方法后，若其中还包含未执行的 yield 语句，则程序会在 gen.close()处报错：RuntimeError: generator 
ignored GeneratorExit 如果不包含 yield 语句，而又继续执行了 next(gen)，就会在 next(gen) 处报错：StopIteration，
也就是生成器迭代越界。
```



③ generator.throw()   # throw方法就是往生成器里面扔一个异常

```js
def gen_func():
    try:
        yield "http://www.jayeblog.cn"
    except Exception as e:
        print(f'--{e}--')
        print('0')
    yield 1
    yield 2


if __name__ == "__main__":
    gen = gen_func()
    print(next(gen))
    gen.throw(Exception, "throw method test")
    print(next(gen))


# 打印输出
http://www.jayeblog.cn
--throw method test--
0
2

解释一下为什么这里第二个 print(next(gen)) 打印的值是 2 而不是 1，当我们第一次调用 next(gen)之后就会 print 出第一个 yield
的值，然后挂起，当调用 throw 方法传入一个异常之后，这个异常被捕捉到，但是 throw 方法会继续执行生成器里后面的语句，
它会驱动生成器继续执行到下一个 yield 才返回，也就是 yield 1 这里，所以当再次执行 next(gen)，就到了yield 2
```



5.生成器的状态

生成器的生命周期中，会有以下四个状态

```js
GEN_CREATED    # 已创建，还未被激活
GEN_RUNNING    # 正在执行，只有在多线程任务中才能看到
GEN_SUSPENDED  # 在 yield 表达式处暂停，可以理解为挂起中
GEN_CLOSED     # 执行结束

def test_gen():
    yield 'j'
    yield 'a'
    yield 'y'
    yield 'e'


if __name__ == '__main__':
    from inspect import getgeneratorstate
    gen = test_gen()
    print(getgeneratorstate(gen))    # GEN_CREATED
 
    gen.send(None)                   # j
    print(getgeneratorstate(gen))    # GEN_SUSPENDED

    next(gen)                        # a
    print(getgeneratorstate(gen))    # GEN_SUSPENDED

    gen.close()
    print(getgeneratorstate(gen))    # GEN_CLOSED
```

6.协程

① 什么是协程？

a.可以简单的理解为：一个可以暂停的函数，并且可以向暂停的地方传入值。

b.它是一种比线程更加轻量级的存在，它在单线程里面就能实现任务的切换。正如一个进程里面可以包含多个线程一样，一个线程里面也可以包含多个协程。

c.进程和线程的切换都是由操作系统控制，切换过程都是 用户态 ——> 内核态 ——> 用户态。而协程则完全由程序控制，切换过程就在用户态中完成，它也不需要锁，所以协程的开销远远小于线程。



② 为什么会有协程的出现？

我们思考一下，当我们想要写一个并发程序，可能会遇到的问题

a.采用常规的同步编程去实现异步效果难度极高，因为函数无法中断，如果一个函数中有阻塞的方法，那么这个函数所在的任务就会全部阻塞。

b.采用多线程的方式，线程相对于进程来说已经轻量化了，但它仍然是由操作系统来创建和切换，因为同步锁和线程状态的频繁切换，导致消耗大并发也上不去。

c.所以总的来说，函数写法简单，但是满足不了多任务的并发。线程消耗大，并发上不去。所以这个时候就有人提出一种概念：有没有一种方法可以兼顾这两者的优点，又可以解决两者的缺点呢？既能用同步的方式编写异步代码，又不需要多线程那样那么大的开销，那么这个东西就是协程了。



③ 从生成器到协程

通过上面的介绍，我们已经看到了生成器它可以暂停、产值、传值、关闭等操作，它已经初步达到了我们对协程的要求，但是它还差另一个方法 yield  from，它可以打开一个双向通道，把最外层的调用方与最内层的子生成器连接起来，然后这两者就可以发送值和接收值了，这个我们下一章再仔细介绍

-位和字节的关系？

​    [python基础：1.位、字节、字的关系](https://www.cnblogs.com/meloncodezhang/p/11271877.html)   

1.位，简称b，或bit，比特，数据存储的最小单位。每个[二进制](https://so.csdn.net/so/search?q=二进制&spm=1001.2101.3001.7020)数字0或1就是一个位(bit)，网络通信常用bps，bit per second ，每秒传输多少位

2.字节，简称byte， 1byte = 8b，电脑下载或网速常用Bps, Byte per second ，每秒传输多少字节。硬盘容量也指的是字节。

1byte=8bit

1kb = 1024byte=2^10byte

1mb = 1024kb=2^20byte

1gb = 2014mb=2^30byte

一般来说：与传输速度相关的b一般都是指bit，与容量相关的b一般都是指byte。

3.字，简称word，1word = 2byte，一个英文字母和数字占一个字节，一个汉字占连个字节

b、B、KB、MB、GB 的关系？

请至少列举5个 PEP8 规范（越多越好）

```
一、缩进和对齐
1.语法缩进：语法上的缩进使用4个空格（参数对齐等不一定要用4个空格），不要混用制表符与空格，Python2程序在命令行运行时，使用-t可以发出制表符与空格混用的警告，而使用-tt就会使这些警告变成错误提示了
2.行宽：代码行宽限制在79个字符（也可以是99个字符），文档和注释限制在72个字符
3.对齐：当圆括号、方括号和花括号中的元素需要换行时，元素应该垂直对齐，而且如果下一条语句需要缩进时，比如if的条件语句和要执行的代码块，这些换行的元素应该使用更多的缩进来区分下面的缩进
4.换行：代码换行时应该优先使用圆括号、方括号和花括号中的隐式续行，视情况使用反斜杠来进行换行
5.二元运算符：在二元云算法的换行时推荐以二元运算符作为新行的开始
6.多条语句同行：即使是简单的语句，即使可以使用分号，但是不推荐写在同一行，比如再简单的if/for/while语句也应该分行写

二、空行
1.顶级定义：顶级函数和类定义的前后使用两个空行隔开
2.类方法：类中方法的定义使用一个空行隔开
3.逻辑分段：函数中的功能组和逻辑段使用空行来隔开（视情况灵活运用）

三、import语句
1.import *和from xxx import *：这种通配符星号的用法应该尽量避免使用
2.import xxx：这种语句有多个时，应该分开导入，不推荐使用import xxx, yyy
3.from xxx import x, xx：这种形式可以一次导入多个而不用分行
4.普通导入：导入应该在文档字符串和注释之后，在全局变量和常量定义之前
5.导入顺序：导入顺序应该是标准库，三方库，以及本地模块，且需要加空行分隔
6.导入路径：导入应该尽量使用绝对路径，或使用显式的相对路径也是可行的（如：from . import xxx)，尽量避免隐式的相对路径
7.双下划线变量导入：在像__version__、__author__等模块级变量的导入应该在文档和注释之后，在import语句之前

四、空格
1.括号等之后的空格：避免紧跟在括号、中括号和大括号之后的空格，例如：func( list_[ 1 ], { 'age': 18 })。应该省去不必要的空格func(list_[1], {'age': 18})
2.逗号等之前的空格：避免紧跟在逗号、分号和冒号之前的空格，例如：a , b = b , a。应该省去不必要的空格a, b = b, a
3.切片中的空格：切片的冒号左右两边应该有相同的空格，切片的下标如果是数字或变量，建议冒号两边不用空格，如果下标是多个变量的表达式或者函数表达式，则建议冒号两边使用一个空格分隔
4.行尾的空格：避免在行尾添加空格，比如在换行符反斜杠后有空格的话，那这个反斜杠就不是换行符了，因为行尾是空格而不是反斜杠
5.二元运算符：除了函数传参和函数指定默认值等特殊情况外，应该总是在二元运算符的两边添加一个空格，如果一个表达式有多个二元运算符（如：+-*/），那么高优先级的二元运算符两边不用空格，低优先级两边添加一个空格，如：x = a*b + c/d

五、注释
1.修改注释：修改代码时一定修改对应的注释，千万不要留下与代码不对应的，甚至是错误的注释，视情况甚至可以删掉注释也不留错误的注释
2.行注释：使用一个#和一个空格开始，并且与注释的代码具有相同缩进，如果需要使用行注释写多段意思的注释，可以使用一个空行注释（即这一行只有一个#）来分隔不同意思的段落
3.代码行之后的注释：应该与前面的代码间隔至少两个空格，然后也是以使用一个#和一个空格开始，但是这种注释应该尽量少用，不必要的话就不用
4.文档注释：应该为所有公共的模块、函数、类和方法编写文档注释，一般使用三个双引号写文档注释，且如果是单行注释，则结尾的三引号应该与注释内容同行，如果是多行注释，则结尾三引号应该单独一行

六、命名
1.旧代码：如果原有的代码与命名规范不一样，应该与原有代码保持一致
2.API：暴露的API或者给别人使用的API应该以使用场景来命名，而不是实现原理命名
3.首字母大写加下划线：这种命名风格不可取，比如Capitalized_Words_With_Underscores
4.单下划线开头：这种命名为弱“内部使用”指示器，即模块内非公有（“protected”），比如在使用from xxx import *语句时是是不会导入单下划线开头的对象的
5.单下划线结尾：这种命名风格是为了避免与Python内部关键字冲突的一种约定
6.双下划线开头：当在类中以双下划线开头定义时，调用它的时候会在前面加上“_ClassName”，如调用类A中的属性__a时，__a就变成了_A__a，这样子类就不可以随便调用这个属性了，可以认为它是“私有”的，但Python中没有“私有”的说法，因为依然可以通过A._A__a去访问属性
7.双下划线开头和结尾：为模块和系统级变量，比如__name__、__init__等，我们自己应该永远避免使用这种命名风格
8.单字符变量：永远要使用O（大写的O）、l（小写的L）和I（大写的I），因为有些字体中无法区分它们是数字0和1还是英文字母L和O
9.包名和模板名：使用简短全小写的名称，包名不建议使用下划线，模块名为了提高可读性可以使用下划线
10.c/c++扩展模块：使用c/c++编写的扩展模块需要在模块名称加一个下划线前缀，如：_socket
11.类名：首字母大写
12.异常名：异常一般也是类，所以首字母也是大写，如果异常确实是一个错误，那需要在类名后加上“Error”后缀
13.函数名：全小写，为了提高可读性也可以使用下划线，大小写混合的情况只限于为了与原来的代码兼容的情况
14.全局变量：只在模块内使用的全局变量和普通变量一样定义，但是需要注意使用“from xxx import *”的xxx模块中的全局变量应该使用单下划线开头防止内部接口或变量的对外暴露
15.实例变量：非公有方法和实例变量使用单下划线开头（“protected”），双下划线开头会触发Python的命名转换规则（在前面加上“_ClassName”）以避免和子类命名的冲突
16.常量：常量通常是模块级的，使用全大写和下划线配合
```



-ascii、unicode、utf-8、gbk 区别？

ASCII

计算机中，所有数据都以0和1来表示。在一开始的时候，要表示的内容比较少，人们使用了ascii编码的方式来编码。

ASCII（American Standard Code for Information Interchange，美国标准信息交换代码）是基于拉丁字母的一套电脑编码系统，主要用于显示现代英语和其他西欧语言，其最多只能用 8 位来表示（一个字节），即：2**8 -1 = 255，所以，ASCII码最多只能表示 255 个符号。

1 1 1 1   1 1 1 1 =2**0+2**1+2**2+2**3+2**4+2**5+2**6+2**7 = 2**8-1=255

Unicode，UTF-8，GBK

随着计算机的发展，显然ASCII码无法将世界上的各种文字和符号全部表示，所以，就需要新出一种可以代表所有字符和符号的编码，即：Unicode

Unicode（统一码、万国码、单一码）是一种在计算机上使用的字符编码。Unicode 是为了解决传统的字符编码方案的局限而产生的，它为每种语言中的每个字符设定了统一并且唯一的二进制编码，以满足跨语言、跨平台进行文本转换、处理的要求。Unicode规定所有字符和符号最少使用2字节（16位）来表示，即2**16-1=65535


UTF-8，是对Unicode编码的压缩和优化，他不再使用最少使用2个字节，而是将所有的字符和符号进行分类：ascii码中的内容用1个字节保存、欧洲的字符用2个字节保存，东亚的字符用3个字节保存...

GBK，也是基于Unicode编码的进一步优化，GBK的文字编码是用双字节来表示的，即不论中、英文字符均使用双字节来表示

    1.在python2默认编码是ASCII, python3里默认是utf-8
    
    2.unicode 分为 utf-32(占4个字节),utf-16(占两个字节)，utf-8(占1-4个字节)， so utf-8就是unicode
    
    3.在py3中encode,在转码的同时还会把string 变成bytes类型，decode在解码的同时还会把bytes变回string

Unicode与UTF-8，GBK的关系，如图：

![image-20221018181544855](C:\Users\stormblinger\AppData\Roaming\Typora\typora-user-images\image-20221018181544855.png)

-三元运算规则以及应用场景？

化简代码量

Python3和Python2中 int 和 long的区别？

xrange和range的区别？

文件操作时：xreadlines和readlines的区别？

列举布尔值为False的常见值？

字符串、列表、元组、字典每个常用的5个方法？

-lambda表达式格式以及应用场景？

lambda 表达式其实就是一个匿名函数,在函数编程中经常作为参数使用。 例子如下       

​                    a = [('a',1),('b',2),('c',3),('d',4)]

 a_1 = list(map(lambda x：x[0],a)) 



-is和==的区别

==的作用是判断两个对象的值是否相同，is 表示的谁是谁，这也就意味着对象完全相等。我们知道一个对象有各自的内存地址和对应的值，当内存地址和值都相同的时候使用 is 可以得到结果 True。另外需要注意的下面两点特殊的情况。 这些变量很可能在许多程序中使用。 通过池化这些对象，Python 可以防止对一致使用的对象进行内存分配调用。 1)介于数字-5 和 256 之间的整数 2)字符串仅包含字母、数字或下划线 



-列举常见的内置函数？

    一、python常见的内置函数
        1、dir()
        2、open()
        3、staticmethod()
        4、enumerate()
        5、int()
        6、ord()/chr()
        7、eval()/exec
        8、isinstance()
        9、dict()
        10、help()
        11、getattr()/setattr()
        12、hex()
        13、next()
        14、id()
        15、sorted()
        16、bin()
        17、filter()
        18、pow()
        19、super()
        20、iter()
        21、print()
        22、tuple
        23、callable()
        24、format
        25、len()
        26、frozenset()
        27、range()
        28、classmethod 修饰符
        29、repr()
        30、zip()
        31、round()
        32、hash()
        33、set()

一、python常见的内置函数
1、dir()

返回参数的属性、方法列表，可以查看某个对象的属性和方法。

print(dir(list))

['__add__', '__class__', '__class_getitem__', '__contains__', '__delattr__', '__delitem__', '__dir__', '__doc__', '__eq__', '__format__', '__ge__', '__getattribute__', '__getitem__', '__gt__', '__hash__', '__iadd__', '__imul__', '__init__', '__init_subclass__', '__iter__', '__le__', '__len__', '__lt__', '__mul__', '__ne__', '__new__', '__reduce__', '__reduce_ex__', '__repr__', '__reversed__', '__rmul__', '__setattr__', '__setitem__', '__sizeof__', '__str__', '__subclasshook__', 'append', 'clear', 'copy', 'count', 'extend', 'index', 'insert', 'pop', 'remove', 'reverse', 'sort']



2、open()

用于打开一个文件，创建一个 file 对象，相关的方法才可以调用它进行读写

open(name[, mode[, buffering]])

    1
    
    name : 文件名称。
    
    mode : 打开文件的模式：只读，写入，追加等。所有可取值见如下的完全列表。这个参数是非强制的，默认为只读r（常见的模式有：w-写入，r-只读，a-追加）。
    
    buffering : 如果 buffering 的值被设为 0，就不会有寄存。如果 buffering 的值取 1，访问文件时会寄存行。如果将 buffering 的值设为大于 1 的整数，表明了这就是的寄存区的缓冲大小。如果取负值，寄存区的缓冲大小则为系统默认。

3、staticmethod()

返回函数的静态方法，一般通过装饰器的形式使用，方便将外部函数集成到类体中

staticmethod(function)

示例：

class Test:
    @staticmethod
    def test():
        print(1111)

以上实例声明了静态方法 test，从而可以实现实例化使用 Test().test()，当然也可以不实例化调用该方法 Test.test()

使用staticmethod装饰器装饰后，定义的方法中不用加self

Test.test()
Test().test()



4、enumerate()

用于将一个可遍历的数据对象(如列表、元组或字符串)组合为一个索引序列，同时列出数据和数据下标，一般结合for循环使用

enumerate(sequence, [start=0])

    1
    
    sequence：一个序列、迭代器或其他支持迭代对象。
    start：下标起始位置的值。
    返回值：<enumerate object at 0x10ab11e40>

print(list(enumerate(["a", "b", "c"])))

[(0, 'a'), (1, 'b'), (2, 'c')]



5、int()

将一个字符串或数字转换为整型

int(x, base=10)

    1
    
    x – 字符串或数字（若 x 为纯数字，则不能有 base 参数，否则报错；若 x 为 str，则 base 可略可有）。
    base – 进制数，默认十进制。

>>> int(3.6)
>>> 3
>>> int('12',16)        # 如果是带参数base的话，12要以字符串的形式进行输入，12 为 16进制
>>> 18



6、ord()/chr()

ord() ：以一个字符（长度为1的字符串）作为参数，返回对应的 ASCII 数值

>>>ord('a')
>>>97



chr(i)：用一个范围在 range（256）内的（就是0～255）整数作参数，返回一个对应的字符，i可以是10进制也可以是16进制的形式的数字

>>>print chr(0x30), chr(0x31), chr(0x61)   # 十六进制
>>>0 1 a
>>>print chr(48), chr(49), chr(97)         # 十进制
>>>0 1 a



7、eval()/exec

1、eval()
执行一个字符串表达式，并返回表达式的值

eval(expression[, globals[, locals]])



>>> eval('2 + 2')
>>> 4



2、exec
exec 执行储存在字符串或文件中的 Python 语句，相比于 eval，exec可以执行更复杂的 Python 代码

exec(object[, globals[, locals]])


​    
​    object：必选参数，表示需要被指定的 Python 代码。它必须是字符串或 code 对象。如果 object 是一个字符串，该字符串会先被解析为一组 Python 语句，然后再执行（除非发生语法错误）。如果 object 是一个 code 对象，那么它只是被简单的执行。
​    globals：可选参数，表示全局命名空间（存放全局变量），如果被提供，则必须是一个字典对象。
​    locals：可选参数，表示当前局部命名空间（存放局部变量），如果被提供，可以是任何映射对象。如果该参数被忽略，那么它将会取与 globals 相同的值。

>>>exec('print("Hello World")')
>>>Hello World
单行语句字符串

>>> exec("print ('runoob.com')")
>>> runoob.com

多行语句字符串

>>> exec ("""for i in range(5):
>>> ...     print ("iter time: %d" % i)
>>> ... """)
>>> iter time: 0
>>> iter time: 1
>>> iter time: 2
>>> iter time: 3
>>> iter time: 4



8、isinstance()

判断一个对象是否是一个已知的类型，类似 type()

isinstance(object, classinfo)


​    
​    object – 实例对象。
​    classinfo – 可以是直接或间接类名、基本类型或者由它们组成的元组。

>>> isinstance (a,str)
>>> False
>>> isinstance (a,(str,int,list))    # 是元组中的一个返回 True
>>> True



type() 与 isinstance()区别：

class A:
    pass

class B(A):
    pass

isinstance(A(), A)    # returns True
type(A()) == A        # returns True
isinstance(B(), A)    # returns True
type(B()) == A        # returns False



9、dict()

用于创建一个字典
创建字典有三种方式，关键字/映射函数/可迭代对象，具体实例如下：

>>>dict()                        # 创建空字典
>>>{}
>>>dict(a='a', b='b', t='t')     # 传入关键字
>>>{'a': 'a', 'b': 'b', 't': 't'}
>>>dict(zip(['one', 'two', 'three'], [1, 2, 3]))   # 映射函数方式来构造字典
>>>{'three': 3, 'two': 2, 'one': 1} 
>>>dict([('one', 1), ('two', 2), ('three', 3)])    # 可迭代对象方式来构造字典
>>>{'three': 3, 'two': 2, 'one': 1}



10、help()

返回对象帮助信息
11、getattr()/setattr()

1、getattr() 函数用于返回一个对象属性值

getattr(object, name[, default])

    1
    
    object – 对象。
    name – 字符串，对象属性。
    default – 默认返回值，如果不提供该参数，在没有对应属性时，将触发 AttributeError。

>>> getattr(a, 'bar')        # 获取属性 bar 值
>>> 1
>>> getattr(a, 'bar2')       # 属性 bar2 不存在，触发异常
>>> Traceback (most recent call last):
>>> File "<stdin>", line 1, in <module>
>>> AttributeError: 'A' object has no attribute 'bar2'
>>> getattr(a, 'bar2', 3)    # 属性 bar2 不存在，但设置了默认值
>>> 3



2、setattr() 函数对应函数 getattr()，用于设置属性值，该属性不一定是存在的

setattr(object, name, value)

    1
    
    object – 对象。
    name – 字符串，对象属性。
    value – 属性值。

>>> setattr(a, 'bar', 5)       # 设置属性 bar 值
>>> a.bar
>>> 5



12、hex()

将一个指定数字转换为 16 进制数

>>>hex(255)
>>>'0xff'



13、next()

返回迭代器的下一个项目，next() 函数要和生成迭代器的 iter() 函数一起使用

next(iterable[, default])


​    
​    iterable – 可迭代对象
​    default – 可选，用于设置在没有下一个元素时返回该默认值，如果不设置，又没有下一个元素则会触发 StopIteration 异常。

首先获得Iterator对象:

it = iter([1, 2, 3, 4, 5])

循环:

while True:
    try:
        # 获得下一个值:
        x = next(it)
        print(x)
    except StopIteration:
        # 遇到StopIteration就退出循环
        break



14、id()

返回对象的唯一标识符，标识符是一个整数（返回对象的内存地址）
15、sorted()

sorted(iterable, key=None, reverse=False)  

    1
    
    iterable – 可迭代对象。
    key – 主要是用来进行比较的元素，只有一个参数，具体的函数的参数就是取自于可迭代对象中，指定可迭代对象中的一个元素来进行排序。
    reverse – 排序规则，reverse = True 降序 ， reverse = False 升序（默认）。

>>> sorted([5, 2, 3, 1, 4])
>>> [1, 2, 3, 4, 5]   # 默认为升序

​    

sort 与 sorted 区别：
sort 是应用在 list 上的方法，sorted 可以对所有可迭代的对象进行排序操作。
list 的 sort 方法返回的是对已经存在的列表进行操作，而内建函数 sorted 方法返回的是一个新的 list，而不是在原来的基础上进行的操作。
16、bin()

返回一个整数 int 或者长整数 long int 的二进制表示

>>>bin(10)
>>>'0b1010'



17、filter()

用于过滤序列，过滤掉不符合条件的元素，返回一个迭代器对象，如果要转换为列表，可以使用 list() 来转换；

filter(function, iterable)


​    
​    function – 判断函数。
​    iterable – 可迭代对象。

该接收两个参数，第一个为函数，第二个为序列，序列的每个元素作为参数传递给函数进行判断，然后返回 True 或 False，最后将返回 True 的元素放到新列表中

def is_odd(n):
    return n % 2 == 1

tmplist = filter(is_odd, [1, 2, 3, 4, 5, 6, 7, 8, 9, 10])
newlist = list(tmplist)

print(newlist)

[1, 3, 5, 7, 9]



18、pow()

返回 xy（x的y次方） 的值

函数是计算x的y次方，如果z在存在，则再对结果进行取模，其结果等效于pow(x,y) %z

pow(x, y[, z])



注意：pow() 通过内置的方法直接调用，内置方法会把参数作为整型，而 math 模块则会把参数转换为 float。

import math   # 导入 math 模块
print ("math.pow(100, 2) : ", math.pow(100, 2))

使用内置，查看输出结果区别

print ("pow(100, 2) : ", pow(100, 2))

math.pow(100, 2) :  10000.0
pow(100, 2) :  10000



19、super()

用于调用父类(超类)的一个方法，主要用于解决多重继承问题，直接用类名调用父类方法在使用单继承的时候没问题，但是如果使用多继承，会涉及到查找顺序（MRO）、重复调用（钻石继承）等种种问题（MRO 就是类的方法解析顺序表, 其实也就是继承父类方法时的顺序表）；

class A:
     def add(self, x):
         y = x+1
         print(y)
class B(A):
    def add(self, x):
        super().add(x)
b = B()
b.add(2)  # 3



20、iter()

用来生成迭代器

iter(object[, sentinel])

    1
    
    object – 支持迭代的集合对象。
    sentinel – 如果传递了第二个参数，则参数 object 必须是一个可调用的对象（如，函数），此时，iter 创建了一个迭代器对象，每次调用这个迭代器对象的__next__()方法时，都会调用 object。

>>>lst = [1, 2, 3]
>>>for i in iter(lst):
>>>...     print(i)
>>>... 
>>>1
>>>2
>>>3



21、print()

print(*objects, sep=' ', end='\n', file=sys.stdout, flush=False)

    1
    
    objects – 复数，表示可以一次输出多个对象。输出多个对象时，需要用 , 分隔。
    sep – 用来间隔多个对象，默认值是一个空格。
    end – 用来设定以什么结尾。默认值是换行符 \n，我们可以换成其他字符串。
    file – 要写入的文件对象。
    flush – 输出是否被缓存通常决定于 file，但如果 flush 关键字参数为 True，流会被强制刷新。

>>> print("aaa","bbb")
>>> aaa bbb

>>> print("www","runoob","com",sep=".")  # 设置间隔符
>>> www.runoob.com



22、tuple

tuple 函数将可迭代系列（如列表）转换为元组

tuple( iterable )


​    
​    terable – 要转换为元组的可迭代序列

23、callable()

检查一个对象是否是可调用的。如果返回 True，object 仍然可能调用失败；但如果返回 False，调用对象 object 绝对不会成功。

对于函数、方法、lambda 函式、 类以及实现了 call 方法的类实例, 它都返回 True。

callable(object)

    1

>>> def add(a, b):
>>> ...     return a + b
>>> ... 
>>> callable(add)             # 函数返回 True
>>> True
>>> class A:                  # 类
>>> ...     def method(self):
>>> ...             return 0
>>> ... 
>>> callable(A)               # 类返回 True
>>> True
>>> a = A()
>>> callable(a)               # 没有实现 __call__, 返回 False
>>> False
>>> class B:
>>> ...     def __call__(self):
>>> ...             return 0
>>> ... 
>>> callable(B)
>>> True
>>> b = B()
>>> callable(b)               # 实现 __call__, 返回 True
>>> True

​    

24、format

str.format()，它增强了字符串格式化的功能。format 函数可以接受不限个参数，位置可以不按顺序

>>>"{} {}".format("hello", "world")    # 不设置指定位置，按默认顺序
>>>'hello world'

>>> "{1} {0} {1}".format("hello", "world")  # 设置指定位置
>>> 'world hello world'

print("网站名：{name}, 地址 {url}".format(name="菜鸟教程", url="www.runoob.com"))

通过字典设置参数

site = {"name": "菜鸟教程", "url": "www.runoob.com"}
print("网站名：{name}, 地址 {url}".format(**site))

通过列表索引设置参数

my_list = ['菜鸟教程', 'www.runoob.com']
print("网站名：{0[0]}, 地址 {0[1]}".format(my_list))  # "0" 是必须的

数字格式化

>>> print("{:.2f}".format(3.1415926))
>>> 3.14



25、len()

len() 方法返回对象（字符、列表、元组等）长度或项目个数

>>>str = "runoob"
>>>len(str)             # 字符串长度
>>>6

​    

26、frozenset()

返回一个冻结的集合，冻结后集合不能再添加或删除任何元素，有集合的中的元素是另一个集合的情况，但是普通集合（set）本身是可变的，那么它的实例就不能放在另一个集合中（set中的元素必须是不可变类型）。
所以，frozenset提供了不可变的集合的功能，当集合不可变时，它就满足了作为集合中的元素的要求，就可以放在另一个集合中了。

>>> b = frozenset('runoob') 
>>> b
>>> frozenset(['b', 'r', 'u', 'o', 'n'])   # 创建不可变集合



27、range()

range() 函数返回的是一个可迭代对象（类型是对象），而不是列表类型， 所以打印的时候不会打印列表

range(start, stop[, step])


​    
​    start: 计数从 start 开始。默认是从 0 开始;
​    stop: 计数到 stop 结束，但不包括 stop。例如：range（0， 5） 是[0, 1, 2, 3, 4]没有5
​    step：步长，默认为1。例如：range（0， 5） 等价于 range(0, 5, 1)

>>> list(range(0, -10, -1))
>>> [0, -1, -2, -3, -4, -5, -6, -7, -8, -9]



28、classmethod 修饰符

classmethod 修饰符对应的函数不需要实例化，不需要 self 参数，但第一个参数需要是表示自身类的 cls 参数，可以来调用类的属性，类的方法，实例化对象等

class A:
    s = 1
    def func1(self):  
        print ('foo') 
    @classmethod
    def func2(cls):
        print ('func2')
        print (cls.s)
        cls().func1()   # 调用 foo 方法

A.func2()               # 不需要实例化



29、repr()

repr() 函数将对象转化为供解释器读取的形式，返回一个对象的 string 格式

>>> s = 'RUNOOB'
>>> repr(s)
>>> "'RUNOOB'"
>>> dict = {'runoob': 'runoob.com', 'google': 'google.com'};
>>> repr(dict)
>>> "{'google': 'google.com', 'runoob': 'runoob.com'}"



30、zip()

将可迭代的对象作为参数，将对象中对应的元素打包成一个个元组，然后返回由这些元组组成的对象，一般会用listuo输出为列表

zip([iterable, ...])

    iterabl – 一个或多个迭代器;

print(list(zip([1,2,3],[2,3,4])))

print(dict(zip([1,2,3],[2,3,4])))

[(1, 2), (2, 3), (3, 4)]
{1: 2, 2: 3, 3: 4}



zip(*) 可理解为解压，返回二维矩阵式

aa=[(1, 2), (2, 3), (3, 4)]
a,b=zip(*aa)
print(a)

print(b)

(1, 2, 3)
(2, 3, 4)



31、round()

返回浮点数 x 的四舍五入值

round( x [, n]  )

    x – 数字。
    n – 表示从小数点位数，其中 x 需要四舍五入，默认值为 0

print ("round(70.23456) : ", round(70.23456))

print ("round(56.659,1) : ", round(56.659,1))

round(70.23456) :  70
round(56.659,1) :  56.7



32、hash()

获取取一个对象（字符串或者数值等）的哈希值

>>>hash('test')            # 字符串
>>>2314058222102390712
>>>hash(1)                 # 数字
>>>1
>>>hash(str([1,2,3]))      # 集合
>>>1335416675971793195
>>>hash(str(sorted({'1':1}))) # 字典
>>>7666464346782421378



33、set()

创建一个无序不重复元素集，可进行关系测试，删除重复数据，还可以计算交集、差集、并集等

>>>x = set('runoob')
>>>y = set('google')
>>>x, y
>>>(set(['b', 'r', 'u', 'o', 'n']), set(['e', 'o', 'g', 'l']))   # 重复的被删除
>>>x & y         # 交集
>>>set(['o'])
>>>x | y         # 并集
>>>set(['b', 'e', 'g', 'l', 'o', 'n', 'r', 'u'])
>>>x - y         # 差集
>>>set(['r', 'b', 'u', 'n'])

​    

-filter、map、reduce的作用？

一、map函数

作用：map主要作用是计算一个序列或者多个序列进行函数映射之后的值

语法：map(function,iterable1,iterable2)

说明：function中参数值可以是一个，也可以是多个；iterable代表function运算中的参数值，有几个参数值就传入几个iterable

注意：1.迭代器需要进行列表转换 2.map中如果传入的序列长度不一，会依据最短的序列计算

1. lambda函数：

x=[1,2,3,4]
y=[5,6,7,8]
print(list(map(lambda x,y:(x+y),x,y)))

#输出结果：
[6, 8, 10, 12]



2. 自定义函数：

def m_num(x,y):
   return  x+y
print(list(map(m_num,x,y)))

#输出结果：
[6, 8, 10, 12]



思路：
把列表1中的元素与列表2中元素依次相加
1+5
2+6
3+7
4+8
二、filter函数

作用：filter主要作用是过滤掉序列中不符合函数条件的元素

语法：fliter(function,sequence)

说明：function可以是匿名函数或者自定义函数，可以对后面的sequence序列的每个元素判定是否符合条件；sequence可以是列表、元组或者字符串

1. lambda函数：

num = [2,3,6,9,90,23,88]
#输出的是filter对象 <filter object at 0x00000113BF8C7390>
print(filter(lambda x:x>10,num))
#需要转成list [90, 23, 88]
print(list(filter(lambda x:x>10,num)))

#输出结果：
<filter object at 0x00000113BF8C7390>
[90, 23, 88]
注意：迭代器需要进行列表转换

2. 自定义函数：

def fil_num(x):
   return x>10

print(list(filter(fil_num,num)))

思路：
把列表中不需要的元素去掉，那首先要确定要过滤得条件是什么
三、reduce函数

作用：reduce是对一个序列进行计算，结果只得到一个值

语法：reduce(function,iterable)

说明：function中必须传入两个参数，iterable可以是列表或者元组

注意：reduce使用前需要导包 from functools import reduce

1. lambda函数：

from functools import reduce
x=[1,2,3,4,5]
print(reduce(lambda x,y:(x*y),x))



2. 自定义函数：

from functools import reduce
x=[1,2,3,4,5]
def ca(x,y):
    return x*y
print(reduce(ca,x))

思路：
对一个列表里的元素做计算，从左到右依次计算两个元素，将得到得值跟下一个元素计算
12 = 2
23 = 6
64 = 24
245 = 120

-如何安装第三方模块？以及用过哪些第三方模块？



-至少列举8个常用模块都有那些？

pathlib：路径操作模块，比 os 模块拼接方便。 

urllib：网络请求模块，包括对 url 的结构解析。

 asyncio： Python 的异步库，基于事件循环的协程模块。 

re：正则表达式模块。 

itertools：提供了操作生成器的一些模块。

time模块

random模块

OS，用来和操作系统交互

sys模块，和Python解释器交互的一个接口





-什么是正则的贪婪匹配？

先来看一段代码，我准备从这段代码中引出贪婪匹配和非贪婪匹配

import re

data='Details: http://www.baidu.com/helloworld nihao\nHELLO'

res=re.findall('http://.+',data)

print(res)

问打印结果是什么？

答案是：http://www.baidu.com/helloworld nihao

如果能明白答案是怎么回事儿就不用往下看了，不明白的老铁，咱走着哈。

首先需要明白正则匹配里面的几个符号的作用。

. 它表示匹配任意一个字符，是"一个啊，老铁 。

+它表示+前面的那一个字符出现的次数是一次或两次或者N次。


现在匹配规则是：

http://.+

因为+有多种情况，我们分类讨论一下。

当+表示前面的字符出现次数是一次的时候，匹配规则就是

http://.    也就是http://后面紧跟着一个任意字符，那么匹配出来的结果就是

http://w

当+表示前面字符出现次数是两次的时候，匹配规则就是

http://..    因为+前面是一个. 表示任意字符，     .+就表示两个任意字符，匹配出来的结果就是

http://ww

当+表示前面字符出现n次的时候，匹配规则就是

http://..............................................., 你以为会匹配无限长吗？ 错，它遇到不可打印字符的时候就停止匹配了，不可打印字符有\n \t \r。


所以，按照正则匹配各个符号的定义，这个匹配规则http://.+有很多个匹配结果的，但是规定了只要那个最长的，这就是贪婪匹配。

最长的结果自然是 http://www.baidu.com/helloworld nihao，到\n结束。

常见的贪婪匹配模样就是一个字符后面加一个+或者*


再来看另外一个问题，稍微变形了一下。

import re

data='Details: http://www.baidu.com/helloworld nihao\nHELLO'

res=re.findall('http://.*?',data)

print(res)

结果是什么？

是http://


如果能明白答案是怎么回事儿就不用往下看了，不明白的老铁，咱走着哈。

http://.*? 这种匹配规则，其实是贪婪匹配后面加一个?，这种就是非贪婪匹配了。

先按照http://.*的各个符号的作用把所有的可能性都列出来，然后选择最短的那一种。

现在来看匹配规则。

首先需要明白正则匹配里面的几个符号的作用。

.表示匹配任意一个字符，一个哈。

*表示*前面的那个字符出现的次数是0次或1次或2次或n次。

由于*有多种情况，需要分类讨论一下

当*表示前面的字符出现0次的时候，匹配规则就是：

http://,任意字符出现0次，就是什么也没有嘛。匹配出来的结果是http://

当*表示前面字符出现1次的时候，匹配规则就是:

http://. 一个任意字符，匹配出来的结果就是http://w

当*表示前面字符出现n次的时候，匹配规则就是

http://...................................... 多个任意字符，遇到\n结束，匹配出来的结果就是

http://www.baidu.com/helloworld nihao

由于是非贪婪匹配，就选择长度最短的那个，所以结果就是

http:// 

-简述 生成器、迭代器、可迭代对象 以及应用场景？

一、先上结论

先说结论，再解释：

iterable是可迭代对象，它的唯一特征是有__iter__函数，调用这个函数会返回一个iterator。iterator是迭代器，它的唯一特征是有__next__函数，调用这个函数会返回下一个元素。有些类同时有以上两个函数，所以即是iterable，又是iterator，这是为了方便，不用额外创建iterator类。generator是用yield函数定义的iterator。它必然也有__next__函数。二、几个iterable例子

可以用for循环遍历的对象都是可迭代对象iterable，比如list, range, tuple等：

nums = [9527, 3721, 56, 97]for n in nums:  print(n)  for n in range(1, 100):  print(n)seasons = ('春', '夏'，'秋'，'冬')for s in seasons:  print(s)

iterable还有很多，它们的唯一必要特征是：

iterable必须有__iter__函数，这个函数的作用就是返回迭代器iterator。

请默默把这句话重复3遍，记住它！

我们来验证一下这个特征，用dir函数看看上面3个iterable有没有__iter__函数：

![img](https://pics6.baidu.com/feed/c2fdfc039245d688a02ceff88a4cb916d31b24d2.png@f_auto?token=746fab01ea7e41fd2d7eb068c25e0966)

再说一遍，__iter__函数是iterable的唯一特征。

注意，这些对象并没有__next__函数，因为__next__是iterator的特征。上面3个都不是iterator。

三、设计模式和原理

迭代器是一个通用的设计模式，不同编程语言都有各自的实现。

我们把list，tuple，和range等iterable当做是一个仓库，里面存放着可以被取用的东西。它们的内部结构各不相同。为了能够用for循环遍历仓库中的东西，我们给这些iterable都配上一个仓库管理员，那就是iterator。for只要找仓库管理员就可以了，而不需要知道里面具体如何存放的。这样问题就简单了。__iter__函数是从仓库中获得管理员的函数。一个对象只要有这个函数，就相当于配备了仓库管理员，就是可迭代对象，就是iterable。现在看iterator，它的唯一特征就是有__next__函数，for循环每次调用这个函数获得下一个元素，直到出现StopIteration异常为止。整个过程就是这样的：

![img](https://pics7.baidu.com/feed/aa64034f78f0f7365ed262952ddb7711eac41388.png@f_auto?token=fbe751572cf55e89ddbcf2dca892fc6d)

通过这个设计模式，for循环不管被迭代的对象是什么牛鬼蛇神（正方形，圆，对话框，圆角正方形等等），它只找管理员：

调用iterable的__iter__函数，获得iterator对象。注意：实际上for是调用Python内置的iter()函数，而这个函数又调用了iterable的__iter__函数。调用iterator的__next__函数，获取下一个元素，直到获得StopIteration为止。注意：实际上for是调用Python内置的next()函数，而这个函数又调用了iterator的__next__函数。我们来验证一下：

![img](https://pics4.baidu.com/feed/908fa0ec08fa513dcd530a5d12e391f3b0fbd9cb.png@f_auto?token=7e3781f32b451ec5a6950931f03d75e8)

从图中可以看出：

a是一个列表调用iter(a)返回的是一个list_iterator对象这个对象有__next__函数到这里你应该对iterator和iterable有了比较好的了解。这里涉及到两个类：

一个对象是仓库，也就是iterable。一个对象是仓管员，也就是iterator。定义自己的Iterator和Iterable

理解了这个原理，我们可以轻松创建一个可迭代对象以及它的迭代器。下面我们来创建一个随机列表：RandList。它的特点是：随机遍历访问一个列表中的内容。

先定义一个iterator:

import random# 这是一个iteratorclassRandListIterator():def__init__(self,  rand_list):        # 复制一份列表，防止影响原列表self.list = rand_list[:]    #  每次遍历随机选择一个，并删除已经返回的元素，直到列表为空def__next__(self):        if len(self.list)  == 0:            raise StopIteration        index = random.randint(0,  len(self.list)-1)        value = self.list[index]        del  self.list[index]        return value 

在定义一个iterable，它使用前面定义的iterator:

\# 这是一个iterableclassRandList():def__init__(self, alist):        self.list = alist    def__iter__(self):        return RandListIterator(self.list)

现在用for循环来循环一下：

alist=[4,6,3,8,23,1]foriinRandList(alist):print(i)

可以随机的访问列表中的内容：

![img](https://pics3.baidu.com/feed/fc1f4134970a304e04413e2cf746638ec8175c1f.png@f_auto?token=332b7fe8a2dc05872b89b443c97deb5d)

一个类既是iterable，也是iterator

上面的例子，我们定义了两个类。很多时候为了方便，我们会让一个类把两件事情都做了，毕竟只要这个类同时有__iter__和__next__函数就可以了。

我们现在定一个随机数生成器，我把它命名为Randable，它的功能是：随机生成10个1到100之间的随机数。

import random classRandable():def__init__(self):        self.count =  0def__iter__(self):        returnselfdef__next__(self):         ifself.count == 10:            raise StopIteration        rand_num =  random.randint(1, 100)        self.count += 1return rand_num 

这个类同时包含了__iter__和__next__两个函数，自己是仓库，又是仓库管理员。就是为了代码方便点，毕竟确实没必要写到两个类里。

使用上面的Randable类：

foriinRandable():    print(i)

执行结果：

![img](https://pics6.baidu.com/feed/4d086e061d950a7b7b2ad48c3f5fa6d1f3d3c9f6.jpeg@f_auto?token=3549e59d69dd26d1e6d523aceade74ec)

到这里，你对iterable和iterator的理解应该很透彻了吧。对这个概念的理解应该可以终结了！

如果没有终结，请多读两遍，也可以给我留言。

更简单的写法generator

就算第二种写法，用一个类定义生成器和迭代器仍然有点复杂，需要定义类。Geneator是一种更简单的写法，只要一个函数就够了。来改写一下上面的Randable的例子：

defrandable():for _ in range(0, 10):        yield random.randint(1, 10)

这样就完事了，现在可以循环了：

foriinrandable():    print(i)



-谈谈你对面向对象的理解？

https://blog.csdn.net/m0_47999117/article/details/124809437

    面向对象就是将编程当成是一个事物，对外界来说，事物是直接使用的，不用去管他内部的情况。而编程就是设置事物能够做什么事。

二. 类和对象

思考：洗衣机洗衣服描述过程中，洗衣机其实就是一个事物，即对象，洗衣机对象哪来的呢？

答：洗衣机是由工厂工人制作出来。

思考：工厂工人怎么制作出的洗衣机？

答：工人根据设计师设计的功能图纸制作洗衣机。

总结：图纸 → 洗衣机 → 洗衣服。

在面向对象编程过程中，有两个重要组成部分：类 和 对象。

类和对象的关系：用类去创建一个对象。
2.1 理解类和对象
2.1.1 类

类是对一系列具有相同特征和行为的事物的统称，是一个抽象的概念，不是真实存在的事物。

    特征即是属性
    行为即是方法

类比如是制造洗衣机时要用到的图纸，也就是说类是用来创建对象。
2.1.2 对象

对象是类创建出来的真实存在的事物，例如：洗衣机。

    注意：开发中，先有类，再有对象。
-Python面向对象中的继承有什么特点？

面向对象三大特性**

- 封装   
  - 将属性和方法书写到类的里面的操作即为封装
  - 封装可以为属性和方法添加私有权限
- 继承   
  - 子类默认继承父类的所有属性和方法
  - 子类可以重写父类属性和方法
- 多态   
  - 传入不同的对象，产生不同的结果

面向对象深度优先和广度优先是什么？

面向对象中super的作用？

是否使用过functools中的函数？其作用是什么？

列举面向对象中带爽下划线的特殊方法，如：__new__、__init__ 

如何判断是函数还是方法？

静态方法和类方法区别？

列举面向对象中的特殊成员以及应用场景

-什么是反射？以及应用场景？

```
# 反射
'''
    反射的定义：主要是应用于类的对象上，在运行时，将对象中的属性和方法反射出来。
    使用场景：可以动态的向对象中添加属性和方法。也可以动态的调用对象中的方法或者属性。
    反射的常用方法：
        1.hasaattr(obj,str)
            判断输入的str字符串在对象obj中是否存在(属性或方法)，存在返回True，否则返回False
        2.getattr(obj,str)
            将按照输入的str字符串在对象obj中查找。如找到同名属性，则返回该属性；如找到同名方法，则返回方法的引用，
            想要调用此方法得使用 getattr(obj,str)()进行调用。
            如果未能找到同名的属性或者方法，则抛出异常：AttributeError。
        3.setattr(obj,name,value)
            name为属性名或者方法名，value为属性值或者方法的引用
            1) 动态添加属性。如上
            2）动态添加方法。首先定义一个方法。再使用setattr(对象名,想要定义的方法名,所定义方法的方法名)
        4.delattr(obj,str)
            将你输入的字符串str在对象obj中查找，如找到同名属性或者方法就进行删除
'''
 
 
class Dog(object):
    def __init__(self, name):
        self.name = name
 
    def eat(self):
        print(f"{self.name}正在吃东西...")
 
dog = Dog("二哈")
 
# hasaattr的使用
# str = input("请输入你要判断的属性名或者方法名:")
# # print(hasattr(dog, str)) # 输入name --> 返回True，对象dog中确实存在name属性
# print(hasattr(dog, str)) # 输入age --> 返回False，对象dog中不存在age属性或者age方法
 
# # getattr的使用
# str = input("请输入你想要查找的属性名或者方法名:")
# print(getattr(dog, str)) # 输入name --> 返回二哈，对象dog中name的属性确实为二哈
# print(getattr(dog, str)) # 输入age --> 抛出异常 AttributeError: 'Dog' object has no attribute 'age'
 
# # setattr的使用
# name = input("请输入想要添加的属性名:")
# value = input("请输入想要添加的属性值:")
# # 添加属性
# setattr(dog, name , value) # name:age,value:5
# print(getattr(dog, name)) # 输出结果为：5，说明添加属性age成功。
# # 添加方法
# name = input("请输入想要添加的属性名:")
# def run():
#     print("二哈正在奔跑...")
# setattr(dog, name, run) # 此时的run为函数run的引用(相当于指针指向run函数)
# getattr(dog, name)() # 方法的调用
 
# delattr的使用
str = input("请输入想要删除的属性名或者方法名:")
print(getattr(dog, str)) # 输入str为name，返回二哈
delattr(dog, str) # 删除name属性
print(getattr(dog, str)) # 抛出异常 AttributeError: 'Dog' object has no attribute 'name'
```



装饰器的写法以及应用场景。

-什么是断言？应用场景？

在 Python 中是断言语句 assert 实现此功能，一般在表达式为 True 的情况下，程序才能通过。                           #assert（）方法，断言成功，则程序继续执行，断言失败，则程序报错 # 断言能够帮助别人或未来的你理解代码， # 找出程序中逻辑不对的地方。一方面， # 断言会提醒你某个对象应该处于何种状态， # 另一方面，如果某个时候断言为假， # 会抛出 AssertionError 异常，很有可能终止程序。 def foo(a)：    assert a==2,Exception("不等于 2")    print("ok",a) if __name__ == '__main__'：    foo(1)

-有用过with statement吗？它的好处是什么？

上下文管理器，`with`语句打开资源，并保证`with`无论块如何完成，都在块完成时关闭资源。考虑一个文件：

-使用代码实现查看列举目录下的所有文件。

os.listdir()将提供目录中的所有内容，文件和目录。如果只想要文件，可以使用方法过滤os.path**

-简述 yield和yield from关键字。

yield：用于定义生成器，返回单值。如果想一直返回生成器数据，则需要自己去定义循环返回。另外，还可以跟send函数一起使用，在单线程中相互切换，多任务协调并发处理任务。

yield from: 后接列表、生成器、协程。与asyncio.coroutine同时使用，定义协程函数。在python3.5以后改成了await。当yield from后面是IO耗时操作的时候，会切换至另一个yield from。

Python数据结构

-Python内建数据类型

列表，有序集合，用方括号表示[]，空列表就是[]；列表为异构有序的，内部指向数据可以不同类型，可以嵌套；支持任意Python序列的运算，如列表，字符串，元祖

-可应用于任意Python序列的运算

索引 [] ：取序列中某个值

连接 +：将序列连接在一起

重复 *：重复N次连接

成员 in：询问序列中是否有某元素

长度 len：询问序列中元素个数

切片 [:]：取出序列的一部分，左闭右开

-Python列表提供的方法

append(x),末尾添加元素

insert(i,x),i位置插入一个元素

pop(),删除并返回最后一个元素，pop(i)

sort()，排序，reverse(),倒叙排序

del list[i]，删除第i个元素

index(x)，返回x第一次出现时的下标

count(x),返回x出现的次数

remove(x)，删除第一次出现的x

max(),min()，最大最小值

-列表生成函数，range（min,max,interval）

interval产生从最小值到最大值之间数序列的间隔

max必须有，其余可默认不填

-字符串，是零个或多个字母，数字，符号的有序集合；由引号，单双不限包住

字符串提供的方法，不能修改

center(w),返回新串，原字符串居中，其余空格填充

count(x)，x出现的次数

ljust(w),靠左，填充空格至长度为W，rjust(w)同左;

lower()，小写，upper()，全部大写

find(x),返回x下标

split(schar，i)，在schar位置将字符串分割成子串,i为分割次数，删除schar，rsplit(schar,1)，不删除schar分割，分割后堵为列表

strip()，去除两端空格，rstrip(),lstrip(),左右空格去除

-元祖，和字符串为异构不可修改异构序列，由括号包含()；数据不改变时，就使用元组，其余情况都使用列表，元素可重复

-集，set，由零个或多个不可修改的Python数据对象组成的无序集合，不允许重复元素，有花括号包含，逗号分隔，空集有set()，集也是异构的，集是无序的

集的方法

union(set)，返回一个包含两个集所有元素的集

intersection(set),返回包含两个集共有元素的集

difference(set)，返回只出现在原有集中的元素

issubset(set)，询问集是否是set的子集

remove(x),将x集中移除

add(x)，添加X

pop()，随机移除一个

clear()，清空集

-字典，也是无序结构，由键：值对组成，花括号表示；

通过键访问字典对应得值，和序列很像，不过是用键不是用下标

，键的位置是散列来决定的

运算，[k],返回键K的值，in，del[k]，删除K键值

字典dict提供的方法

keys()返回所有键，values()返回所有值，items()返回键值对

get(k)，返回K的值，无返回None，get(k,i)，返回键K的值，无返回i





4种简单而强大的数据结构。栈、队列、双 端队列和列表都是有序的数据集合，其元素的顺序取决于添加顺序或移除顺序。一旦某个元素被添加进来，它与前后元素的相

对位置将保持不变。这样的数据集合经常被称为线性数据结构 。 

栈

栈是有序集合，添加操作和移除操作总发生在同一端，即顶端，另一端则被称为底端。最新添加的元素将被最先 

移除。这种排序原则被称作**LIFO** （last-in first-out），即后进先出。

栈的反转特效

每个浏览器都有返回按钮，当我们从一个网页跳转到另一个网页时，这些网页---实际上是URL----都被存放在一个栈中。当前正在浏览的网页位于栈的 顶端，最早浏览的网页则位于底端。如果点击返回按钮， 便开始反向浏览这些网页

栈的抽象数据类型

stack()				创建一个空栈，不需要参数，且会返回一个空栈

push(item)		将一个元素添加到栈的顶端

pop()				将栈顶端元素移除，返回顶端元素，并修改栈

peek()				返回顶端元素，但不移除

isEmpty()			检查栈是否为空，返回布尔值

size()					返回栈中元素个数，返回整数

-用Python实现栈的抽象数据类型

Python列表是有序集合，对于列表[2, 5, 3, 6, 7, 4] ，只需要考虑将它的哪一边视为栈的顶端。一旦确定了顶端，所有的操作就可 

以利用append 和pop 等列表方法来实现

```
class Stack:
	def __init__(self):
		self.items = []
	# 判断是否为空列表，判断是否栈为空
	def isEmpty(self):
		return self.items == []
	def push(self, item):
		self.items.append(item)
	def pop(self):
		return self.items.pop()
	def peek(self):
		return self.items[len(self.items)-1]
	def size(self):
		return len(self.items)
		
	
```

同样也可以选择列表的头部作为栈的顶端，这种情况下无法直接使用pop和append方法。必须用pop和insert方法显式地访问下标为0的元素，即列表中的第一个元素

-栈的用法

匹配括号

左括号进栈，遇右括号出栈

进制转换

十进制转换为2进制用除2法

十进制转换成任意进制数

前序，中序，后序表达式

-队列

队列 是有序集合，添加操作发生在“尾部”，移除操作则发生在“头部”。新元素从尾部进入队列，然后一直向前移动到头部，直到成为下一个被移除的元素。 最新添加的元素必须在队列的尾部等待，在队列中时 间最长的元素则排在最前面。这种排序原则被称 作**FIFO** （first-in first-out），即先进先出

现实情况，排队，操作系统中使用一些队列来控制计算机进程

-队列抽象数据类型

-用Python实现队列

-实际代码

-双端队列

-双端队列抽象数据类型

-用Python实现双端队列

-实际

-列表

-递归

递归 是解决问题的一种方法，它将问题不断地分成更小的子问题，直到子问题可以用普通的方法解决。通常情况下，递归会使用 一个不停调用自己的函数。

实际问题

计算一列数之和，除用for和while循环外，可用递归求和

例如：

```
def listsum(numlist):
	if len(numlist) == 1:
		return numlist[0]
	else:
		return numlist[0] + listsum(numlist[1:])
```

递归三原则：

(1) 递归算法必须有基本情况 ； 

(2) 递归算法必须改变其状态并向基本情况 靠近； 

(3) 递归算法必须递归地调用自己。

将整数转换成任意进制的字符串

栈帧实现递归

-搜索和排序

搜索是指从元素集合中找到某个特 定元素的算法过程。搜索过程通常返回True 或False ，分别表示元素是否存在

Python提供了运算符in，可以方便地检查元素是否在列表中

-顺序搜素，从列表中的第一个元素开始，沿着默认的顺序逐个查看，直到找到目标元素或者查完列表

list[pos] == item ;pos += 1

有序列表，可提前发现是否存在目标值

-二分搜索二分搜索不是从第一个元素开 始搜索列表，而是从中间的元素着手。如果 这个元素就是目标元素，那就立即停止搜 索；如果不是，则可以利用列表有序的特 性，排除一半的元素

-散列

散列表 是元素集合，其中的元素以一种便 于查找的方式存储。散列表中的每个位置通 常被称为槽 ，其中可以存储一个元素。槽 用一个从0开始的整数标记，例如0号槽、1 号槽、2号槽，等等。初始情形下，散列表 中没有元素，每个槽都是空的。可以用列表 来实现散列表，并将每个元素都初始化为 Python中的特殊值None

-散列函数

 将散列表中的元素与其所属位置 

对应起来。

-排序

是指将集合中的元素按某种顺序排 列的过程

-冒泡排序

冒泡排序 多次遍历列表。它比较相邻 的元素，将不合顺序的交换。每一轮遍 历都将下一个最大值放到正确的位置 上。本质上，每个元素通过“冒泡”找到 自己所属的位置

[2, 1, 4, 3, 6 , 5]

124365

124365

123465

-选择排序

选择排序 在冒泡排序的基础上做了改 进，每次遍历列表时只做一次交换。要 实现这一点，选择排序在每次遍历时寻 找最大值，并在遍历完之后将它放到正 确位置上。

[2, 1, 4, 3, 6 , 5]

214356

21435

2134

213

12

-插入排序

插入排序 的时间复杂度也是 O（n）²，但 原理稍有不同。它在列表较低的一端维 护一个有序的子列表，并逐个将每个新 元素“插入”这个子列表

[2, 1, 4, 3]

1243

1243

1234

-希尔排序

-归并排序

-快速排序

-堆排序



-树

什么是树

用树实现映射

用列表实现树

用类和引用实现树

将树实现为递归数据结构

用堆实现优先级队列

-图

什么是图？

邻接矩阵











Python算法

哈希，查找，排序，递归，复杂度

## 3，单元测试相关

什么是单元测试

Unit testing

针对程序模块进行正确性检验

一个函数，一个类进行验证

自底向上保证程序正确性

为什么要写单元测试

三无代码不可取（无文档，无注释，无单测）

保证代码逻辑的正确性

单测影响设计，易测的代码往往是高内聚低耦合的

回归测试，防止改一处整个服务不可用（屎山）

单元测试相关库

nose/pytest较为常用

mock模块用啦模拟替换网络请求

coverage统计测试覆盖率

举例

```
Binarysearch.py

def binary_search(array,target):

pass

def test():

“””
```





如何设计测试用例：

-正常值功能测试

-边界值（比如最大最小，最左最右值）

-异界值（比如None， 控制，非法值）

 

”””

\#正常值，包含有和无两种结果

assert  binary_search([0,1,2,3,4,5], 1) == 1

执行pytest可以找到相应文件中所有test的函数，通过会显示绿色

## 4，数据库（重点）

数据库问题：

布隆过滤器：

https://blog.csdn.net/songszp/article/details/126714149

http://wjhsh.net/xiao-xue-di-p-14468589.html

Redis是什么？

Redis是一个基于内存的key-value存储系统，数据结构包括字符串，list，set，zset，和hash，bitmap，geohash，hyperloglog,streams

你在哪些场景使用Redis？

作为队列使用，（因为是基于内存、一般不会作为消费队列、作为[循环队列](https://www.zhihu.com/search?q=循环队列&search_source=Entity&hybrid_search_source=Entity&hybrid_search_extra={"sourceType"%3A"answer"%2C"sourceId"%3A2531388973})必要适用）；

模拟类似于token这种需要设置过期时间的场景，登录失效；

分布式缓存，避免大量请求底层关系型数据库，大大降低数据库压力；

分布式锁；

基于 bitmap 实现布隆过滤器；

排行榜-基于zset（有序集合数据类型）；

计数器-对于浏览量、播放量等并发较高，使用 redis incr 实现计数器功能；

分布式会话；

消息系统；

为什么Redis是单线程的？

因为Redis是基于内存的操作，CPU不是Redis的瓶颈，Redis的瓶颈最有可能是机器内存的大小或者网络带宽。既然单线程容易实现，而且CPU不会成为瓶颈，那就顺理成章地采用单线程的方案了。

Redis持久化有几种方式？

redis 提供了两种持久化的方式，分别是**快照方式（RDB Redis DataBase）和文件追加（AOF Append Only File）**。

显而易见，快照方式重启恢复快、但是数据更容易丢失，文件追加数据更完整、重启恢复慢。

**混合持久化方式**，Redis 4.0之后新增的方式，混合持久化是结合RDB和AOF的优点，在写入的时候先把当前的数据以RDB的形式写入到文件的开头，再将后续的操作以AOF的格式存入文件当中，这样既能保证重启时的速度，又能降低数据丢失的风险。

在恢复时，先恢复快照方式保存的文件，然后再恢复追加文件中的增量数据。

什么是缓存穿透？怎么解决？

缓存穿透是指用户请求的数据在缓存中不存在即没有命中，同时在数据库中也不存在，导致用户每次请求该数据都要去数据库中查询一遍，然后返回空。

如果有恶意攻击者不断请求系统中不存在的数据，会导致短时间大量请求落在数据库上，造成数据库压力过大，甚至击垮数据库系统。

这就叫做缓存穿透。

怎么解决？

对查询结果为空的情况也进行缓存，缓存时间设置短一点，或者该key对应的数据insert之后清理缓存。  

对一定不存在的key进行过滤。可以把所有的可能存在的key放到一个大的Bitmap中，查询时通过该Bitmap过滤。(也就是布隆过滤器的原理：**[大白话讲解布隆过滤器](https://link.zhihu.com/?target=https%3A//mp.weixin.qq.com/s%3F__biz%3DMzUzNDUyOTY0Nw%3D%3D%26mid%3D2247483968%26idx%3D1%26sn%3D0251e9eed08ca688cc4442cf8223b643%26chksm%3Dfa921140cde5985647e8b5a98b0e0270d59f962a2daba6784b26546a892677e5d7965811b402%26token%3D529078779%26lang%3Dzh_CN%23rd)**)  

什么事缓存雪崩？

缓存雪崩是指缓存中数据大批量到过期时间，而查询数据量巨大，请求直接落到数据库上，引起数据库压力过大甚至宕机。和缓存击穿不同的是，缓存击穿指并发查同一条数据，缓存雪崩是不同数据都过期了，很多数据都查不到从而查数据库。

怎么解决？

常用的解决方案有：

- 均匀过期
- 加互斥锁
- 缓存永不过期
- 双层缓存策略

**均匀过期**：设置不同的过期时间，让缓存失效的时间点尽量均匀。通常可以为有效期增加随机值或者统一规划有效期。

**加互斥锁**：跟缓存击穿解决思路一致，同一时间只让一个线程构建缓存，其他线程阻塞排队。

**缓存永不过期**：跟缓存击穿解决思路一致，缓存在物理上永远不过期，用一个异步的线程更新缓存。

**双层缓存策略**：使用主备两层缓存：

主缓存：有效期按照经验值设置，设置为主读取的缓存，主缓存失效后从数据库加载最新值。

备份缓存：有效期长，获取锁失败时读取的缓存，主缓存更新时需要同步更新备份缓存。

Redis使用上如何做内存优化？

**缩短键值的长度**缩短值的长度才是关键，如果值是一个大的业务对象，可以将对象序列化成二进制数组；首先应该在业务上进行精简，去掉不必要的属性，避免存储一些没用的数据；其次是[序列化](https://www.zhihu.com/search?q=序列化&search_source=Entity&hybrid_search_source=Entity&hybrid_search_extra={"sourceType"%3A"answer"%2C"sourceId"%3A2531388973})的工具选择上，应该选择更高效的序列化工具来降低字节数组大小；以JAVA为例，内置的序列化方式无论从速度还是压缩比都不尽如人意，这时可以选择更高效的序列化工具，如: [protostuff](https://www.zhihu.com/search?q=protostuff&search_source=Entity&hybrid_search_source=Entity&hybrid_search_extra={"sourceType"%3A"answer"%2C"sourceId"%3A2531388973})，kryo等

**共享对象池**对象共享池指Redis内部维护[0-9999]的整数对象池。创建大量的整数类型redisObject存在内存开销，每个redisObject内部结构至少占16字节，甚至超过了整数自身空间消耗。所以Redis内存维护一个[0-9999]的整数对象池，用于节约内存。  除了整数值对象，其他类型如list,hash,set,zset内部元素也可以使用整数对象池。因此开发中在满足需求的前提下，尽量使用整数对象以节省内存。

**字符串优化**因为redis的惰性删除机制，字符串缩减后的空间不释放，作为预分配空间保留。尽量做新增不做更新。

**编码优化**所谓编码就是具体使用哪种底层数据结构来实现。编码不同将直接影响数据的内存占用和读写效率。这个需要掌握redis底层的数据结构。

**控制key的数量**

你们Redis使用哪种部署方式？

redis部署分为单节点、主从部署（master-slave）、哨兵部署（Sentinel）、集群部署（cluster）。

单节点：也就是单机部署；

主从部署：分为一主一从或一主多从，主从之间同步分为全量或增量。量同步：master 节点通过 BGSAVE 生成对应的RDB文件，然后发送给slave节点，slave节点接收到写入命令后将master发送过来的文件加载并写入；增量同步：即在 master-slave 关系建立开始，master每执行一次数据变更的命令就会同步至slave节点。一般会将写请求转发到master，读请求转发到slave。提高了redis的性能。

哨兵部署：分别有哨兵集群与Redis的主从集群，哨兵作为操作系统中的一个监控进程，对应监控每一个Redis实例，如果master服务异常（ping pong其中节点没有回复且超过了一定时间），就会多个哨兵之间进行确认，如果超过一半确认服务异常，则对master服务进行下线处理，并且选举出当前一个slave节点来转换成master节点；如果slave节点服务异常，也是经过多个哨兵确认后，进行下线处理。提高了redis集群高可用的特性，及横向扩展能力的增强。

集群部署：属于**“[去中心化](https://www.zhihu.com/search?q=去中心化&search_source=Entity&hybrid_search_source=Entity&hybrid_search_extra={"sourceType"%3A"answer"%2C"sourceId"%3A2531388973})****”**的一种方式，多个 master 节点保存整个集群中的全部数据，而数据根据 key 进行 crc-16 校验算法进行散列，将 key 散列成对应 16383 个  slot，而 Redis cluster 集群中每个 master 节点负责不同的slot范围。每个 master 节点下还可以配置多个  slave 节点，同时也可以在集群中再使用 sentinel 哨兵提升整个集群的高可用性。

Redis实现分布式锁需要注意什么？

1. 加锁过程要保证原子性；
2. 保证谁加的锁只能被谁解锁，即Redis加锁的value，解锁时需要传入相同的value才能成功，保证value唯一性；
3. 设置锁超时时间，防止加锁方异常无法释放锁时其他客户端无法获取锁，同时，超时时间要大于业务处理时间；

使用Redis命令 SET lock_key unique_value NX EX seconds 进行加锁，单命令操作，Redis是串行执行命令，所以能保证只有一个能加锁成功。

线程池的工作原理

当数据库数据量很大时（百万级），许多批量数据修改请求的响应会非常慢，一些不需要即时响应的任务可以放到后台的异步线程中完成，发起异步任务的请求就可以立即响应

选择用[线程池](https://so.csdn.net/so/search?q=线程池&spm=1001.2101.3001.7020)的原因是：线程比进程更为可控。不像子进程，子线程会在所属进程结束时立即结束。线程可共享内存。

请求任务异步处理的原理

使用python manage.py runserver模式启动的Django应用只有一个进程，对于每个请求，主线程会开启一个子线程来处理请求。请求子线程向主线程申请一个新线程，然后把耗时的任务交给新线程，自身立即响应，这就是请求任务异步处理的原理

数据库线程池的实现

可视化线程池

如果想要管理这批异步线程，知道他们是否在运行中，可以使用线程池（ThreadPoolExecutor）。

线程池会先启动若干数量的线程，并让这些线程都处于睡眠状态，当向线程池submit一个任务后，会唤醒线程池中的某一个睡眠线程，让它来处理这个任务，当处理完这个任务，线程又处于睡眠状态。

submit任务后会返回一个期程（future），这个对象可以查看线程池中执行此任务的线程是否仍在处理中

因此可以构建一个全局可视化线程池：

from concurrent.futures.thread import ThreadPoolExecutor

class ThreadPool(object):

def __init__(self):

\# 线程池

self.executor = ThreadPoolExecutor(20)

\# 用于存储每个项目批量任务的期程

self.future_dict = {}

\# 检查某个项目是否有正在运行的批量任务

def is_project_thread_running(self, project_id):

future = self.future_dict.get(project_id, None)

if future and future.running():

\# 存在正在运行的批量任务

return True

return False

\# 展示所有的异步任务

def check_future(self):

data = {}

for project_id, future in self.future_dict.items():

data[project_id] = future.running()

return data

def __del__(self):

self.executor.shutdown()

\# 主线程中的全局线程池

\# global_thread_pool的生命周期是Django主线程运行的生命周期

global_thread_pool = ThreadPool()

使用：

\# 检查异步任务

if global_thread_pool.is_project_thread_running(project_id):

raise exceptions.ValidationError(detail='存在正在处理的批量任务，请稍后重试')

\# 提交一个异步任务

future = global_thread_pool.executor.submit(self.batch_thread, project_id)

global_thread_pool.future_dict[project_id] = future

\# 查看所有异步任务

@login_required

def check_future(request):

data = global_thread_pool.check_future()

return HttpResponse(status=status.HTTP_200_OK, content=json.dumps(data))

串行执行

使用线程锁

在全局线程池中初始化线程锁

class ThreadPool(object):

def __init__(self):

self.executor = ThreadPoolExecutor(20)

self.future_dict = {}

self.lock = threading.Lock()

然后执行线程前需要获取锁并再执行结束后释放锁

def batch_thread(self):

global_thread_pool.lock.acquire()

try:

...

global_thread_pool.lock.release()

except Exception:

trace_log = traceback.format_exc()

logger.error('异步任务执行失败:\n %s' % trace_log)

global_thread_pool.lock.release()

需要捕捉异常预防子线程出错而无法释放锁的情况

异步线程任务执行前先检查数据库连接是否可用，然后关掉不可用连接

由于django的数据库连接是保存到线程本地变量中的，通过ThreadPoolExecutor创建的线程会保存各自的数据库连接。

当连接被保存的时间超过mysql连接的最大超时时间，连接失效，但不会被线程释放。

之后再调起线程执行涉及到数据库操作的异步任务时，会用到失效的数据库连接，导致报错“MySQL server has gone away”。

解决方案是在线程池的所有异步任务执行前先检查数据库连接是否可用，然后关掉不可用连接

def batch_thread(self):

for conn in connections.all():

conn.close_if_unusable_or_obsolete()



数据库优化

了解 Django 中的查询集是优化的关键，因此，请记住以下几点：

- 查询集是惰性的，这意味着在你对查询集执行某些操作（例如对其进行迭代）之前，不会发出相应的数据库请求。

- 始终通过指定要返回的值的数量来限制数据库查询的结果。

- 在 Django 中，查询集可以通过迭代、切片、缓存和 python 方法（例如len()等）进行评估count()。确保充分利用它们。

- Django 查询集被缓存，因此如果你重复使用相同的查询集，将不会发出多个数据库请求，从而最大限度地减少数据库访问。

- 一次检索你需要的所有内容，但请确保你只检索你需要的内容。

- ## Django中的查询优化

  ### 数据库索引

  数据库索引是一种在从数据库中检索记录时加快查询速度的技术。随着应用程序大小的增加，它可能会变慢，并且用户会注意到，因为获取所需数据需要更长的时间。因此，在处理生成大量数据的大型数据库时，索引是一项不可协商的操作。

  索引是一种基于各个字段对大量数据进行排序的方法。当你在数据库中的字段上创建索引时，你将创建另一个数据结构，其中包含字段值以及指向与其相关的记录的指针。然后对该索引结构进行排序，使二进制搜索成为可能。

  例如，这是一个名为 Sale 的 Django 模型：

  ```
  # models.py
  
  from django.db import models
  
  class Sale(models.Model):
      sold_at = models.DateTimeField(
          auto_now_add=True,
      )
      charged_amount = models.PositiveIntegerField()1.2.3.4.5.6.7.8.9.
  ```

  在定义 Django 模型时，可以将数据库索引添加到特定字段，如下所示：

  ```
  # models.py
  
  from django.db import models
  
  class Sale(models.Model):
      sold_at = models.DateTimeField(
          auto_now_add=True,
          db_index=True, #DB Indexing
      )
      charged_amount = models.PositiveIntegerField()1.2.3.4.5.6.7.8.9.10.
  ```

  如果你为此模型运行迁移，Django 将在表 Sales  上创建一个数据库索引，并且它将被锁定直到索引完成。在本地开发设置中，数据量很少，连接很少，这种迁移可能感觉是瞬间的，但是当我们谈论生产环境时，有很多并发连接的大型数据集可能会导致停机，如获取锁和创建数据库索引可能需要很长时间。

  你还可以为两个字段创建单个索引，如下所示：

  ```
  # models.py
  
  from django.db import models
  
  class Sale(models.Model):
      sold_at = models.DateTimeField(
          auto_now_add=True,
          db_index=True, #DB Indexing
      )
      charged_amount = models.PositiveIntegerField()
  
      class Meta:
          indexes = [
              ["sold_at", "charged_amount"]]1.2.3.4.5.6.7.8.9.10.11.12.13.14.
  ```

  ### 数据库缓存

  数据库缓存是从数据库获得快速响应的最佳方法之一。它确保对数据库的调用更少，从而防止过载。标准缓存操作遵循以下结构：

  ![img](https://s6.51cto.com/oss/202208/31/7977c5034faf7075aab97099429b5d3e726b42.png)

  Django 提供了一种缓存机制，可以使用不同的缓存后端，如 Memcached 和 Redis，让你避免多次运行相同的查询。

  Memcached 是一个开源的内存系统，可保证在不到一毫秒的时间内提供缓存结果。它易于设置和扩展。另一方面，Redis 是一种开源缓存解决方案，具有与  Memcached 相似的特性。大多数离线应用程序使用以前缓存的数据，这意味着大多数查询永远不会到达数据库。

  用户会话应该保存在 Django 应用程序的缓存中，并且因为 Redis 在磁盘上维护数据，所以登录用户的所有会话都来自缓存而不是数据库。

  要在 Django 中使用 Memcache，我们需要定义以下内容：

  - **BACKEND：**定义要使用的缓存后端。
  - **LOCATION：**ip:port 值 where ip 是 Memcached 守护程序的 IP 地址， port 是运行 Memcached 的端口，或者是指向你的 Redis 实例的 URL，使用适当的方案。

  要使用 Memcached 启用数据库缓存，请pymemcache使用以下命令使用 pip 进行安装：

  ```
  pip install pymemcache1.
  ```

  然后，你可以settings.py按如下方式配置缓存设置：

  ```
  CACHES = {
      'default': {
          'BACKEND': 'django.core.cache.backends.memcached.PyMemcacheCache',
          'LOCATION': '127.0.0.1:11211',
      }
  }1.2.3.4.5.6.
  ```

  在上面的示例中，Memcached 使用以下 pymemcache 绑定在 localhost (127.0.0.1) 端口 11211 上运行：

  同样，要使用 Redis 启用数据库缓存，请使用以下命令使用 pip 安装 Redis：

  ```
  pip install redis1.
  ```

  tings.py然后通过添加以下代码来配置你的缓存设置：

  ```
  CACHES = {
      'default': {
          'BACKEND': 'django.core.cache.backends.redis.RedisCache',
          'LOCATION': 'redis://127.0.0.1:6379',
      }
  }1.2.3.4.5.6.
  ```

  Memcached 和 Redis 也可用于存储用户身份验证令牌。因为每个登录的人都必须提供一个令牌，所以所有这些过程都会导致大量的数据库开销。使用缓存的令牌将大大加快数据库访问速度。

  ### 尽可能使用迭代器

  Django 中的查询集通常会在评估发生时缓存其结果，对于该查询集的任何进一步操作，它首先检查是否有缓存的结果。但是，当你使用 时iterator()，它不会检查缓存并直接从数据库中读取结果，也不会将结果保存到查询集。

  现在，你一定想知道这有什么帮助。考虑一个查询集，它返回大量具有大量内存的对象进行缓存，但只能使用一次，在这种情况下，你应该使用iterator()。

  例如，在下面的代码中，所有记录将从数据库中获取，然后加载到内存中，然后我们将遍历每条记录：

  ```
  queryset = Product.objects.all()
  for each in queryset:
      do_something(each)1.2.3.
  ```

  而如果我们使用iterator()，Django 将保持 SQL 连接打开并读取每条记录，并 do_something() 在读取下一条记录之前调用：

  ```
  queryset = Product.objects.all().iterator()
  for each in queryset:
      do_something(each)1.2.3.
  ```

  ### 使用持久性数据库连接

  Django 为每个请求创建一个新的数据库连接，并在请求完成后关闭它。这种行为是由 引起的CONN_MAX_AGE，它的默认值为 0。但是应该设置多长时间呢？这取决于你网站上的流量；音量越高，维持连接所需的秒数就越多。通常建议从较低的数字开始，例如 60。

  你需要将额外的选项包装在 中 OPTIONS，如留档中详细说明：

  ```
  DATABASES = {
    'default': {
         'ENGINE': 'django.db.backends.mysql',
         'NAME': 'dashboard',
         'USER': 'root',
         'PASSWORD': 'root',
         'HOST': '127.0.0.1',
         'PORT': '3306',
         'OPTIONS': {
              'CONN_MAX_AGE': '60',
         }
    }
  }1.2.3.4.5.6.7.8.9.10.11.12.13.
  ```

  ### 使用查询表达式

  查询表达式定义了可以在更新、创建、过滤、排序、注释或聚合操作中使用的值或计算。Django 中常用的内置查询表达式是 F 表达式。让我们看看它是如何工作的并且很有用。

  在 Django Queryset API  中，F()表达式用于直接引用模型字段值。它允许你引用模型字段值并对它们执行数据库操作，而无需从数据库中获取它们并进入 Python  内存。相反，Django 使用该F()对象来生成定义所需数据库活动的 SQL 短语。

  例如，假设我们想将所有产品的价格提高 20%，那么代码将如下所示：

  ```
  products = Product.objects.all()
  for product in products:
      product.price *= 1.2
      product.save()1.2.3.4.
  ```

  但是，如果我们使用F()，我们可以在单个查询中执行此操作，如下所示：

  ```
  from django.db.models import F
  
  Product.objects.update(price=F('price') * 1.2)1.2.3.
  ```

  ### 使用 select_related() 和 prefetch_related()

  Django 通过最小化数据库请求的数量来提供优化查询集select_related()的prefetch_related()参数。

  根据官方 Django 文档：

  *select_related() “遵循”外键关系，在执行查询时选择其他相关对象数据。*

  *prefetch_related() 对每个关系进行单独的查找，并在 Python 中进行“加入”。*

  #### select_related()

  我们select_related()在要选择的项目是单个对象时使用，这意味着 forward ForeignKey、OneToOne和 backOneToOne字段。

  你可以使用select_related()创建单个查询，该查询返回单个实例的所有相关对象，用于一对多和一对一连接。执行查询时，select_related()从外键关系中检索任何额外的相关对象数据。

  select_related()通过生成 SQL 连接并在SELECT表达式中包含相关对象的列来工作。因此，select_related()在同一数据库查询中返回相关项目。

  虽然select_related()会产生更复杂的查询，但获取的数据会被缓存，因此处理获取的数据不需要任何额外的数据库请求。

  语法看起来像这样：

  ```
  queryset = Tweet.objects.select_related('owner').all()1.
  ```

  #### prefetch_related()

  相反，prefetch_related()用于多对多和多对一连接。它生成一个查询，其中包括查询中给出的所有模型和过滤器。

  语法看起来像这样：

  ```
  Book.objects.prefetch_related('author').get(id=1).author.first_name1.
  ```

  ### 使用bulk_create()和bulk_update()

  bulk_create() 是一种通过一次查询将提供的对象列表创建到数据库中的方法。类似地，bulk_update() 是一种使用一个查询更新提供的模型实例上的给定字段的方法。

  例如，如果我们有一个如下所示的帖子模型：

  ```
  class Post(models.Model):
      title = models.CharField(max_length=300, unique=True)
      time = models.DateTimeField(auto_now_add=True)
      def __str__(self):
          return self.title1.2.3.4.5.
  ```

  现在，假设我们要在这个模型中添加多条数据记录，那么我们可以bulk_create()这样使用：

  ```
  #articles
  articles  = [Post(title="Hello python"), Post(title="Hello django"), Post(title="Hello bulk")]
  
  #insert data
  Post.objects.bulk_create(articles)1.2.3.4.5.
  ```

  输出如下所示：

  ```
  >>> Post.objects.all()
  
  <QuerySet [<Post: Hello python>, <Post: Hello django>, <Post: Hello bulk>]>1.2.3.
  ```

  如果我们想更新数据，那么我们可以bulk_update()这样使用：

  ```
  update_queries = []
  
  a = Post.objects.get(id=14)
  b = Post.objects.get(id=15)
  c = Post.objects.get(id=16)
  
  #set update value
  a.title="Hello python updated"
  b.title="Hello django updated"
  c.title="Hello bulk updated"
  
  #append
  update_queries.extend((a, b, c))
  
  Post.objects.bulk_update(update_queries, ['title'])1.2.3.4.5.6.7.8.9.10.11.12.13.14.15.
  ```

  输出如下所示：

  ```
  >>> Post.objects.all()
  
  
  <QuerySet [<Post: Hello python updated>, <Post: Hello django updated>, <Post: Hello bulk updated>]>1.2.3.4.
  ```

  ## 

Redis支持的数据类型

Redis缓存淘汰的策略

1、Redis内存不足的缓存淘汰策略提供了8种。

noeviction：当内存使用超过配置的时候会返回错误，不会驱逐任何键。

allkeys-lru：加入键的时候，如果过限，首先通过LRU算法驱逐最久没有使用的键。

volatile-lru：加入键的时候如果过限，首先从设置了过期时间的键集合中驱逐最久没有使用的键。

allkeys-random：加入键的时候如果过限，从所有key随机删除。

volatile-random：加入键的时候如果过限，从过期键的集合中随机驱逐。

volatile-ttl：从配置了过期时间的键中驱逐马上就要过期的键。

volatile-lfu：从所有配置了过期时间的键中驱逐使用频率最少的键。

allkeys-lfu：从所有键中驱逐使用频率最少的键。



2、这八种大体上可以分为4中，lru、lfu、random、ttl。

lru：Least Recently Used)，最近最少使用。

lfu：Least Frequently Used，最不经常使用法。

ttl：Time To Live，生存时间。

random：随机。



3、默认是noeviction。对于写请求不再提供服务，直接返回错误（DEL请求和部分特殊请求除外。

4、eviction：“逐出；赶出；收回”。

5、volatile：“不稳定的”。



MySQL中myisam与innodb区别

**主要有以下区别：**

1、MySQL默认采用的是MyISAM。

2、MyISAM不支持事务，而InnoDB支持。InnoDB的AUTOCOMMIT默认是打开的，即每条SQL语句会默认被封装成一个事务，自动提交，这样会影响速度，所以最好是把多条SQL语句显示放在begin和commit之间，组成一个事务去提交。

3、InnoDB支持数据行锁定，MyISAM不支持行锁定，只支持锁定整个表。即MyISAM同一个表上的读锁和写锁是互斥的，MyISAM并发读写时如果等待队列中既有读请求又有写请求，默认写请求的优先级高，即使读请求先到，所以MyISAM不适合于有大量查询和修改并存的情况，那样查询进程会长时间阻塞。因为MyISAM是锁表，所以某项读操作比较耗时会使其他写进程饿死。

4、InnoDB支持外键，MyISAM不支持。

5、InnoDB的主键范围更大，最大是MyISAM的2倍。

6、InnoDB不支持全文索引，而MyISAM支持。全文索引是指对char、varchar和text中的每个词（停用词除外）建立倒排序索引。MyISAM的全文索引其实没啥用，因为它不支持中文分词，必须由使用者分词后加入空格再写到数据表里，而且少于4个汉字的词会和停用词一样被忽略掉。

7、MyISAM支持GIS数据，InnoDB不支持。即MyISAM支持以下空间数据对象：Point,Line,Polygon,Surface等。



8、没有where的count(*)使用MyISAM要比InnoDB快得多。因为MyISAM内置了一个计数器，count(*)时它直接从计数器中读，而InnoDB必须扫描全表。

所以在InnoDB上执行count(*)时一般要伴随where，且where中要包含主键以外的索引列。

为什么这里特别强调“主键以外”？因为InnoDB中primary index是和raw data存放在一起的，而secondary index则是单独存放，然后有个指针指向primary key。

所以只是count(*)的话使用secondary index扫描更快，而primary key则主要在扫描索引同时要返回raw data时的作用较大。



在Python项目开发中如何避免SQL注入

python中拼接动态sql的多种方式

在python中,对于这条动态sql的拼接至少存在以下四种方案

1. %s占位符形式

```sql
sql = "SELECT vip, coin FROM user_asset WHERE uid='%s' " % uid



cursor.execute(sql)
```

1. format形式

```sql
sql = "SELECT vip, coin FROM user_asset WHERE uid='{}' ".format(uid)



cursor.execute(sql)
```

1. f string形式

```sql
sql = f"SELECT vip, coin FROM user_asset WHERE uid='{uid}' "



cursor.execute(sql)
```

1. MySQLdb定义的 %s占位符形式

```sql
sql = "SELECT vip, coin FROM user_asset WHERE uid=%s "



cursor.execute(sql, (uid, ))
```

其中1,2,3三种方式均是通过python本身的占位符语法先动态生成完整sql，而后直接提交到db执行，我们将其归为第一类，后面均以第1种方式作为代表进行分析，第4种方法则归为第二类。

MySQLdb的字符串格式化不是标准的python的字符串格式化,
 应当一直使用%s用于字符串格式化

**python中无论整数，字符串占位符都为 %s，且不需加单引号**

存在[sql注入](https://so.csdn.net/so/search?q=sql注入&spm=1001.2101.3001.7020)风险的第一类方法

第一类方法其实十分危险，是需要我们极力避免的错误方式，因为它存在确切的sql注入风险。具体分析来看，uid作为一个字符串类型，要想生成sql中带引号的参数，需要额外再在占位符两侧添加引号才行，否则将生成错误的sql，如下例:

```sql
In [4]: "SELECT vip, coin FROM user_asset WHERE uid='%s' " % uid



Out[4]: "SELECT vip, coin FROM user_asset WHERE uid='u123456' " # 加引号输出为合法sql



In [5]: "SELECT vip, coin FROM user_asset WHERE uid=%s " % uid



Out[5]: 'SELECT vip, coin FROM user_asset WHERE uid=u123456 # 不加引号输出为非法sql
```

问题在于uid的来源并不一定是可信的，如果uid参数是由客户端直接传过来、或者其他不可信的恶意来源传递，服务端直接取用该参数拼接sql的话，就可能直接被sql注入攻击，比如客户端传递恶意的uid本身带有引号的情况，则可以生成包括以下sql在内的各种恶意sql:

```perl
In [46]: uid="' or 1 or ''='"



In [47]: "SELECT vip, coin FROM user_asset WHERE uid='%s' " % uid



Out[47]: "SELECT vip, coin FROM user_asset WHERE uid='' or vip or '___'='' " # 匹配所有VIP



 



In [48]: uid="' or coin>100 or '___'='"



In [49]: "SELECT vip, coin FROM user_asset WHERE uid='%s' " % uid



Out[49]: "SELECT vip, coin FROM user_asset WHERE uid='' or coin>100 or '___'='' " # 匹配所有游戏币>100的用户



 



In [62]: uid = "'; delete FROM test_user_asset WHERE ''='"



In [63]:  "SELECT vip, coin FROM user_asset WHERE uid='%s' " % uid



Out[63]: "SELECT vip, coin FROM user_asset WHERE uid=''; delete FROM test_user_asset WHERE ''='' " # 极端恶意！删除全表记录
```

由此可见，通过使用python占位符直接拼装sql执行，是十分危险的行为。

防止注入的安全方式

事实上，在各类语言中拼装sql的标准写法应该都是采用第4种方式，即传入包含占位符的sql与参数列表，由库内部处理最终sql的拼装，其内部会对参数进行保护性转义之后再拼入sql之中。
 那MySQLdb内部具体是如何处理参数转义拼接的呢？有没有办法可以得到最终拼装完成的sql在日志中输出方便调试呢？

cursor.execute内部的参数转义机制

先看第一个问题，通过查看源码可以在MySQLdb的cursors.py 中找到execute函数定义，其中有如下代码：

```python
    def execute(self, query, args=None):



        """Execute a query.







        query -- string, query to execute on server



        args -- optional sequence or mapping, parameters to use with query.







        Note: If args is a sequence, then %s must be used as the



        parameter placeholder in the query. If a mapping is used,



        %(key)s must be used as the placeholder.







        Returns integer represents rows affected, if any



        """



        while self.nextset():



            pass



        db = self._get_db()



 



        if isinstance(query, unicode):



            query = query.encode(db.encoding)



 



        if args is not None:



            if isinstance(args, dict):



                nargs = {}



                for key, item in args.items():



                    if isinstance(key, unicode):



                        key = key.encode(db.encoding)



                    nargs[key] = db.literal(item)



                args = nargs



            else:



                args = tuple(map(db.literal, args))



            try:



                query = query % args



            except TypeError as m:



                raise ProgrammingError(str(m))



        assert isinstance(query, (bytes, bytearray))



        res = self._query(query)



        return res
```

可以看到，如果传入args为tuple，则将通过`args = tuple(map(db.literal, args))`将其每个参数通过db.literal进行转义，最终还是通过 `query = query % args` 生成字符串，由于所有参数都已经经过转义了，所以能避免之前的注入问题。
 那么能不能得到execute内部最终生成的这个query sql呢，很遗憾我们发现query是个函数内的局部变量，所以外部是无法直接获取其值的。当然如果一定要获取最终生成的sql也不是没办法，可以在代码中模拟这一literal操作拼接sql，而后输出。
 接下来探究一下db.literal是个什么函数，外部能否直接调用它。

Connection.literal函数

经过一通查找，发现literal函数定义在connections.py文件中：

```python
    def literal(self, o):



        """If o is a single object, returns an SQL literal as a string.



        If o is a non-string sequence, the items of the sequence are



        converted and returned as a sequence.







        Non-standard. For internal use; do not use this in your



        applications.



        """



        if isinstance(o, unicode):



            s = self.string_literal(o.encode(self.encoding))



        elif isinstance(o, bytearray):



            s = self._bytes_literal(o)



        elif isinstance(o, bytes):



            if PY2:



                s = self.string_literal(o)



            else:



                s = self._bytes_literal(o)



        elif isinstance(o, (tuple, list)):



            s = self._tuple_literal(o)



        else:



            s = self.escape(o, self.encoders)



            if isinstance(s, unicode):



                s = s.encode(self.encoding)



        assert isinstance(s, bytes)



        return s
```

可以看到，db.literal其实就是根据传入参数的类型，再调用不同类型的literal方法对其进行转义，而且db.literal本身是个实例方法，这意味着至少需要一个Connection 实例才可以引用到这一个方法。

使用literal生成防sql注入的最终sql

通过初始化一个Connection示例，便可以调用其literal方式进行参数转义了，以下示例代码演示了通过literal对参数转义生成最终防注入风险的安全sql：

```python
#!/usr/bin/python3



import MySQLdb



 



conn = MySQLdb.connect(host="127.0.0.1", port=3306, user="test", password="test123", db="test")



curosr = conn.cursor()



sql0 = "SELECT vip, coin FROM user_asset WHERE uid='%s' " # str类型直接占位替换需要加上引号



sql1 = "SELECT vip, coin FROM user_asset WHERE uid=%s " # 占位符%s会通过库内部literal处理转义, 直接使用即可



uid = "u123456"



print('\nuid=%s' % uid)



args = (uid, )



print("0:", sql0 % args) # 直接占位符替换



print("1:", (sql1.encode() % tuple(map(conn.literal, args))).decode()) # 通过literal处理后占位符替换, 生成为bytes类型, decode为str类型后输出



 



uid = "' or 1 or ''='"



print('\nuid=%s' % uid)



args = (uid, )



print("0:", sql0 % args) # 直接占位符替换



print("1:", (sql1.encode() % tuple(map(conn.literal, args))).decode()) # 通过literal处理后占位符替换, 生成为bytes类型, decode为str类型后输出



 uid = "'; delete FROM test_user_asset WHERE ''='"



print('\nuid=%s' % uid)



args = (uid, )



print("0:", sql0 % args) # 直接占位符替换



print("1:", (sql1.encode() % tuple(map(conn.literal, args))).decode()) # 通过literal处理后占位符替换, 生成为bytes类型, decode为str类型后输出
```

输出结果：

```sql
uid=u123456



0: SELECT vip, coin FROM user_asset WHERE uid='u123456'



1: SELECT vip, coin FROM user_asset WHERE uid='u123456'



uid=' or 1 or ''='



0: SELECT vip, coin FROM user_asset WHERE uid='' or 1 or ''=''



1: SELECT vip, coin FROM user_asset WHERE uid='\' or 1 or \'\'=\''



 



uid='; delete FROM test_user_asset WHERE ''='



0: SELECT vip, coin FROM user_asset WHERE uid=''; delete FROM test_user_asset WHERE ''=''



1: SELECT vip, coin FROM user_asset WHERE uid='\'; delete FROM test_user_asset WHERE \'\'=\''
```

可以看到，uid内部添加的单引号'都会被'转义后才拼入sql之中。
 需要注意的是，Connection.literal函数注释已明确说明该函数是`Non-standard. For internal use; do not use this in your applications.`，所以该函数的直接调用应仅限于调试用途，不可用于线上业务逻辑，同时由于必须现在实例化一个Connection对象才可调用其literal方法，要注意连接的正常关闭，防止泄漏。

MySQL常考，索引优化

索引优化

1. 尽量全值匹配

    当建立索引后，能再where条件中使用索引列，就尽量使用。
    例如 alter table staffs add index idx_staffs_nameAgePos(name,age,pos);
    尽量加上三个列在where里，EXPLAIN SELECT * FROM staffs WHERE NAME = 'July' AND age = 25 AND pos = 'dev'

2. 最佳左前缀法则

    如果是复合索引，就要遵守最左前缀法则，意思是：查询从最左前列开始，并且不跳过索引中的列。
    同样索引列是name,age,pos。
    EXPLAIN SELECT * FROM staffs WHERE NAME = 'July'; （会用索引NAME ）
    EXPLAIN SELECT * FROM staffs WHERE NAME = 'July' AND AGE = 25;（会用索引NAME 和AGE）
    EXPLAIN SELECT * FROM staffs WHERE NAME = 'July' AND POS= 'dev'（只会用到NAME，因为跳过了AGE）
    EXPLAIN SELECT * FROM staffs WHERE AGE = 25 AND POS='dev';（不会用索引，因为最左前列NAME没有使用）

3. 不在索引列上做任何操作

    不在索引列上（计算，函数，自动或者手动的进行类型转换），会导致索引失效。
    EXPLAIN SELECT * FROM staffs WHERE left(NAME,4) = 'July';（不会用到索引）

4. 范围条件放最后（是指索引定义顺序的最后）

    EXPLAIN SELECT * FROM staffs WHERE NAME = 'July' and age =22 and pos='manager'
    中间有范围查询会导致后面的索引列全部失效(按照name,age,pos，索引创建的顺序，age后面的POS会失效)
    EXPLAIN SELECT * FROM staffs WHERE NAME = 'July' and age >22 and pos='manager' （索引只会用到NAME 和AGE列）
    对于in条件查询，如果索引没有生效，使用in不会有影响；如果索引有效，使用in则会进行全表扫描
    EXPLAIN SELECT * FROM staffs WHERE NAME = 'July' and age = 22 and pos in ('a','b' ) (name，age，pos索引都存在，没有跳过的，pos使用in则会导致全表扫描)
    EXPLAIN SELECT * FROM staffs WHERE NAME = 'July' and pos in ('a','b' ) （索引age跳过了，所以pos不会生效，加in不会对索引有影响）

5. 尽量使用覆盖索引

    覆盖索引（只访问索引的查询（索引列和查询列一致）），而尽量避免 select *

6. 不等于要慎用

    在使用不等于（!= 或者<>），会导致索引失效
    EXPLAIN SELECT * FROM staffs WHERE NAME <> 'July';
    如果定要需要使用不等于,请用覆盖索引
    EXPLAIN SELECT name,age,pos FROM staffs WHERE NAME != 'July';

7. NULL和Not NULL要慎用

    |-- 在字段为NOT NULL的情况下，如果使用 is null 或者 is not null，会导致索引失效。解决方案（覆盖索引）
    EXPLAIN select * from staffs where name is not null （索引失效）
    EXPLAIN select name,age,pos from staffs where name is not null （可以使用索引）
    |-- 在字段为可以为NULL的情况下，使用IS NULL，索引正常；使用 IS NOT NULL，则索引失效。（解决方案同上，覆盖索引）

8. LIKE查询要当心

    like以通配符开头('%abc...')mysql索引失效会变成全表扫描的操作。
    EXPLAIN select * from staffs where name like '%july%' （索引失效，一定要用的话，用覆盖索引）
    EXPLAIN select * from staffs where name like 'july%' （可以使用索引，类似遵循最左前缀原则）

9. 字符类型加引号

    EXPLAIN select * from staffs where name = 917 （索引失效）
    EXPLAIN select * from staffs where name = ‘917’ （可以使用索引）

10. OR改UNION效率高

    EXPLAIN select * from staffs where name='July' or name = 'z3' （索引失效，解决方案UNION或者覆盖索引）
    |-- 解决方式：UNION
    EXPLAIN
    select * from staffs where name='July'
    UNION
    select * from staffs where name = 'z3'
    |-- 解决方式：覆盖索引
    EXPLAIN
    select name,age from staffs where name='July' or name = 'z3'

除了索引优化之外，还有一些查询优化的技巧：

1. 延迟关联

select c1 from table where c2='M' order by c3 limit 100000, 10; 类似这种分页查询
可以通过延迟关联(deferred join)技术，通过覆盖索引（c2+c3）来得到所需要的主键，然后再根据这些主键关联原表获得需要的行。

    select c1 
    from table,  (select id from table where c2='M' order by c3 limit 100000, 10) as table2
    where table.id = table2.id 

2. LIMIT 1

当查询结果只可能为1条数据的时候，加上LIMIT 1可以增加性能，MySQL数据库引擎会在找到一条数据后停止搜索，而不是继续往后查少下一条符合记录的数据。


-Crud

CRUD    增删改查

 1.新增(INSERT)

语法：INSERT INTO 表名(列1,列2,...,列N) values(值1,值2,...,值N)

注：
   1) 如果数据是字符型，必须使用单引号或者双引号，如："value";
   2) 在缺省列名的情况，插入不能少或多字段值;
   3) 在插入部分字段时，要指定插入的数据字段并依此插入;

   2.删除(DELETE)

DELETE语法：DELETE FROM 表名 WHERE 条件

3.修改(UPDATE)

 语法：UPDATE 表名 set 列1=值1,列2=值2,... WHERE 条件

   注：
   1) 不推荐使用全表修改所有字段数据，除非有特殊需求; 
   2) 推荐使用带条件方式修改字段数据;

4.查询(SELECT)

语法：SELECT * FROM 表名 WHERE 条件

分组：group by

模糊查询：like '%关键字%'

   注：
   1) 在查询时，推荐使用指定查询字段的方式，效率更高;

查询优化
1)不使用select*
2)尽量不使用子查询
3)尽量不使用null值查询   

事务的原理，特性，事务并发控制

什么是事务？Transaction，转账操作

事务是数据库并发控制的基本单位

事务可以看做是一系列SQL语句的集合

事务必须要么全部执行成功，要么全部执行失败，回滚

事务的ACID特性和含义

原子性

一致性

隔离性

持久性：事务结束之后，修改是永久的不会丢失

-事务的并发控制，四种异常情况

幻读

非重复读

脏读

丢失修改

四种事务隔离级别

读未提交

读已提交

可重复读

串行化

如何解决高并发场景下的插入重复

使用数据库的唯一索引

使用队列异步写入

使用Redis等实现分布式锁

-乐观锁和悲观锁

 

 

常用的字段，含义和区别

字符串，数值，日期和时间

数据常用的引擎和区别

InnoDB，和myisam

大多默认用InnoDB

-MySQL索引原理及优化常见考题

MySQL索引

索引的原理，类型，结构，

什么是索引

索引的类型，

创建索引的注意事项， 使用原则

索引失效

什么情况使用不同的索引

 

如何排查和消除慢查询

-SQL语句以考察各种常用连接

内连接

外连接

全连接

 

Redis缓存

-缓存的使用场景；Redis的使用，缓存使用中的坑

什么是缓存

为什么要使用加密码，使用场景

Redis的常用数据类型，使用方式，

缓存使用问题：诗句一致性问题，缓存穿透，击穿，雪崩问题

MySQL和Redis

1. Redis incr实现计数器功能

2. Redis与MySQL如何实现数据同步

   1，MySQL查完数据，再同步写入到Redis中

   缺点1：会对接口造成延迟，因为同步写入Redis本身就有延迟，并且还要做重试，如果Redis写入失败，还需要重试，那就更费时间了

   缺点2：不解耦，如果Redis蹦了，那直接卡线程了

   <font color = 'blue'> 缺点3：如果人为该数据库，那就没法同步了，除非再人为删除对应得Redis，但删除Redis这个过程也有个时间差</font>

   2，MySQL查完数据，通过发送MQ，在消费者线程去同步Redis

   缺点1：多了层MQ，也就是会有很大的概率导致同步延迟问题

   缺点2：要对MQ的可用性做预防

   <font color = 'blue'>缺点3：如果人为该数据库，那就没办法同步了</font>

   优点1：可以大幅减少接口的延迟返回的问题

    优点2：MQ本身有重试机制，无需人工去写重试代码

    优点3：解耦，把查询MySQL和同步Redis完全分离，互不干扰

   3，订阅MySQL的binlog文件（可借助canal来进行）

   canalserver会伪装成MySQLserver从库，去订阅MySQLserver主库的binlog文件

   canal启动的时候会配置对应得消息MQ（rabbitMQ，rocketMQ， kafka），监听到binlog文件有变化时，会吧变化的SQL语句转换成json格式，并作为消息内容发送到MQ中

   项目中只要监听到对应MQ，8就能拿到binlog改动的内容，json数据中心有明确的操作类型（curd），以及对应得数据，把对应数据同步到Redis即可

   <font color = 'blue'>缺点1：canal订阅binlog的整个操作过程是单线程的，所以面临超高并发的情况下，性能可能不太出色，当然可以部署多个canal与多个消费者，但是要注意消息重复消费问题，做好幂等性校验</font>

   <font color = 'blue'>优点1：即使人为改数据库，也会监听到，并且也会同步</font>

   <font color = 'blue'>优点2：异步同步，不会对接口返回有格外延迟</font>

   4，延迟双删

   在执行修改SQL之前，先将Redis的数据删除

   执行更新SQL

   延迟一段时间

   再次删除Redis的数据

   ```
   //延迟双删伪代码
   
   deleteRedisCache(key); //删除redis缓存
   
   updateMysql(obj);	//更新MySQL
   
   Thread.sleep(100); 	//延迟一段时间
   
   deleteRedisCache(key);	//再次删除该key的缓存
   ```

   <font color = 'blue'>缺点：这个延迟时间不好把控，到底延迟多久这个很难去评估</font>>

   扩展：如果不适用延迟双删，仅仅是delete缓存，然后改MySQL数据。只有这两步会出现什么问题呢？

   5，单个请求，单线程没问题，高并发多线程下会出问题

   6，如果Thread1线程要更新数据，此时Thread1线程把Redis清理了

   7，此时Thread2线程来了，但Thread1还没有更新完MySQL完毕

   8，Thread2查询Redis肯定是null，此时Thread2就要查MySQL了，然后再把查到的数据写到缓存

   9，由于Thread1还没来得及修改MySQL数据，所以此时Thread2查出来的数据是旧数据，Thread2把旧数据又写入到Redis了

   10，此时Thread3线程来了，查询Redis发现有数据，则直接拿缓存数据了，此时Thread3查出来的是旧数据，直接带着旧数据返回了，这就是问题所在

   11，而延迟双删的第二次删除作用就是防止Thread2把旧数据又写入了，有了延迟双删，Thread3查询Redis的时候还是null，就会从MySQL去拿新数据了

   12，所以正常的这个延迟时间，应该是Thread2查缓存到拿MySQL数据，到再保存到Redis这整个时间，作为Thread1的延迟时间，但是这个Thread2这个过程的时间会受到很多因素影响，因此很难断定究竟会是多久

   5，延迟双写

   ```
   
   
    //延迟双写伪代码
   
    updateMysql(obj);	//更新mysql 
   
    addRedis(key);	//再次删除该key的缓存
   ```

   上述代码缺陷：

   高并发下，两条线程同时执行上面代码，并对MySQL修改，且修改内容不通，可能会导致Redis与MySQL数据不椅子

   T1线程执行完updateMysql，释放了行锁，此时T2线程再执行updateMysql与addRedis，最后T1执行addRedis，这种情况会导致数据库改成了T2线程的数据，但Redis确实T1线程的数据

   优化

   ```
   //完美延迟双写伪代码
   
   开启事务
   
   uodateMysql(obj);  //更新MySQL
   
   addreids(key);    //再次删除该key的缓存
   ```

   提交事务 

   上述代码改正，把两句代码放到一个事务里面，只有T1执行完MySQL与Redis的时候，T2才能开始执行，就可以保证数据一致性，推荐使用分布式锁双写
   
   缺点：MySQL与Redis是单线程的，性能方面不行，因此不推荐使用
   
   <font color = 'blue'>总结：推荐使用canal的方式，进行异步同步。其次是MQ方式</font>>
   
   ------

## 5，网络编程

   常用协议TCP/UDP/HTTP

-简述 OSI 七层协议。

### 应用层

与其它计算机进行通讯的一个应用，它是对应应用程序的通信服务的。例如，一个没有通信功能的字处理程序就不能执行通信的[代码](https://baike.baidu.com/item/代码?fromModule=lemma_inlink)，从事字[处理](https://baike.baidu.com/item/处理?fromModule=lemma_inlink)工作的程序员也不关心OSI的第7层。但是，如果添加了一个传输文件的选项，那么字[处理器](https://baike.baidu.com/item/处理器?fromModule=lemma_inlink)的程序就需要实现OSI的第7层。示例：[TELNET](https://baike.baidu.com/item/TELNET/810597?fromModule=lemma_inlink)，[HTTP](https://baike.baidu.com/item/HTTP/243074?fromModule=lemma_inlink)，[FTP](https://baike.baidu.com/item/FTP/13839?fromModule=lemma_inlink)，[NFS](https://baike.baidu.com/item/NFS/812203?fromModule=lemma_inlink)，[SMTP](https://baike.baidu.com/item/SMTP/175887?fromModule=lemma_inlink)等。



### 表示层

这一层的主要功能是定义数据格式及加密。例如，FTP允许你选择以二进制或ASCII格式传输。如果选择二进制，那么发送方和接收方不改变文件的内容。如果选择ASCII格式，发送方将把文本从发送方的[字符集](https://baike.baidu.com/item/字符集?fromModule=lemma_inlink)转换成标准的ASCII后发送数据。在接收方将标准的ASCII转换成接收方计算机的字符集。示例：加密，ASCII等。



### 会话层

它定义了如何开始、控制和结束一个会话，包括对多个双向消息的控制和管理，以便在只完成连续消息的一部分时可以通知应用，从而使表示层看到的数据是连续的，在某些情况下，如果表示层收到了所有的[数据](https://baike.baidu.com/item/数据/5947370?fromModule=lemma_inlink)，则用数据代表表示层。示例：RPC，SQL等。



### 传输层

这层的功能包括是选择差错恢复协议还是无差错恢复协议，及在同一[主机](https://baike.baidu.com/item/主机?fromModule=lemma_inlink)上对不同应用的[数据流](https://baike.baidu.com/item/数据流?fromModule=lemma_inlink)的输入进行复用，还包括对收到的顺序不对的[数据包](https://baike.baidu.com/item/数据包?fromModule=lemma_inlink)的重新排序功能。示例：[TCP](https://baike.baidu.com/item/TCP/33012?fromModule=lemma_inlink)，[UDP](https://baike.baidu.com/item/UDP/571511?fromModule=lemma_inlink)，[SPX](https://baike.baidu.com/item/SPX/610336?fromModule=lemma_inlink)。



### 网络层

这层对端到端的包传输进行定义，它定义了能够标识所有结点的[逻辑地址](https://baike.baidu.com/item/逻辑地址?fromModule=lemma_inlink)，还定义了[路由](https://baike.baidu.com/item/路由?fromModule=lemma_inlink)实现的方式和学习的方式。为了适应[最大传输单元](https://baike.baidu.com/item/最大传输单元?fromModule=lemma_inlink)长度小于包长度的[传输介质](https://baike.baidu.com/item/传输介质?fromModule=lemma_inlink)，网络层还定义了如何将一个包分解成更小的包的分段方法。示例：IP，IPX等。



### 数据链路层

它定义了在单个链路上如何传输数据。这些协议与被讨论的各种介质有关。示例：[ATM](https://baike.baidu.com/item/ATM/8314845?fromModule=lemma_inlink)，[FDDI](https://baike.baidu.com/item/FDDI/572177?fromModule=lemma_inlink)等。



### 物理层

OSI的物理层规范是有关[传输介质](https://baike.baidu.com/item/传输介质?fromModule=lemma_inlink)的特性，这些规范通常也参考了其他组织制定的标准。连接头、帧、帧的使用、电流、编码及光调制等都属于各种物理层规范中的内容。物理层常用多个规范完成对所有细节的定义。示例：[Rj45](https://baike.baidu.com/item/Rj45/3401007?fromModule=lemma_inlink)，[802.3](https://baike.baidu.com/item/802.3/960717?fromModule=lemma_inlink)等

![image-20221019172512722](C:\Users\stormblinger\AppData\Roaming\Typora\typora-user-images\image-20221019172512722.png)

![image-20221019172614622](C:\Users\stormblinger\AppData\Roaming\Typora\typora-user-images\image-20221019172614622.png)

-什么是C/S和B/S架构？

C/S架构和B/S架构含义及其特点
C/S框架
概念：Client-Server，客户端对服务器模式
特点：在参与网络通信的所有节点中，有一个明显的节点作为服务器，其他节点充当客户端。一台服务器为多个客户端提供服务。
缺点

    客户端操作不便
    客户端维护成本较高，一旦软件有了版本更新，那么所有的客户端全部都需要重新下载最新版本的程序。

优点

    充分发挥客户端PC机的运算能力，利用PC机本地的硬件资源处理业务功能，只需要向服务器发送少量的处理结构即可。所以，C/S架构极大程度降低了服务器端的压力。
    充分发挥客户端PC机的运算能力，适合做一些特效的渲染，能够展示出非常美观、逼真的视觉效果。
    因为与服务器端只需要进行少量数据交互，所以基于C/S架构的应用程序往往能达到特别快的响应速度。

B/S框架
概念：Browser-Server，浏览器对服务器模式。
特点：B/S和C/S并不是两种完全不同的架构，实际情况是B/S架构就是一种特殊的C/S架构。或者可以说B/S架构就是在C/S的基础上演变得来的，是一种特殊的C/S架构。固定了用户操作客户端的方式为浏览器软件。
缺点

    几乎客户端不做数据处理，所有的处理都由服务器端统一完成，所以在B/S架构中服务器端的压力往往较大。
    浏览器能够渲染出的视觉效果十分有限，所以仅仅适用于以文字、图片为主的信息展示，不适合做特效展示，例如游戏。

优点

    使用非常简单，只要有一台能联网的计算机，计算机中装有任意一款浏览器软件，就可以访问任何一个应用程序。
    实现了客户端“零维护”，如果需要更新版本，只需要在服务器端将前后端代码进行更新，对于客户端来讲永远看到的都是最新版本。因为C/S架构中视图界面是依赖于下载安装好的保存在自己磁盘本地的代码解析得来，而B/S架构用户看到的视图界面是通过每次发送HTTP请求，由服务器现场返回HTML代码，再经浏览器解释执行渲染生成的。


-简述 三次握手、四次挥手的流程。

在开始讲解之前，先来讲几个重要字段的全称，方便记忆：

    seq：（sequence number）序号
    ack：（acknowledgement number）确认号
    标志位：
        SYN ：(SYNchronization）同步
        ACK ：(ACKnowlegment）确认
        FIN   :（FINish）终止
————————————————
版权声明：本文为CSDN博主「麦田里的POLO桔」的原创文章，遵循CC 4.0 BY-SA版权协议，转载请附上原文出处链接及本声明。
原文链接：https://blog.csdn.net/qq_44647809/article/details/115143100

1、三次握手

（1）三次握手的详述

首先Client端发送连接请求报文，Server段接受连接后回复ACK报文，并为这次连接分配资源。

Client端接收到ACK报文后也向Server段发生ACK报文，并分配资源，这样TCP连接就建立了。

2、四次挥手

（1）四次挥手的详述

　　假设Client端发起中断连接请求，也就是发送FIN报文。Server端接到FIN报文后，意思是说"我Client端没有数据要发给你了"，但是如果你还有数据没有发送完成，则不必急着关闭Socket，可以继续发送数据。所以你先发送ACK，"告诉Client端，你的请求我收到了，但是我还没准备好，请继续你等我的消息"。

这个时候Client端就进入FIN_WAIT状态，继续等待Server端的FIN报文。当Server端确定数据已发送完成，则向Client端发送FIN报文，"告诉Client端，好了，我这边数据发完了，准备好关闭连接了"。

Client端收到FIN报文后，"就知道可以关闭连接了，但是他还是不相信网络，怕Server端不知道要关闭，所以发送ACK后进入TIME_WAIT状态，如果Server端没有收到ACK则可以重传。

“，Server端收到ACK后，"就知道可以断开连接了"。

Client端等待了2MSL后依然没有收到回复，则证明Server端已正常关闭，那好，我Client端也可以关闭连接了。

Ok，TCP连接就这样关闭了！

-什么是arp协议？

ARP协议属于TCP/IP协议里一种将IP地址解析为MAC地址的协议，位于TCP/IP五层模型中的网络层。该协议是用来在局域网内解析IP地址对应的物理地址

-TCP和UDP的区别？

tcp和udp的区别有：1、udp是无连接的，tcp是面向连接的；2、udp是不可靠传输，tcp是可靠传输；3、udp是面向报文传输，tcp是面向字节流传输。

-什么是局域网和广域网？

**广域网和局域网的区别**	





**两者范围不一样**



局域网就是在固定的一个地理区域内由2台以上的电脑用网线和其他网络设备搭建而成的一个封闭的计算机组，范围在几千米以内；广域网是一种地域跨度非常大的网络集合，范围在几十公里到几千公里。





**两者的 IP 地址设置不一样**





局域网里面，必须在网络上有一个唯一的IP地址，这个 IP 地址是唯一的，在另外一个局域网，这个IP地址仍然能够使用。广域网上的每一台电脑（或其他网络设备）都有一个或多个广域网IP地址，而且不能重复。





**两者连接的方式不一样**





局域网是靠交换机来进行连接的，而广域网则是靠路由器将多个局域网进行连接。广域网包含局域网，一个个的局域网组成广域网。、

-为何基于tcp协议的通信比基于udp协议的通信更可靠

 tcp协议一定是先建好双向链接，发一个数据包要得到确认才算发送完成，没有收到就一直给你重发；udp协议没有链接存在，udp直接丢数据，不管你有没有收到。

 

TCP的可靠保证，是它的三次握手双向机制，这一机制保证校验了数据，保证了他的可靠性。

而UDP就没有了，udp信息发出后,不验证是否到达对方,所以不可靠。

不过UDP的速度是TCP比不了的，而且UDP的反应速度更快，QQ就是用UDP协议传输的，HTTP是用TCP协议传输的，不用我说什么，自己体验一下就能发现区别了。

再有就是UDP和TCP的目的端口不一样（这句话好象是多余的），而且两个协议不在同一层，TCP在三层，UDP不是在四层就是七层。

-什么是防火墙以及作用？

一、什么是防火墙?

　　防火墙是一种用于网络安全的设备，用于监控传入和传出的网络流量，(根据一组预定的安全规则确定是允许还是阻止网络流量)。您可以拒绝访问未经授权的流量，而允许合法流量到达目的地。防火墙还可以阻止恶意软件访问您的计算机。
　　二、防火墙的作用

　　防火墙具有很好的保护作用。攻击者要想接触到目标计算机，前提是必须穿越防火墙的安全防线。设置防火墙能减少或消除不需要的网络连接并增加合法流量的自由流动。防火墙是基础架构的重要补充，因为它们可以帮助将计算机和服务器与 Internet 隔离开来，从而提供数据的安全性和隐私性。如前所述，它们不仅监控进出您的服务器的流量，而且在某些情况下还会限制该流量。

　　三、防火墙如何工作?

　　防火墙监控所有数据流量，以根据预设规则允许良好数据并阻止不良数据。它使用以下三种方法中的一种或任意组合：数据包过滤、状态检查和代理服务。

　　数据包过滤方法被实施以监控网络连接。数据包是沿给定网络路径传输的打包在一起的数据单元。对包进行分析并与配置规则或“访问列表”进行比较。然后，防火墙会确定允许或拒绝访问您的环境的内容。

　　状态检查方法允许根据状态、端口和协议分析流量模式。防火墙监视连接上从打开到关闭的活动。它跟踪已知的可信数据包，以确定来自网站或应用程序的授权数据与来自黑客或其他网站安全漏洞的任何数据。

　　代理服务方法可防止 Internet 流量与服务器之间的直接网络连接。这种类型的实现使状态检查更进一步。防火墙充当您的服务器和最终用户发出的请求之间的中介。检查整个数据包，并根据规则集阻止或允许。


-简述 进程、线程、协程的区别 以及应用场景？

 1.进程是计算器最小资源分配单位 .

2.线程是CPU调度的最小单位 .

3.进程切换需要的资源很最大，效率很低 .

4.线程切换需要的资源一般，效率一般（当然了在不考虑GIL的情况下） .

5.协程切换任务资源很小，效率高（协程本身并不存在，是程序员通过控制IO操作完成） .

 6.多进程、多线程根据cpu核数不一样可能是并行的，但是协程是在一个线程中 所以是并发.

-GIL锁是什么鬼？

GIL：又称全局解释器锁。作用就是限制多线程同时执行，保证同一时间内只有一个线程在执行。线程非独立的，所以同一进程里线程是数据共享，当各个线程访问数据资源时会出现“竞争”状态，即数据可能会同时被多个线程占用，造成数据混乱，这就是线程的不安全。所以引进了互斥锁，确保某段关键代码、共享数据只能由一个线程从头到尾完整地执行。
GIL并不是Python的特性，Python完全可以不依赖于GIL

-Python中如何使用线程池和进程池？

简单介绍每一步：
一、导入包

from concurrent.futures import ProcessPoolExecutor, ThreadPoolExecutor, wait, ALL_COMPLETED

    1

二、创建一个函数

这个函数是打算要做的事。

def test(num):
    sum = 0
    for i in range(num):
        sum += i
    return sum

    1
    2
    3
    4
    5

三、创建一个主函数，在主函数里面创建进程池（或线程池）

def main():
    pool = ProcessPoolExecutor(max_workers=3)	# 进程池
	# pool = ThreadPoolExecutor(max_workers=3)	# 线程池

    1
    2
    3

创建进程池时max_workers的最大值为CPU核心的两倍；创建线程池时max_workers的最大值为CPU核心的五倍。
四、将任务放入进/线程池中

task = [pool.submit(test, num) for num in [3, 5, 7]]	# 注意test后面没有括号！！！

    1

五、等待进/线程池完成任务

wait(task, return_when=ALL_COMPLETED)	

    1

这里是等待所有任务完成才能拿到结果。也可以不用等待所有任务完成就拿到结果，即每个进/线程执行完就可以拿到结果，去掉return_when就行。
六、得到结果

    for i in task:
        print(i.result())
   -浏览器输入一个URL中间经历的过程

  中间设计到了哪些过程

   包含哪些网络协议

   每个协议都干了什么

 物理层，链路层，网络层，传输层，应用层

   流程图：

   DNS查询--》TCP握手--》HTTP请求--》方向代理Nginx--》uwsgi/gunicom--》webAPP响应---》TCP握手

   -TCP三次握手，状态转换，用wireshark抓包更直观

   TCP和UDP的区别

   面向了连接，可靠的，基于字节流

   无连接，不可靠，面向报文

   -HTTP协议常考

   HTTP请求的组成，

http请求由请求行，请求头，请求体组成

请求行由请求方法、请求url、http协议及版本组成。

        请求方法：get、post、head、put、delete、trace、connect、options
    
            get用来请求指定页面的内容，并返回实体主体，post向服务器提交资源数据进行请求处理，post请求可能会导致新的资源的建立或者已有资源的修改，post比get更安全，因为get方法数据存放在请求行里的url末尾，而post方法数据存放在请求体里，get有url的长度限制，post能发送的数据更大，post不会被缓存在服务器日志，post能发送更多的数据类型，get只能发送ASCII码；但是post比get慢；
    
            Head：用于获取报头，类似于get请求，只不过返回的响应中没有具体的内容
    
            Put：从客户端向服务器传送的数据取代指定的文档的内容
    
            Delete：请求服务器删除指定的页面
    
            Trace：回显服务器收到的请求，主要用于测试或诊断
    
            Connect：保留将来使用
    
            OPTIONS：允许客户端查看服务器的性能

请求头

        Host：请求的web服务器域名地址，也就是请求被发送的目的地
    
        Accept：指定客户端接受哪些类型的信息
    
        Accept-Language：浏览器说明自己接收的语言
    
        Authorization：证明客户端有权查看某个资源
    
        Cookie：http请求发送时，会把保存在请求域名下的所有cookie值一起发送给web服务器
    
        Content-Type：Body编码方式
    
                application/x-www-form-urlencoded: 默认数据编码方式，表单数据向服务器提交时所采用的编码类型，但是在向服务器发送大量的文本，包含非ASCII字符的文本或二进制数据时，这种编码方式效率很低
    
                application/json：用来告诉服务器消息的主体是序列化后的JSON字符串，使用这个编码方式需要参数本身就是JSON格式的数据，参数会被直接放到请求实体里，不进行任何处理
    
                 Multipart/form-data：文件上传时，使用的这种编码方式，可以发送文本数据，也支持二进制数据上传


   HTTP常见状态码

一、1开头的状态码(信息类)

    100，接受的请求正在处理，信息类状态码

二、2开头的状态码(成功类)

    2xx(成功)表示成功处理了请求的状态码
    200(成功)服务器已成功处理了请求。

三、3开头的状态码(重定向)

    3xx(重定向)表示要完成请求，需要进一步操作。通常这些状态代码用来重定向。
    301，永久性重定向，表示资源已被分配了新的 URL
    302，临时性重定向，表示资源临时被分配了新的 URL
    303，表示资源存在另一个URL，用GET方法获取资源
    304，(未修改)自从上次请求后，请求网页未修改过。服务器返回此响应时，不会返回网页内容

四、4开头的状态码(客户端错误)

    4xx(请求错误)这些状态码表示请求可能出错，妨碍了服务器的处理
    400(错误请求)服务器不理解请求的语法
    401表示发送的请求需要有通过HTTP认证的认证信息
    403(禁止)服务器拒绝请求
    404(未找到)服务器找不到请求网页

五、5开头的状态码(服务器错误)

    5xx(服务器错误)这些状态码表示服务器在尝试处理请求时发生内部错误。这些错误可能是服务器本身的错误，而不是请求的错误
    500，(服务器内部错误)服务器遇到错误，无法完成请求
    503，表示服务器处于停机维护或超负载，无法处理请求


   HTTP GET和POST区别

表单提交中get和post方式的区别有5点

1.get是从服务器上获取数据，post是向服务器传送数据。

2.get是把参数数据队列加到提交表单的ACTION属性所指的URL中，值和表单内各个字段一一对应，在URL中可以看到。post是通过HTTPpost机制，将表单内各个字段与其内容放置在HTML HEADER内一起传送到ACTION属性所指的URL地址。用户看不到这个过程。

3.对于get方式，服务器端用Request.QueryString获取变量的值，对于post方式，服务器端用Request.Form获取提交的数据。

4.get传送的数据量较小，不能大于2KB。post传送的数据量较大，一般被默认为不受限制。但理论上，IIS4中最大量为80KB，IIS5中为100KB。（这里有看到其他文章介绍get和post的传送数据大小跟各个浏览器、操作系统以及服务器的限制有关）

5.get安全性非常低，post安全性较高。

 

表单提交中get和post方式的区别有5点

1.get是从服务器上获取数据，post是向服务器传送数据。

2.get是把参数数据队列加到提交表单的ACTION属性所指的URL中，值和表单内各个字段一一对应，在URL中可以看到。post是通过HTTPpost机制，将表单内各个字段与其内容放置在HTML HEADER内一起传送到ACTION属性所指的URL地址。用户看不到这个过程。

3.对于get方式，服务器端用Request.QueryString获取变量的值，对于post方式，服务器端用Request.Form获取提交的数据。

4.get传送的数据量较小，不能大于2KB。post传送的数据量较大，一般被默认为不受限制。但理论上，IIS4中最大量为80KB，IIS5中为100KB。

5.get安全性非常低，post安全性较高。

HTTP请求：get与post方法的区别

HTTP 定义了与服务器交互的不同方法，最基本的方法是 get 和 post。事实上 get 适用于多数请求，而保留  post仅用于更新站点。根据 HTTP 规范，get  用于信息获取，而且应该是安全的和幂等的。所谓安全的意味着该操作用于获取信息而非修改信息。换句话说，get  请求一般不应产生副作用。幂等的意味着对同一  URL的多个请求应该返回同样的结果。完整的定义并不像看起来那样严格。从根本上讲，其目标是当用户打开一个链接时，她可以确信从自身的角度来看没有改变资源。比如，新闻站点的头版不断更新。虽然第二次请求会返回不同的一批新闻，该操作仍然被认为是安全的和幂等的，因为它总是返回当前的新闻。反之亦然。post请求就不那么轻松了。post 表示可能改变服务器上的资源的请求。仍然以新闻站点为例，读者对文章的注解应该通过  post请求实现，因为在注解提交之后站点已经不同了（比方说文章下面出现一条注解）；

在FORM提交的时候，如果不指定Method，则默认为get请求，Form中提交的数据将会附加在url之后，以?分开与url分开。字母数字字符原样发送，但空格转换为“+“号，其它符号转换为%XX,其中XX为该符号以16进制表示的ASCII（或ISOLatin-1）值。get请求请提交的数据放置在HTTP请求协议头中，而post提交的数据则放在实体数据中；

以下为代码演示：

[![复制代码](https://common.cnblogs.com/images/copycode.gif)](javascript:void(0);)

![img](https://images.cnblogs.com/OutliningIndicators/ExpandedBlockStart.gif)

![复制代码](https://common.cnblogs.com/images/copycode.gif)

```
 1 <body>
 2     <form action="/tablelist/tablelist.ashx" method="get">
 3         <input type="text" name="id" value=" " />
 4         <input type="text" name="name" value=" " />
 5         <input type="submit" name="submit" value="get提交的一种方式" />
 6     </form>
 7     <form action="/tablelist/tablelist.ashx" method="post">
 8         <input type="text" name="id" value=" " />
 9         <input type="text" name="name" value=" " />
10         <input type="submit" name="submit" value="post提交的一种方式" />
11     </form>
12 </body>
```

![复制代码](https://common.cnblogs.com/images/copycode.gif)

[![复制代码](https://common.cnblogs.com/images/copycode.gif)](javascript:void(0);)

运行第一个form表单，结果如下图

![img](https://images0.cnblogs.com/blog/569548/201402/061508185692768.png)

表单中的参数被提交到Action所指定的URL中，值和表单内各个字段一一对应，在URL中可以看到。空格符号被转为了加号，提交按钮中的中文被转换为了%+以16进制表示的ASCII（或ISOLatin-1）值。表单提交到服务器后，返回了用户在表单中指定参数所对应的结果。

运行post表单后的结果如下图：

![img](https://images0.cnblogs.com/blog/569548/201402/061515251534079.png)

服务器没有返回用户表单中提交参数所对应的结果，但是表单内各个字段与其内容被放置在HTML HEADER内一起传送到ACTION属性所指的URL地址，用户在地址栏看不到。

通过get方法提交数据，可能会带来安全性的问题。比如一个登陆页面。当通过get方法提交数据时，用户名和密码将出现在URL上。如果：

１、 登陆页面可以被浏览器缓存；

２、 其他人可以访问客户的这台机器。那么，别人即可以从浏览器的历史记录中，读取到此客户的账号和密码。所以，在某些情况下，get方法会带来严重的安全性问题。 

 

get与post的区别2

get：是以实体的方式得到由请求URI所指定资源的信息，如果请求URI只是一个数据产生过程，那么最终要在响应实体中返回的是处理过程的结果所指向的资源，而不是处理过程的描述。

post：用来向目的服务器发出请求，要求它接受被附在请求后的实体，并把它当作请求队列中请求URI所指定资源的附加新子项，post被设计成用统一的方法实现下列功能：

1：对现有资源的解释

2：向电子公告栏、新闻组、邮件列表或类似讨论组发信息。

3：提交数据块

4：通过附加操作来扩展数据库

从上面描述可以看出，get是向服务器发索取数据的一种请求；而post是向服务器提交数据的一种请求，要提交的数据位于信息头后面的实体中。

   什么是幂等性，哪些HTTP方法是幂等的

 HTTP 幂等方法，是指无论调用多少次都不会有不同结果的 HTTP 方法。不管你调用一次，还是调用一百次，一千次，结果都是相同的。

 HTTP GET 方法，用于获取资源，不管调用多少次接口，结果都不会改变，所以是幂等的。

 只是查询数据，不会影响到资源的变化，因此我们认为它幂等。

#### 一、`HTTP`请求的组成

`HTTP`协议由哪些部分组成？使用抓包工具去查看和理解
 1.状态行
 2.请求头
 3.消息主体（用`GET`请求时，会没有消息主体，`POST`请求时，则有）

```shell
# 1、安装 httpie
pip install httpie

# 2.用 http 命令
http baidu.com
```

运行结果：
 用`http -v`

```shell
http -v baidu.com
```

执行结果：
 注意：常用的`HTTP`请求头也是面试常考点

#### 二、`HTTP`响应的组成

1.状态行
 2.响应头
 3.响应正文

#### 三、`HTTP`常见状态码

了解常见的`HTTP`相应状态码
 1.1XX表示信息。服务器收到请求，需要请求者继续执行操作
 2.2XX表示成功。操作被成功接受并处理
 3.3XX表示重定向。需要进一步操作完成请求。
 4.4XX表示客户端错误。请求有语法错误或者无法完成请求
 4.5XX表示服务器错误。服务器在处理请求的过程中发生错误

注意：牢记常见状态码的含义（`220`, `301`, `302`, `400`, `403`, `500`)等

#### 四、`HTTP` `GET`/`POST`区别

常见的 `HTTP`方法：`GET`/`POST`/`PUT`/`DELETE`

- `GET`获取
- `POST`创建
- `PUT`更新
- `DELETE`删除

1. `Restful`语义上来说，一个是获取，一个是创建
2. `GET`是幂等的，`POST`非幂等
3. `GET`请求参数放到`url`（明文），长度有限制；`POST`放在请求体，更安全

#### 五、什么是幂等性

什么是幂等？哪些`HTTP`方法是幂等的
 1.幂等方法是无论调用多少次都得到相同结果的`HTTP`方法
 2.例如：`a=4`是幂等的，但是 `a += 4`就是非幂等的
 3.幂等的方法客户端可以安全地重发请求
 4.安全指的`HTTP`是否会修改服务端的数据，会则是不安全的，不会则是安全的

#### 六、幂等方法

| HTTP Method（请求方法） | Idempotent（幂等性） | Safe（安全性） |
| ----------------------- | -------------------- | -------------- |
| OPTIONS                 | yes                  | yes            |
| GET                     | yes                  | yes            |
| HEAD                    | yes                  | yes            |
| PUT                     | yes                  | no             |
| POST                    | no                   | no             |
| DELETE                  | yes                  | no             |
| PATCH                   | no                   | no             |

说明：幂等是指多次请求结果和请求一次结果一样；安全指的是是否会修改数据。

#### 七、什么是 `HTTP` 长连接

`HTTP persistent connection, HTTP 1.1`
 1.短连接：建立连接。。。数据传输。。。关闭连接（连接的建立和关闭开销大）
 2.长连接：`Connection: Keep-alive`。保持`TCP`连接不断开。

3.如何区分不同的 `HTTP`请求呢？`Content-Length|Transfer-Encoding:chunked`

`Content-Length`首部告诉浏览器报文中实体主体的大小

#### 八、`cookie`和`session`区别

`HTTP`是无状态的，如何识别用户呢？需要在服务端给用户生成一个标识，然后每次让客户端带过去给后端
 1.`Session`一般是服务器生成之后给客户端（通过 `url`参数或`cookie`）
 2.`Cookie`是实现`session`的一种机制，通过 `HTTP` `cookie`字段实现
 3.`Session`通过在服务器保存 `sessionid`识别用户，`cookie`存储在客户端

 

​    Socket编程基础

   -TCP/UDP socket编程原理，HTTP编程原理

   如何使用socket模块

一、Socket参数使用介绍

Python使用 socket 模块创建套接字，语法格式如下：

import socketsocket.socket(family=AF_INET, type=SOCK_STREAM, proto=0, fileno=None) # 默认参数

1. socket()参数

family:

    socket.AF_INET - IPv4(默认)
    
    socket.AF_INET6 - IPv6
    
    socket.AF_UNIX - 只能够用于单一的Unix系统进程间通信

type:

    socket.SOCK_STREAM - 流式socket, for TCP (默认)
    
    socket.SOCK_DGRAM - 数据报式socket, for UDP
    
    socket.SOCK_RAW - 原始套接字
    
    socket.SOCK_RDM - 可靠UDP形式
    
    socket.SOCK_SEQPACKET - 可靠的连续数据包服务

2. socket对象内建方法

服务端套接字方法：

    s.bind() - 绑定地址（host,port）到套接字，在AF_INET下,以元组（host,port）的形式表示地址。
    
    s.listen() - 开启TCP监听，操作系统可以挂起的最大连接数量，该值至少为1。
    
    s.accept() - 被动接受TCP客户端连接,(阻塞式)等待连接的到来。

客户端套接字方法：

    s.connect() - 主动初始化TCP服务器连接，一般address的格式为元组（hostname,port），如果连接出错，返回socket.error错误。
    
    s.connect_ex() - connect()函数的扩展版本，出错时返回出错码，而不是抛出异常。

公共套接字方法：

    s.recv() - 接收TCP数据，数据以byte类型返回，bufsize指定要接收的最大数据量。
    
    s.send() - 发送TCP数据，将string中的数据转化为byte类型发送到连接的套接字，返回值是要发送的字节数量，该数量可能小于string的字节大小。
    
    s.sendall() - 发送完整TCP数据，将string中的数据转化为byte类型发送到连接的套接字，但在返回之前会尝试发送所有数据，成功返回None，失败则抛出异常。
    
    s.recvfrom() - 接收UDP数据，与recv()类似，但返回值是（data,address），其中data是包含接收数据的字符串，address是客户端的套接字地址。
    
    s.sendto() - 发送UDP数据，将数据发送到套接字，参数形式为（data,(address,port)）的元组，address为远程服务端地址，返回值是发送的字节数。
    
    s.close() - 关闭套接字。
    
    s.getpeername() - 返回连接套接字的远程地址，返回值通常是元组（ipaddr,port）。
    
    s.getsockname() - 返回套接字自己的地址，通常是一个元组(ipaddr,port)。
    
    s.setsockopt(level,optname,value) - 设置给定套接字选项的值。
    
    s.getsockopt(level,optname[.buflen]) - 返回套接字选项的值。
    
    s.settimeout(timeout) - 设置套接字操作的超时时间，timeout是一个浮点数，单位是秒
    
    s.gettimeout() - 返回当前超时期的值，单位是秒，如果没有设置超时期，则返回None。
    
    s.fileno() - 返回套接字的文件描述符。
    
    s.setblocking(flag) - 如果flag为0，则将套接字设为非阻塞模式，否则将套接字设为阻塞模式（默认值）。非阻塞模式下，如果调用recv()没有发现任何数据，或send()调用无法立即发送数据，那么将引起socket.error异常。
    
    s.makefile() - 创建一个与该套接字相关连的文件。 作者：茑萝newroad https://www.bilibili.com/read/cv18021375 出处：bilibili

   如何建立TCP socket客户端和服务端

**一、客户端**

```
#1、创建socket套接字
#2、建立TCP连接
#3、接收、发送数据
import  socket   #导入socket套接字模块
def main(target,port):
# 1、创建socket套接字
 client = socket.socket(socket.AF_INET,socket.SOCK_STREAM)
# 2、建立TCP连接
 client.connect(((target,port)))
# 3、接收、发送数据
 client.send(b"successful to connection....")      #发送数据要是用二进制，因此在python3中要使用b
 response = client.recv(1024)    #设置response变量，接收数据，1024为接收数据的大小
 print(response)    #输出接收到的数据
 client.close()    #关闭TCP连接
if __name__ == "__main__":
    target = "127.0.0.1"
    port  = 4444
    main(target,port)

二、服务端
#1、创建socket套接字
#2、绑定IP和端口
#3、进行监听
#4、接收和发送数据

import socket
def main(target,port):
# 1、创建socket套接字
 server = socket.socket(socket.AF_INET,socket.SOCK_STREAM)
#2、绑定IP和端口
 server.bind((target,port))
# 3、进行监听
 server.listen(10)  #10为监听的数量
 print("[*] listening on %s:%d" %(target,port))
# 4、接收和发送数据
 while True:
     client,addr = server.accept()  #client为连接过来的套接字对象，addr为客户端的ip和端口，addr为列表
print("[*]Accept from %s:%d " % (addr[0],addr[1]))  #打印连接过来的客户端ip和端口
     response = client.recv(1024)     
print(response)
     client.send(b"[*]successful to connection....")
     client.close()

if __name__ == "__main__":
    target = "0.0.0.0"
    port =4444
    main(target,port)
```

   客户端和服务端之间的通信

   使用socket发送HTTP请求

   python用socket发送http请求

平时我们使用浏览器浏览web资源，写爬虫的时候，我们会使用封装好的库，比如requests，或者使用爬虫框架。工欲善其事必先利其器，顶层封装好的东西，是为了我们使用着方便，节省开发时间，尽管各种http库功能强大，但学习底层的技术仍然有着实践意义，只有了解底层，才能真正理解顶层的封装和设计，遇到那些艰难的问题时，才会有思路，有方案。

1. 用socket发送http请求

浏览器也好，爬虫框架也罢，在最底层，都是在使用socket发送http请求，然后接收服务端返回的数据，浏览器会对返回的数据进行渲染，最终呈现在我们眼前，爬虫框架相比于浏览器，只是少了一个渲染的过程。

用socket发送http请求，首先要建立一个TCP socket，然后连接到服务端socket，http请求的3次握手，本质上是TCP  socket建立连接的3次握手。连接建立好了以后，就要发送数据了，这里的数据，可不是随意发送的，而是要遵照http协议，下面的代码，演示了socket发送http请求的过程

```python
import socket


url = 'www.zhangdongshengtech.com'
port = 80

# 创建TCP socket
sock = socket.socket(socket.AF_INET, socket.SOCK_STREAM)
# 连接服务端
sock.connect((url, port))
# 创建请求消息头
request_url = 'GET /article-types/6/ HTTP/1.1\r\nHost: www.zhangdongshengtech.com\r\nConnection: close\r\n\r\n'
print(request_url)
# 发送请求
sock.send(request_url.encode())
response = b''
# 接收返回的数据
rec = sock.recv(1024)
while rec:
    response += rec
    rec = sock.recv(1024)
print(response.decode())
```

代码非常简单，不做过多解释，我们重点要了解的是http协议，request_url的内容是

```text
GET /article-types/6/ HTTP/1.1
Host: www.zhangdongshengtech.com
Connection: close
```

请求的消息头，每一行都有各自的作用，在消息头和消息体之间，有两次换行，由于我们发送的是GET请求，没有消息体，因此两个换行后就结束了。

这3行，每一行都至关重要。

第一行的内容，包含了三个重要信息

1. GET 指明本次请求所使用的method，这是一次GET请求
2. /article-types/6/ 指明了要请求的资源地址
3. HTTP/1.1 指明http协议的版本，更早以前是1.0，现在大家都在用1.1

第二行的内容，指明了host，一台服务器上，也许不只是部署了一个web服务，而是多个，他们都是80端口，url = '[www.zhangdongshengtech.com](http://www.zhangdongshengtech.com)' 只是告诉socket去哪里建立连接，这仅仅是个域名而已，程序根据域名找到IP地址，如果服务端部署多个服务，为了服务端区分一个请求是指向哪个服务，客户端需要在请求头中指明host，服务端会根据这个host来做请求的转发，这就是常说的nginx反向代理。

第三行，定义了Connection的值是close，如果不定义，默认是keep-alive，  如果是keep-alive，那么服务端在返回数据后不会断开连接，而是允许客户端继续使用这个连接发送请求，我故意设置成close，目的就是让服务端主动断开，这样，当程序在使用while循环时，接收完所有的数据后，sock.recv(1024)  返回的就是None，这样，就可以停止程序了。如果是keep-alive，无法通过连接断开来判断数据是否已经全部接收，那么就只能通过返回数据的消息头来获取数据的长度，进而决定本次请求返回的数据到哪里结束。

2. 返回的消息体

程序输出了服务端返回的数据，由于数据量很大，我们只截取消息头的部分进行讲解，消息体只是网页源码而已，没什么可说的。

```text
HTTP/1.1 200 OK
Server: openresty/1.11.2.1
Date: Sun, 05 May 2019 03:11:05 GMT
Content-Type: text/html; charset=utf-8
Content-Length: 29492
Connection: close
Set-Cookie: session=eyJjc3JmX3Rva2VuIjp7IiBiIjoiTn
prd1pqZGhaamd6T1dObFlUQTRZVFJqTkRJeU9USmtNalU0TldOaU1UQXdNamsxTkdSaVpRPT0ifX0.D6_lyQ.
4EqkK8taszUkPtMsol-8pzF_LQM; HttpOnly; Path=/

<!DOCTYPE html>
<html lang="en">
<head>
```

返回的数据中，消息头和消息体之间，也是两个换行。

第一行，HTTP/1.1 200 OK 指明了http协议的版本，已经本次请求返回的状态码，200表示成功响应，比较常见的还有404，500，302，这些状态码的含义，你可以自己百度一下。

第二行开始的消息头内容中，比较重要的是Content-Length，它的值是29492，这表明，消息体的长度是29492，如果Connection的值是keep-alive，客户端就得根据这个值来读取消息体。

请求的消息体和服务端响应的消息体中，除了第一行外，其他的长的很像字典形式的key-value对，叫首部，本文只涉及到个别几个首部，其他首部及其含义，你可以自行百度。

3. 从消息头解析出content_length

增加一点难度，从消息头里获取content_length，在获取返回数据时，当消息体的长度满足要求时，停止获取数据，并关闭连接

```python
import socket


url = 'www.zhangdongshengtech.com'
port = 80
# 创建TCP socket
sock = socket.socket(socket.AF_INET, socket.SOCK_STREAM)
# 连接服务端
sock.connect((url, port))
# 创建请求消息头
request_url = 'GET /article-types/6/ HTTP/1.1\r\nHost: www.zhangdongshengtech.com\r\n\r\n'
print(request_url)
# 发送请求
sock.send(request_url.encode())
body = ''
# 接收返回的数据
rec = sock.recv(1024)


index = rec.find(b'\r\n\r\n')       # 找到消息头与消息体分割的地方
head = rec[:index]
body = rec[index+4:]

# 获取Content-Length
headers = head.split(b'\r\n')
for header in headers:
    if header.startswith(b'Content-Length'):
        content_length = int(header.split(b' ')[1])

length = len(body)

while length < content_length:
    rec = sock.recv(1024)
    length += len(rec)
    body += rec

sock.close()
print(length)
print(head.decode())
print(body.decode())
```

 

​    

## 6，Python并发库

   -五种IO模型

一 IO操作本质 数据复制的过程中不会消耗CPU   `# 1 内存分为内核缓冲区和用户缓冲区# 2 用户的应用程序不能直接操作内核缓冲区，需要将数据从内核拷贝到用户才能使用# 3 而IO操作、网络请求加载到内存的数据一开始是放在内核缓冲区的`   ![image-20200325231658991](https://tva1.sinaimg.cn/large/00831rSTly1gd8rw1pqm5j31700m2tdf.jpg) 二 IO模型 **1. BIO – 阻塞模式I/O** 用户进程从发起请求，到最终拿到数据前，一直挂起等待； 数据会由用户进程完成拷贝   `'''举个例子：一个人去 商店买一把菜刀，他到商店问老板有没有菜刀（发起系统调用）如果有（表示在内核缓冲区有需要的数据）老板直接把菜刀给买家（从内核缓冲区拷贝到用户缓冲区）这个过程买家一直在等待如果没有，商店老板会向工厂下订单（IO操作，等待数据准备好）工厂把菜刀运给老板（进入到内核缓冲区）老板把菜刀给买家（从内核缓冲区拷贝到用户缓冲区）这个过程买家一直在等待是同步io'''`   ![image-20200325231903075](https://tva1.sinaimg.cn/large/00831rSTly1gd8rw5x47nj318s0nwq9b.jpg) **2. NIO – 非阻塞模式I/O** 用户进程发起请求，如果数据没有准备好，那么立刻告知用户进程未准备好；此时用户进程可选择继续发起请求、或者先去做其他事情，稍后再回来继续发请求，直到被告知数据准备完毕，可以开始接收为止； 数据会由用户进程完成拷贝   `'''举个例子：一个人去 商店买一把菜刀，他到商店问老板有没有菜刀（发起系统调用）老板说没有，在向工厂进货（返回状态）买家去别地方玩了会，又回来问，菜刀到了么（发起系统调用）老板说还没有（返回状态）买家又去玩了会（不断轮询）最后一次再问，菜刀有了（数据准备好了）老板把菜刀递给买家（从内核缓冲区拷贝到用户缓冲区）整个过程轮询+等待：轮询时没有等待，可以做其他事，从内核缓冲区拷贝到用户缓冲区需要等待是同步io同一个线程，同一时刻只能监听一个socket，造成浪费，引入io多路复用，同时监听读个socket'''`   ![image-20200325232410816](https://tva1.sinaimg.cn/large/00831rSTly1gd8rw9zwy9j317i0o6473.jpg) **3. IO Multiplexing - I/O多路复用模型** 类似BIO，只不过找了一个代理，来挂起等待，并能同时监听多个请求； 数据会由用户进程完成拷贝   `'''举个例子：多个人去 一个商店买菜刀，多个人给老板打电话，说我要买菜刀（发起系统调用）老板把每个人都记录下来（放到select中）老板去工厂进货（IO操作）有货了，再挨个通知买到的人，来取刀（通知/返回可读条件）买家来到商店等待，老板把到给买家（从内核缓冲区拷贝到用户缓冲区）多路复用：老板可以同时接受很多请求（select模型最大1024个，epoll模型），但是老板把到给买家这个过程，还需要等待，是同步io'''`   ![image-20200325232430850](https://tva1.sinaimg.cn/large/00831rSTly1gd8rwe9ouij31740nkwll.jpg) **4. AIO – 异步I/O模型** 发起请求立刻得到回复，不用挂起等待； 数据会由内核进程主动完成拷贝   `'''举个例子：还是买菜刀现在是网上下单到商店（系统调用）商店确认（返回）商店去进货（io操作）商店收到货把货发个卖家（从内核缓冲区拷贝到用户缓冲区）买家收到货（指定信号）整个过程无等待异步ioAIO框架在windows下使用windows IOCP技术，在Linux下使用epoll多路复用IO技术模拟异步IO市面上多数的高并发框架，都没有使用异步io而是用的io多路复用，因为io多路复用技术很成熟且稳定，并且在实际的使用过程中，异步io并没有比io多路复用性能提升很多，没有达到很明显的程度并且，真正的AIO编码难度比io多路复用高很多'''`   ![image-20200325232454769](https://tva1.sinaimg.cn/large/00831rSTly1gd8rwiz5grj318c0pgn3u.jpg) 5 select poll 和epoll   `#  1 select poll 和epoll都是io多路复用技术select, poll , epoN都是io多路复用的机制。I/O多路复用就是通过一种机 制个进程可以监视多个描述符，一旦某个描述符就绪（一般是读就绪或者写就绪），能够通知程序进行相应的读写操作。但select, poll , epoll本质上都是同步I/O ,因为他们都需要在读写事件就绪后自己负责进行读写， 也就是说这个读写过程是阻塞的，而异步I/O则无需自己负责进行读写，异 步I/O的实现会负责把数据从内核拷贝到用户空间。# 2 selectselect函数监视的文件描述符分3类，分别是writefds、readfds、和 exceptfds。调用后select函数会阻塞，直到有描述副就绪（有数据可读、 可写、或者有except）,或者超时（timeout指定等待时间，如果立即返回 设为null即可），函数返回。当select函数返回后，可以通过遍历fdset,来 找到就绪的描述符。select目前几乎在所有的平台上支持，其良好跨平台支持也是它的一个 优点。select的一个缺点在于单个进程能够监视的文件描述符的数量存在最大限制，在Linux上一般为1024 ,可以通过修改宏定义甚至重新编译内核的 方式提升这一限制，但是这样也会造成效率的降低。# 3 poll不同于select使用三个位图来表示三个fdset的方式，poll使用一个 pollfd的指针实现。pollfd结构包含了要监视的event和发生的event,不再使用select '参数-值'传递的方式。同时，pollfd并没有最大数量限制（但是数量过大后 性能也是会下降）。和select函数一样，poll返回后，需要轮询pollfd来获取就绪的描述符。从上面看，select和poll都需要在返回后，通过遍历文件描述符来获取 已经就绪的socket。事实上，同时连接的大量客户端在一时刻可能只有很少的处于就绪状态，因此随着监视的描述符数量的增长，其效率也会线性下降# 4 epollepoll是在linux2.6内核中提出的，是之前的select和poll的增强版本。相对 于select和poll来说，epoll更加灵活，没有描述符限制。epoll使用一个文 件描述符管理多个描述符，将用户关系的文件描述符的事件存放到内核的一个事件表中，这样在用户空间和内核空间的copy只需一次。# 5 更好的例子理解老师检查同学作业，一班50个人，一个一个问，同学，作业写完了没？select，poll老师检查同学作业，一班50个人，同学写完了主动举手告诉老师，老师去检查 epoll# 6 总结在并发高的情况下，连接活跃度不高，epoll比select好，网站http的请求，连了就断掉并发性不高，同时连接很活跃，select比epoll好，websocket的连接，长连接，游戏开发`   三 同步I/O与异步I/O 同步I/O 概念：导致请求进程阻塞的I/O操作，直到I/O操作任务完成 类型：BIO、NIO、IO Multiplexing  异步I/O 概念：不导致进程阻塞的I/O操作 类型：AIO  注意： 同步I/O与异步I/O判断依据是，是否会导致用户进程阻塞 BIO中socket直接阻塞等待（用户进程主动等待，并在拷贝时也等待） NIO中将数据从内核空间拷贝到用户空间时阻塞（用户进程主动询问，并在拷贝时等待） IO Multiplexing中select等函数为阻塞、拷贝数据时也阻塞（用户进程主动等待，并在拷贝时也等待） AIO中从始至终用户进程都没有阻塞（用户进程是被动的） 四 并发-并行-同步-异步-阻塞-非阻塞   `# 1 并发并发是指一个时间段内，有几个程序在同一个cpu上执行，但是同一时刻，只有一个程序在cpu上运行跑步，鞋带开了，停下跑步，系鞋带# 2 并行指任意时刻点上，有多个程序同时运行在多个cpu上跑步，边跑步边听音乐# 3 同步：指代码调用io操作时，必须等待io操作完成才返回的调用方式# 4 异步异步是指代码调用io操作时，不必等io操作完成就返回调用方式# 6 阻塞指调用函数时候，当前线程别挂起# 6 非阻塞指调用函数时候，当前线程不会被挂起，而是立即返回# 区别：同步和异步是消息通讯的机制阻塞和非阻塞是函数调用机制`   五 IO设计模式   ` Reactor模式，基于同步I/O实现- Proactor模式，基于异步I/O实现`   Reactor模式通常采用IO多路复用机制进行具体实现   `- kqueue、epoll、poll、select等机制`   Proactor模式通常采用OS Asynchronous IO(AIO)的异步机制进行实现   `- 前提是对应操作系统支持AIO，比如支持异步IO的linux(不太成熟)、具备IOCP的windows server(非常成熟)`   Reactor模式和Proactor模式都是事件驱动，主要实现步骤： 事件注册：将事件与事件处理器进行分离。将事件注册到事件循环中，将事件处理器单独管理起来，记录其与事件的对应关系。 事件监听：启动事件循环，一旦事件已经就绪/完成，就立刻通知事件处理器 事件分发：当收到事件就绪/完成的信号，便立刻激活与之对应的事件处理器 事件处理：在进程/线程/协程中执行事件处理器 使用过程中，用户通常只负责**定义事件和事件处理器**并将其注册以及一开始的**事件循环的启动**，这个过程就会是以异步的形式执行任务。 Reactor模式 ![image-20200325235039094](https://tva1.sinaimg.cn/large/00831rSTly1gd8rwn55tej31620u0ti4.jpg) Proactor模式 ![image-20200325235058486](https://tva1.sinaimg.cn/large/00831rSTly1gd8rwqdfvnj315x0u047e.jpg) 对比分析 Reactor模型处理耗时长的操作会造成事件分发的阻塞，影响到后续事件的处理； Proactor模型实现逻辑复杂；依赖操作系统对异步的支持，目前实现了纯异步操作的操作系统少，实现优秀的如windows  IOCP，但由于其windows系统用于服务器的局限性，目前应用范围较小；而Unix/Linux系统对纯异步的支持有限，因而应用事件驱动的主流还是基于select/epoll等实现的reactor模式 Python中：如asyncio、gevent、tornado、twisted等异步模块都是依据事件驱动模型设计，更多的都是使用reactor模型，其中部分也支持proactor模式，当然需要根据当前运行的操作系统环境来进行手动配置

   一些常见的提升并发能力的方式

​                                            **python如何实现高并发问题**                                        

​                                            *千次阅读*                                    

2020-10-29 21:19:38

一、什么是高并发

高并发（High Concurrency）是互联网分布式系统架构设计中必须考虑的因素之一，它通常是指，通过设计保证系统能够同时并行处理很多请求。

高并发相关常用的一些指标有响应时间（Response Time），吞吐量（Throughput），每秒查询率QPS（Query Per Second），并发用户数等。

响应时间：系统对请求做出响应的时间。例如系统处理一个HTTP请求需要200ms，这个200ms就是系统的响应时间。

吞吐量：单位时间内处理的请求数量。

QPS：每秒响应请求数。在互联网领域，这个指标和吞吐量区分的没有这么明显。

并发用户数：同时承载正常使用系统功能的用户数量。例如一个即时通讯系统，同时在线量一定程度上代表了系统的并发用户数。

二、如何提升系统的并发能力

互联网分布式架构设计，提高系统并发能力的方式，方法论上主要有两种：垂直扩展（Scale Up）与水平扩展（Scale Out）。

垂直扩展：提升单机处理能力。垂直扩展的方式又有两种：

（1）增强单机硬件性能，例如：增加CPU核数如32核，升级更好的网卡如万兆，升级更好的硬盘如SSD，扩充硬盘容量如2T，扩充系统内存如128G；

（2）提升单机架构性能，例如：使用Cache来减少IO次数，使用异步来增加单服务吞吐量，使用无锁数据结构来减少响应时间；

在互联网业务发展非常迅猛的早期，如果预算不是问题，强烈建议使用"增强单机硬件性能”的方式提升系统并发能力，因为这个阶段，公司的战略往往是发展业务抢时间，而"增强单机硬件性能”往往是最快的方法。

不管是提升单机硬件性能，还是提升单机架构性能，都有一个致命的不足：单机性能总是有极限的。所以互联网分布式架构设计高并发终极解决方案还是水平扩展。

水平扩展：只要增加服务器数量，就能线性扩充系统性能。水平扩展对系统架构设计是有要求的，如何在架构各层进行可水平扩展的设计，以及互联网公司架构各层常见的水平扩展实践，是本文重点讨论的内容。

python解决高并发的几种方式

1.HTML页面静态化

2.图片服务器分离（我使用的是fastdfs轻量级的分布式文件存储系统）

3.使用缓存（缓存存在于内存中读取快我的项目中使用redis作为缓存的数据库，redis是内存型数据作为存储缓存的数据库挺适合）

4.数据库集群、库表散列

5.使用负载均衡的方法（简单的配置可以用nginx来配置负载均衡，只需要设置 如下代码，即可实现简单的负载均衡

upstream djangoserver {

server192.168.72.49:8080;

server192.168.72.49:8081;

}

6.镜像

镜像是大型网站常采用的提高性能和数据安全性的方式，镜像的技术可以解决不同网络接入商和地域带来的用户访问速度差异，比如ChinaNet和EduNet之间的差异就促使了很多网站在教育网内搭建镜像站点，数据进行定时更新或者实时更新。在镜像的细节技术方面，这里不阐述太深，有很多专业的现成的解决架构和产品可选。也有廉价的通过软件实现的思路，比如Linux上的rsync等工具。

7.最新：CDN加速技术（此技术还在了解阶段，可自行去网上查找相关的资料）

三、常见的互联网分层架构

常见互联网分布式架构如上，分为：

（1）客户端层：典型调用方是浏览器browser或者手机应用APP

（2）反向代理层：系统入口，反向代理

（3）站点应用层：实现核心应用逻辑，返回html或者json

（4）服务层：如果实现了服务化，就有这一层

（5）数据-缓存层：缓存加速访问存储

（6）数据-数据库层：数据库固化数据存储

整个系统各层次的水平扩展，又分别是如何实施的呢？

四、分层水平扩展架构实践

反向代理层的水平扩展

反向代理层的水平扩展，是通过"DNS轮询”实现的：dns-server对于一个域名配置了多个解析ip，每次DNS解析请求来访问dns-server，会轮询返回这些ip。

当nginx成为瓶颈的时候，只要增加服务器数量，新增nginx服务的部署，增加一个外网ip，就能扩展反向代理层的性能，做到理论上的无限高并发。

站点层的水平扩展

站点层的水平扩展，是通过"nginx”实现的。通过修改nginx.conf，可以设置多个web后端。

当web后端成为瓶颈的时候，只要增加服务器数量，新增web服务的部署，在nginx配置中配置上新的web后端，就能扩展站点层的性能，做到理论上的无限高并发。

服务层的水平扩展

服务层的水平扩展，是通过"服务连接池”实现的。

站点层通过RPC-client调用下游的服务层RPC-server时，RPC-client中的连接池会建立与下游服务多个连接，当服务成为瓶颈的时候，只要增加服务器数量，新增服务部署，在RPC-client处建立新的下游服务连接，就能扩展服务层性能，做到理论上的无限高并发。如果需要优雅的进行服务层自动扩容，这里可能需要配置中心里服务自动发现功能的支持。

数据层的水平扩展

在数据量很大的情况下，数据层（缓存，数据库）涉及数据的水平扩展，将原本存储在一台服务器上的数据（缓存，数据库）水平拆分到不同服务器上去，以达到扩充系统性能的目的。

互联网数据层常见的水平拆分方式有这么几种，以数据库为例：

按照范围水平拆分

每一个数据服务，存储一定范围的数据，上图为例：

user0库，存储uid范围1-1kw

user1库，存储uid范围1kw-2kw

这个方案的好处是：

（1）规则简单，service只需判断一下uid范围就能路由到对应的存储服务；

（2）数据均衡性较好；

（3）比较容易扩展，可以随时加一个uid[2kw,3kw]的数据服务；

不足是：

请求的负载不一定均衡，一般来说，新注册的用户会比老用户更活跃，大range的服务请求压力会更大；

按照哈希水平拆分

每一个数据库，存储某个key值hash后的部分数据，上图为例：

user0库，存储偶数uid数据

user1库，存储奇数uid数据

这个方案的好处是：

（1）规则简单，service只需对uid进行hash能路由到对应的存储服务；

（2）数据均衡性较好；

（3）请求均匀性较好；

不足是：

不容易扩展，扩展一个数据服务，hash方法改变时候，可能需要进行数据迁移；

这里需要注意的是，通过水平拆分来扩充系统性能，与主从同步读写分离来扩充数据库性能的方式有本质的不同。

通过水平拆分扩展数据库性能：

（1）每个服务器上存储的数据量是总量的1/n，所以单机的性能也会有提升；

（2）n个服务器上的数据没有交集，那个服务器上数据的并集是数据的全集；

（3）数据水平拆分到了n个服务器上，理论上读性能扩充了n倍，写性能也扩充了n倍（其实远不止n倍，因为单机的数据量变为了原来的1/n）；

通过主从同步读写分离扩展数据库性能：

（1）每个服务器上存储的数据量是和总量相同；

（2）n个服务器上的数据都一样，都是全集；

（3）理论上读性能扩充了n倍，写仍然是单点，写性能不变；

缓存层的水平拆分和数据库层的水平拆分类似，也是以范围拆分和哈希拆分的方式居多，就不再展开。

五、总结

高并发（High Concurrency）是互联网分布式系统架构设计中必须考虑的因素之一，它通常是指，通过设计保证系统能够同时并行处理很多请求。

提高系统并发能力的方式，方法论上主要有两种：垂直扩展（Scale Up）与水平扩展（Scale  Out）。前者垂直扩展可以通过提升单机硬件性能，或者提升单机架构性能，来提高并发性，但单机性能总是有极限的，互联网分布式架构设计高并发终极解决方案还是后者：水平扩展。

互联网分层架构中，各层次水平扩展的实践又有所不同：

（1）反向代理层可以通过"DNS轮询”的方式来进行水平扩展；

（2）站点层可以通过nginx来进行水平扩展；

（3）服务层可以通过服务连接池来进行水平扩展；

（4）数据库可以按照数据范围，或者数据哈希的方式来进行水平扩展；

各层实施水平扩展后，能够通过增加服务器数量的方式来提升系统的性能，做到理论上的性能无限

   多线程模型，擦行间新的线程处理请求

   多进程模型，创建新的进程处理请求

   IO多路复用

   Select/poll/epoll区别

   Python如何实现IO多路复用

一.  IO操作
 凡是'在内存中存在的数据交换的操作'都可以认为是IO操作，如：

内存和磁盘的交互：read write
 内存和终端的交互：print input
 内存和网络的交互：recv send
 1.1 阻塞IO
 默认形态，效率很低的一种IO；常见的阻塞场景：

因为某种条件没有达到造成的阻塞，如：input accept recv
 处理IO事件的时间消耗较长带来的阻塞，如：文件的读写过程，网络数据的发送过程
 1.2 非阻塞IO
 通过修改IO事件的属性，使其变为非阻塞状态，以避免条件阻塞的情况。非阻塞IO往往和循环搭配使用，这样可以不断执行部分需要执行的代码，也不影响对阻塞事件的判断。以下示例通过s.setblocking(False)设置套接字为非阻塞套接字，并处理由此产生的BlockingIOError异常：





   -有哪些并发网络库

   Tornado和gevent和asyncio

## 7，操作系统

   常用Linux命令:

```
   -man   查询函数说明

   \- --help	函数帮助

   -tldr 		简化版man要pip 安装

   -tar		压缩解压缩

   -chown/chmod/chgrp		change权限，组

   -ls/rm/cd/cp/mv/touch/rename/ln(软连接和硬链接)

   -locate/find/grep	定位查找和搜索

   文件或者日志查看工具

   -vi /nanao

   -cat/head/tail  查看文件日志

   -more/less交互式查看文件

   进程操作命令

   -ps查看进程

   -kill杀死进程，执行原理

   -top/htop监控进程

   内存操作命令

   -free查看可用内存

   了解没一列的具体含义

   网络操作命令、

   -ifconfig查看网卡信息

   -Lsof/netstat查看端口信息

   -Ssh/scp远程登录复制，tcpdump抓包

   用户和组操作命令

   -useradd/usermod

   Groupadd/groupmod
```

​    shell编程

变量，

显式定义变量，变量名不加美元符号$,比如，name="hah"

使用定义过的变量，直接在变量前加$符号，花括号可不加

字符串

单引号里任何字符都会原样输出

单引号内不能出现单引号

双引号可以有变量，可以有转义字符

拼接字符串 

```
your_name="qinjx"
greeting="hello, "$your_name" !"
greeting_1="hello, ${your_name} !"
echo $greeting $greeting_1
```

获取字符串长度 

```
string="abcd"
echo ${#string} #输出 4
```

 提取子字符串 

```
string="alibaba is a great company"
echo ${string:1:4} #输出liba
```

查找子字符串 

```
string="alibaba is a great company"
echo `expr index "$string" is`
```

注意： 以上脚本中 "`" 是反引号，而不是单引号 "'"，

数组

用（）表示数组，用空格分割元素

```
数组名=(值1 值2 ... 值n)
```

读取数组 

读取数组元素值的一般格式是：

```
${数组名[下标]}
```

例如：

```
valuen=${array_name[n]}
```

使用@符号可以获取数组中的所有元素，例如：

```
echo ${array_name[@]}
```

获取数组的长度 

获取数组长度的方法与获取字符串长度的方法相同，例如：

```
# 取得数组元素的个数
length=${#array_name[@]}
# 或者
length=${#array_name[*]}
# 取得数组单个元素的长度
lengthn=${#array_name[n]}
```

#开头为注释

运算符，

expr 是一款表达式计算工具，使用它能完成表达式的求值操作。

例如，两个数相加(注意使用的是反引号 ` 而不是单引号 ')：

```
#!/bin/bash

val=`expr 2 + 2`
echo "两数之和为 : $val"
```

关系运算符只支持数字，不支持字符串，除非字符串的值是数字。

下表列出了常用的关系运算符，假定变量 a 为 10，变量 b 为 20：

| 运算符 | 说明                                                  | 举例                       |
| ------ | ----------------------------------------------------- | -------------------------- |
| -eq    | 检测两个数是否相等，相等返回 true。                   | [ $a -eq $b ] 返回 false。 |
| -ne    | 检测两个数是否不相等，不相等返回 true。               | [ $a -ne $b ] 返回 true。  |
| -gt    | 检测左边的数是否大于右边的，如果是，则返回 true。     | [ $a -gt $b ] 返回 false。 |
| -lt    | 检测左边的数是否小于右边的，如果是，则返回 true。     | [ $a -lt $b ] 返回 true。  |
| -ge    | 检测左边的数是否大于等于右边的，如果是，则返回 true。 | [ $a -ge $b ] 返回 false。 |
| -le    | 检测左边的数是否小于等于右边的，如果是，则返回 true。 | [ $a -le $b ] 返回 true    |

传递参数，

向脚本传递参数，脚本内获取参数的格式为：**$n**。**n** 代表一个数字，1 为执行脚本的第一个参数，2 为执行脚本的第二个参数

以下实例我们向脚本传递三个参数，并分别输出，其中 **$0** 为执行的文件名：

```
#!/bin/bash
# author:W3Cschool教程
# url:www.w3cschool.cn

echo "Shell 传递参数实例！";
echo "执行的文件名：$0";
echo "第一个参数为：$1";
echo "第二个参数为：$2";
echo "第三个参数为：$3";
```

为脚本设置可执行权限，并执行脚本，输出结果如下所示：

```
$ chmod +x test.sh 
$ ./test.sh 1 2 3
Shell 传递参数实例！
执行的文件名：test.sh
第一个参数为：1
第二个参数为：2
第三个参数为：3
```



命令，

echo，printf输出

 test 命令用于检查某个条件是否成立，它可以进行数值、字符和文件三个方面的测试

流程控制，

if else-if else 语法格式：



```
if condition1
then
    command1
elif condition2
then    
    command2
else
    commandN
fi
```

顺序输出字符串中的字符：

```
for str in 'This is a string'
do
    echo $str
done
```

while循环用于不断执行一系列命令，也用于从输入文件中读取数据；命令通常为测试条件。其格式为：

```
while condition
do
    command
done
```

until循环执行一系列命令直至条件为真时停止。

until循环与while循环在处理方式上刚好相反。

一般while循环优于until循环，但在某些时候—也只是极少数情况下，until循环更加有用。

until 语法格式:

```
until condition
do
    command
done
```

```
echo '输入 1 到 4 之间的数字:'
echo '你输入的数字为:'
read aNum
case $aNum in
    1)  echo '你选择了 1'
    ;;
    2)  echo '你选择了 2'
    ;;
    3)  echo '你选择了 3'
    ;;
    4)  echo '你选择了 4'
    ;;
    *)  echo '你没有输入 1 到 4 之间的数字'
    ;;
esac
```

在循环过程中，有时候需要在未达到循环结束条件时强制跳出循环，Shell使用两个命令来实现该功能：break和continue。



函数，

下面定义一个带有return语句的函数：

```
#!/bin/bash
funWithReturn(){
    echo "这个函数会对输入的两个数字进行相加运算..."
    echo "输入第一个数字: "
    read aNum
    echo "输入第二个数字: "
    read anotherNum
    echo "两个数字分别为 $aNum 和 $anotherNum !"
    return $(($aNum+$anotherNum))
}
funWithReturn
echo "输入的两个数字之和为 $? !"
```

输出类似下面：

```
这个函数会对输入的两个数字进行相加运算...
输入第一个数字: 
1
输入第二个数字: 
2
两个数字分别为 1 和 2 !
输入的两个数字之和为 3 !
```

函数返回值在调用该函数后通过 $? 来获得。

注意：所有函数在使用前必须定义。这意味着必须将函数放在脚本开始部分，直至shell解释器首次发现它时，才可以使用。调用函数仅使用其函数名即可

在Shell中，调用函数时可以向其传递参数。在函数体内部，通过 $n 的形式来获取参数的值，例如，$1表示第一个参数，$2表示第二个参数...

IO重定向

重定向命令列表如下：

| 命令            | 说明                                               |
| --------------- | -------------------------------------------------- |
| command > file  | 将输出重定向到 file。                              |
| command < file  | 将输入重定向到 file。                              |
| command >> file | 将输出以追加的方式重定向到 file。                  |
| n > file        | 将文件描述符为 n 的文件重定向到 file。             |
| n >> file       | 将文件描述符为 n 的文件以追加的方式重定向到 file。 |
| n >& m          | 将输出文件 m 和 n 合并。                           |
| n <& m          | 将输入文件 m 和 n 合并。                           |
| << tag          | 将开始标记 tag 和结束标记 tag 之间的内容作为输入。 |

文件包含

和其他语言一样，Shell 也可以包含外部脚本。这样可以很方便的封装一些公用的代码作为一个独立的文件。

Shell 文件包含的语法格式如下：

```
. filename   # 注意点号(.)和文件名中间有一空格

或

source filename
```

### 实例

创建两个 shell 脚本文件。

test1.sh 代码如下：

```
#!/bin/bash
# author:W3Cschool教程
# url:www.w3cschool.cn

url="http://www.w3cschool.cn"
```

test2.sh 代码如下：

```
#!/bin/bash
# author:W3Cschool教程
# url:www.w3cschool.cn

#使用 . 号来引用test1.sh 文件
. ./test1.sh

# 或者使用以下包含文件代码
# source ./test1.sh

echo "W3Cschool教程官网地址：$url"
```

接下来，我们为 test2.sh 添加可执行权限并执行：

```
$ chmod +x test2.sh 
$ ./test2.sh 
W3Cschool教程官网地址：http://www.w3cschool.cn
```

> **注：**被包含的文件 test1.sh 不需要可执行权限。

-git版本控制

https://blog.csdn.net/qq_21561501/article/details/122611578

## 8，进程/线程

   进程和线程的区别，应用层层面，不涉及太深的底层

   进程是对运行时程序的封装，是系统资源调度和分配的基本单位

   线程是进程的子任务，cpu调度和分配的基本单位，实现进程内并发

   一个进程可以包含多个线程，线程依赖进程存在，并共享进程内存

   -什么是线程安全?

   一个线程的修改被另一个线程访问覆盖掉了

   -Python哪些操作是线程安全的？

   一个操作可以在多线程环境中安全使用，或区域正确的结果

   线程安全的操作好比线程是顺序执行而不是并发执行的（i+=1）

   一般如果涉及到写操作需要考虑如何让多个线程安全访问数据

   -线程同步的方式

   互斥锁：通过互斥机制防止多个线程同事访问公共资源

   信号量：控制同一时刻多个线程访问同一个资源的线程数

   事件：通过通知的方式保持多个线程同步

   -进程间通信的方式

   IPC进程间传递信号或者数据

   管道/匿名管道/有名管道

   信号：比如用户使用ctrl + C产生终止信号

   消息队列

   共享内存用的少

   信号量

   套接字：最常用的方式，我们的web应用都是这种方式

   -Python中如何使用多线程

   Threading模块

   Threading.thread类用来创建线程

   Start（）方法启动线程

   可以用join（）等待线程结束

```
   Import threading

   Lock = threading.Lock()

   N = [0]

   Def foo()

   With lock:

   N[0] = n[0] + 1

   N[0] = n[0] +1

   Threads = []

   For i in range(5000):

   T = threading.Thread(target = foo)

   Threads.append(t)

   For t in threads:

   T. start()

   Print(n)
```

  -Python 中如何使用多进程

   Python中gil，可以用多进程实现CPU密集程序

   Multiprocessing多进程模块

   Multiprocessin.process类实现多进程

   一般用在CPU密集程序里，避免GIL的影响

   -斐波那契数列

 ```
   # 多进程
 
    Import multiprocessing
 
    Def fib(n):
 
    If n <= 1:
 
    Return 1
 
    Return fib(n-1) + fib(n-2)
 
    `		if  __name__ == ‘__main__’:
 
    `		jobs = []
 
    For i in range(10,20):
 
    P = multiprocessing.Process（target = fib, args=(i,)）
 
    Jobs.append(p)
 
    P.start()
 ```



​    

## 9，内存管理

   -什么是分页机制？

   逻辑地址和物理地址分离的内存分配管理方案，减少内存碎片

   程序的逻辑地址划分为固定大小的页page

   物理地址划分为同等大小的帧frame

   通过页表对应逻辑地址和物理地址

​    

   -什么分段机制

   分段是为了满足代码的一些逻辑需求

   数据共享，数据保护，动态链接等

   通过段表实现逻辑地址和物理地址的映射关系

   每个段内部是连续内存分配，段和段之间是离散分配的

   -分页和分段的区别

   页是出于内存利用率的角度，段是出于用户角度

   -什么是虚拟内存

   通过把一部分暂时不用的内存信息放到硬盘上

   局部性原理，程序运行时候只有部分必要的信息装入内存

   内存中暂时不需要的内容放到硬盘上

   系统似乎提供了比实际内存打的多容量，称为虚拟内存

   -什么是内存抖动（颠簸）

   本质是平凡的页调度行为

   频繁的页调度，进程不断产生缺页终端

   置换一个页，又不断再次需要这个页

   运行程序太多，页面替换策略不好，终止进程或者增加物理内存

   -Python的垃圾回收机制原理

   Python无需我们手动回收内存，

   引用计数为主，缺点：循环引用无法解决

   引入标记清楚和分代回收解决引用计数的问题

   引用计数为主+标记清楚和分代回收为辅

## 10，系统设计考点解析

### 什么是系统设计？主要用途

   -系统设计是一个定义系统架构，模块，接口和数据满足特定需求的过程，

从目标系统看的逻辑模型转换为物理模型

主要用途：

   -比如设计一个短网址服务，评论服务，feed流系统，抢红包系统

   -微服务架构很多系统被按照业务拆分，需要单独设计一个系统服务

   公司里提供一个供其他所有业务使用的一个短网址服务

​    

### 系统设计的知识点及具体实现

   -需要具备相关领域，算法的经验，有一定的架构设计能力

   -熟悉后端技术组件，比如消息队列，缓存，数据库，框架

   -具备文档撰写，流程图绘制，架构设计，编码实现等综合能力

#### 知识点1：系统设计怎么回答

   系统设计三大要素：

   -使用场景和限制条件

   -数据存储设计

   -算法模拟设计

#### 具体1：按照三个要素来回答：

   问面试官：什么场景和条件下使用

   设计数据存储系统

   设计算法相关模块

* 要素之一：场景和限制

   什么场景使用？有哪些条件

   -这个系统是在什么地方使用的？比如短网址系统提供给站内各种服务生成短网址

   -限制条件：用户估计有多少？至少能支撑多少用户（服务）？

   -估算并发qps:峰值qps是多少？平均qps是多少？

* 要素之二：数据存储设计

   数据库的选型

   -按需设计数据表，需要哪些字段，使用什么类型？数据增长规模

   -数据库选型：是否需要持久化？使用关系型还是Nosql?

   -如何优化？如何设计索引？是否可以使用缓存？

* 要素之三：算法模块设计

   算法解决问题的核心。程序=算法+数据结构，系统=服务+存储

   -需要哪些接口？接口如何设计？

   -使用什么算法或者模型

   不同实现方式之间的优劣对比，如何取舍？

####  知识点,2：:扩展，容错

* 用户多了，qps高了如何处理

  每秒数据库执行的查询量即为QPS，它是衡量MySQL数据库性能的一个主要指标，但此查询量不仅包括select、DML语句，还包括其它如set类的操作，如果set类操作占比比较大的话，它很可能会影响到数据库的性能；

  在做db基准测试的时候，qps,tps是衡量数据库性能的关键指标。QPS(Queryper second)每秒查询量,TPS(Transactionper second)每秒事务量。

  QPS：Queries / Seconds

  Queries 是系统状态值--总查询次数

  TPS：(Com_commit + Com_rollback) / Seconds

  mysql中没有直接的事务计数器，需要通过事务提交数和事务回滚数来计算。这是Mysql的两个重要性能指标

  异常现象

  生产一MySQL数据库日常平均QPS为2000左右，峰值3500，但业务新需求上线后，其QPS竟高达2.2万，是以前平均值的11倍，此种情况过于异常。询问业务侧是否上线大业务处理场景，业务侧反馈无，业务系统和以前几乎一样。异常分析

  既然业务侧不能提供有用的信息，那只能从数据库侧着手分析了。先从数据库监控平台查看：

  1、近1小时qps曲线

  近一小时内，数据库QPS都在2.2万以上

  2、近1小时数据库各类操作曲线

  从图上可以看出，每秒对数据库的set操作高达19547次，在QPS中占比高达99%。

  3、分析general log

  从监控平台仅能看到set操作占比最高，但却无法知道具体是什么set操作，基于此种情况，只好打开数据库的generallog，分析上图中set操作具体是什么。

  打开数据库的generallog，收集了7分钟的信息，并进行分析，分析结果如下：

  “SETautocommit=0”操作，7分钟内执行2143322次，每秒5103次

  “SETautocommit=1”操作，7分钟内执行2143331次，每秒次5103

  “set session transaction readwrite”操作，7分钟内执行1696531次，每秒4039次

  “set session transaction readonly”操作，7分钟内执行1696530次，每秒4039次

  以上4种操作每秒执行次数加起来就和数据库的QPS差不多了，因些可以肯定就是这4种SET操作过于频繁执行，才引起数据库的QPS过于异常。

  4、解决方案

  如果说以上4种set操作是为了对事务进行有效控制，按一个事务内包含一个SQL语句1:1的比例分配，那也应该有相应的select、update、delete、insert操作次数，但从实际的情况来看，每秒仅有291次insert、2次select操作，明显与事务控制操作次数不符。

  2，使用[Mysql](https://cloud.tencent.com/product/cdb?from=10680)中如果CPU在95%及以上，Qps突然增到2万以上，这时Mysql随时有死去风险。

  这时该怎么办？

  应急方法：

  第一: 先限制Innodb的并发处理.如果innodb_thread_concurrency = 0 可以先改成 16或64 看机器压力,如果 非常大,先改成16让机器的压力下来,然后慢慢增大,适应自已的业务. 

  set global innodb_thread_concurrency=16;

  第二: 对于连接数已经超过600的情况,可以适当的限制一下连接数,宁可让前端报一下错,也别让DB挂. 只要DB活着总是可以用来加载一下数据,慢慢的DB压力也会降下来的. 限制单用户连接数在300以下

  set global max_user_connections=300;

  关闭 innodb_stats_on_metadata防止对读取information_schema时造成大量读取磁盘进行信息统计 （有些监控程序从这里抓取数据，会终止）

  set global innodb_stats_on_metadata=0;

  ================================

  应急处置完毕后进行RootCause分析：

  思路：

  1、确定高负载的类型 htop，dstat命令看负载高是CPU还是IO

  2、监控具体的sql语句，是insert update 还是 delete导致高负载

  3、检查mysql慢日志

  打开慢查询方法：vi  my.cnf,在[mysqld]如下几行:

  log_slow_queries = /data/slow.log #慢查询日志路径

  long_query_time = 1 #记录SQL查询超过1s的语句

  log-queries-not-using-indexes = 1 #记录没有使用索引的sql

  4、检查硬件问题

  dstat

  看具体哪个用户哪个进程占用了相关系统资源，当前CPU、内存谁在使用

  查看系统到底在干什么

   mysql> show full processlist;

  检查mysql配置参数是否有问题，引起大量的IO或者高CPU操作

  innodb_flush_log_at_trx_commit、innodb_buffer_pool_size 、key_buffer_size 等重要参数

  \1. mysql> show variables like '%innodb%';

  查看当前事务，内存使用情况

  \1. mysql> show engine innodb status \G

  最后通过zabbix或者cacti等监控来查看IO、CPU、MEMORY、磁盘等是否有异常

  通过以上基本就可以把问题找出来了

* 数据存储多了不够存了如何处理

  ### 怎样最简单的扩展容量

  1. 什么是分区表

  - 将InnoDB的一个表分为多个表
  - server层依然看做一个表
  - ![image-20221014184052078](C:\Users\stormblinger\AppData\Roaming\Typora\typora-user-images\image-20221014184052078.png)
  - 指定分区位置
  - ![image-20221014184121116](C:\Users\stormblinger\AppData\Roaming\Typora\typora-user-images\image-20221014184121116.png)
  - 分区方式
  - 范围分区
  - hash分区
  - list分区

  1. 分区表的优势

  - 降低B+树的层级，搜索加速
  - 将一个数据表物理上分为多个文件，方便处理

  1. 分区表的缺陷

  - 第一次需要访问所有分区
  - 公用MDL锁
  - 分区之后，所有的分区依然位与同一节点

  1. 总结：

  - 分区表可以优化单节点容量，增强分区之间隔离

  - 第一次访问需要打开所有ibd，可能达到上限

  - 可以通过存储在不同的磁盘上，提高容量

  - 分表

  - 垂直：按照字段分表，一般分为冷热

  - 水平：按照行分表，常用范围、hash切分

  - 水平分表类似于分区表，不过server层也分了

  - 分库

  - 垂直：将数据表分散在多个数据库或者多个节点中

  - 水平：将数据表水平拆分，每个数据库结构相同

  - 分区表

    1. 分区表

    - 分区表可以优化单节点性能，分区之间隔离
    - 第一次访问需要打开所有ibd, 可能达到上限
    - 可以通过存储在不同的磁盘上，提高容量

    1. 分库分表的意义

    - 分库分表可以提升数据库性能
    - 分库分表使得数据的使用方法更加复杂，数据丢失的可能性增加
    - 使用分库分表的中间件更方便的客户端调用

    1. dble（double）

    - dble是一个高性能、易用的分库分表中间件
    - dble基于MyCat, 并做了改进
    - dble在功能上以水平分表为主
    - 怎样提高分库分表架构的可靠性

    1. 复制与分库分表架构的结合
        ![img](https://img2022.cnblogs.com/blog/1696528/202204/1696528-20220405161828824-566906490.png)
        [dble分库分表+主从架构](https://github.com/actiontech/dble/tree/master/docker-images/rwSplit)
    2. 分库分表的查询性能优化

    - 所有的sql语句尽量带有拆分字段，因为dble的数据表是按照拆分字段进行拆分的
    - 尽量减少节点间数据的交互，将sql直接转发
    - 尽量将连接键作为拆分字段

* 故障如何处理？单点失败，多点失败，雪崩问题

* 

  #### 知识点3：短网址系统设计与实现

   如何设计与实现一个短网址系统？

   什么是短网址系统？包含哪些功能（接口）

   短网址系统的存储设计？需要存储哪些字段

   如何设计算法生成短网址

####  具体3：  什么是短网址系统

   TinyUrl Service

   -把一个长网址转成短网址的服务

   -比如 https://bitly.com/

   -转换之后网址的后缀不超过7位（大小写字母，数字）

​    场景和限制

   使用场景：提供短网址服务为公司其他各业务服务

   -功能：一个长网址转成短网址并存储；根据短网址还原长URL

   -要求短网址的后缀不超过7位（大小写字母和数字）

   -预估峰值插入请求数量级：数百；查询请求数量级：数千

​      数据存储设计

   根据需求设计数据存储方式

   -使用MySQL即可满足

   -需要的字段有哪些

   -如何根据查询设计索引

​     算法实现设计

   短网址生成算法有哪些？对比优缺点

   -两个API：long2short_url。Short2long_url

   -常用算法：hash算法截取，自增序列算法

   -对比多种算法，我们采取自增序列算法实现

   -短网址系统设计

   MySQL数据表  id  token(索引)  url（原网址）  created_at

   短网址生成算法  CHARS = “abcdefghijklmnopqrstuvwxyzABCDEFGHIJKLMNOPQRSTUVWXYZ0123456789”

   1，md5摘要算法，取前7个字符，但是容易冲突，在高并发场景下并不友好

```
  Import hashlib

   Url = “”

   Hashlib.md5(url.encode())

   Hashlib.md5(url.encode()).hexdigest()

   Len(Hashlib.md5(url.encode()).hexdigest()

   ) = 32  得到一个32位的定长字符串
```

2，字符集类似于62进制的数字，二进制 0,1

   十六进制 0-9 a-f

   十进制 ->62进制

   -进制转换

   不断取余，倒叙输出

   Bin（）

   小的flask短网址实现

   编码实现

   -使用flask框架演示本系统实现

   -代码里实现了短网址生成算法

   -数据库使用MySQL

   -计数器使用Redis

   回答重点

   -最好图文并茂

   -遵守三个要素来回答

   -包含数据表的设计，API的设计，算法的设计

   -图文并茂，有数据表，接口定义，流程图 

####  知识点4：如何设计一个秒杀系统

   难点：如何应对高并发的用户请求

   -什么是秒杀系统？你有没有使用过？

   -如何根据我们提到的三个要素来设计秒杀系统

   -秒杀系统涉及到哪些后端组件

 #### 具体4：秒杀系统

场景：现场要卖100件商品，然后我们根据以往这样秒杀活动的数据经验来看，目测抢这100件商品的人足足有10万人。服务器顶不住，有可能会DB直接挂

问题：

高并发，秒杀的特点就是时间极短，瞬间用户量大

缓存雪崩，缓存击穿，缓存穿透

超卖，恶意请求，链接暴露，数据库每秒上万甚至十几万的QPS

解决办法：服务单一职责，给秒杀单独也开个服务，把秒杀的代码业务逻辑放一起，单独给他建立一个数据库，现在的互联网架构部署都是分库的，一样的就是订单服务对应订单库，秒杀我们也给他建立自己的秒杀库，单一职责的好处就是就算秒杀没抗住，秒杀库崩了，服务挂了，也不会影响到其他的服务

秒杀链接加盐：把URL动态化，用MD5之类的加密算法加密随机的字符串去做URL，通过前端代码获取URL后台校验才能通过

Redis集群：单机的Redis顶不住，那就做Redis集群，主从同步，读写分离，再搞点哨兵，开启持久化直接高可用

Nginx：做负载均衡，多租点流量机

恶意请求拦截：防止单个用户请求次数过多

资源静态化：把前端分离的东西提前放入CDN服务器

按钮控制：秒杀前，按钮置灰，时间到了，才能点击，点击之后也得置灰几秒；

限流：前端限流，按钮不会让你一直点，点击一下或者两下然后几秒后才可以继续点击；后端限流，前端秒杀结束，后端直接关闭后续无效请求

库存预热：提前把商品的库存加载到Redis中去，让整个流程都在Redis里面去做，等秒杀结束了，再异步的去修改库存

限流&降级&熔断&隔离；削峰填谷

#### 知识点5：如何设计一个feed流系统

什么事feed流？

feed流事朋友圈，微博，抖音的一种呈现推荐内容给用户，并能够持续更新的方式

feed流三个显著的特点：

消息的可靠性高，无论关注或删除，用户能看到的信息是不能出错的

多账户的互动模式：可以由好友，热门，系统推荐等内容

对广告更友好：广告是基于推荐的方式展示给用户，

feed流系统的技术特点：

feed流：基于feed+流的概念组合，

timeline：feed流的一种类型，意为时间线，按发布的时间顺序排序，先发布的先看到，后发布的排列在最顶端

rank：推荐系统使用的排序权重，一般讲用户可能喜欢这条消息的权重越高，rank越靠前

收件箱，用来存储消费，最终接收到的一个feed流列表

feed流的元数据：把关键信息提取出来，比如一个视频，我们只会存储他的ID，类别，格式，长度，大小等

#### 具体5：feed流系统

feed流生产的过程：

首先，Feed流系统的用户或者机构产生内容，系统将内容收录到内容库中；
其次，系统根据已有的业务规则，将新增的内容，提交到分发中心；
最后，分发中心根据要投递的名单列表，向对应的账号收件箱中，插入Feed的索引信息。

feed流获取的过程：

从账号收件箱中获取增量Feed信息；
根据时间戳、好友关系、自定义等条件，过滤要显示的数据，并进行排序；
用户消费收件箱里的信息内容。

feed流实现的三种方式：

拉模式：发布者的内容仅仅写入自己的发件箱里，当用户访问自己的收件箱时，首先遍历这个用户的关系列表，然后再从关系列表的发件箱里读取数据，最后按照规则进行过滤和排序，这种方式的优点是写数据快，但读数据就要慢很多，适合于大V模式；
推模式：发布者的内容不仅写入自己的发件箱里，通过批处理任务，还需要写到对应关系人的收件箱里，当用户访问自己的收件箱时，直接从自己的收件箱里读取数据，最后按照规则进行过滤和排序，这种方式的优点是写数据很慢，但读数据就要快很多，适合于关系社交

推拉结合模式：大V和普通用户分开维护

feed流系统的架构设计：

Feed流本质上是一个数据流，是将 “N个发布者的信息单元” 通过 “关注关系” 传送给 “M个接收者

这层抽象的概念，总结了我们设计Feed流的两个核心：即存储系统、推送系统

Feed流系统虽然数据量级很大、对数据的可靠性要求很高，但它有一个很显著的特点，就是“关系极其简单”，这对于我们的技术选型提供了实现的思路。针对这种情况，我们可以设计如下的三种解决方案：

    Mysql分库分表方案，非常稳定可靠，如果数据量级不大，采用这种方案能够有效的降低开发时间及运维时间，由于发展历史很长，技术也很成熟；
    NoSQL方案，典型的代表是HBase，key-value型的数据库天然的适合二元关系场景，能够存储海量的数据，由于发展历史也很长了，因此可靠性是有保障的，但HBase的问题在于需要一定的运维成本；
    商业方案，TableStore(表格存储)是阿里云2013年推出的一款分布式数据库，存储账号关系是一个比较好的选择。
-----------------------------------
推送系统目的是解决消息的时效性。Feed流系统有一个很显著的特点，即读写是非常不平衡的，绝大多数情况下，用户都在“读”，而内容只有少量的人来“写”。因此，推送系统会面临如下几个方面的压力：

    TPS/QPS是千万级的；
    读写操作对于延迟非常敏感；
    消息严格意义上是不能丢失的；
    对于主键自增有很强烈的诉求

Feed流系统的广告主要设计为四个部分：流量端（内容生产）、投放端（广告主投放设置）、检索端（广告策略设定）、展示端（广告数据的展示）

-Redis-incr计数器

代码1；

```
import redis

class CountRedis(object):
    def __init__(self, host="localhost", port=6379, password=None, db=0):
        """
        :param host: 主机ip
        :param port: 运行端口
        :param password: 密码
        :param db: 存储到那个数据库(一共有16个)
        """
        redis_info = {
            "host": host,
            "port": port,
            "password": password,
            "db": db
        }
        self.con = redis.Redis(**redis_info)

    def redis_count(self, key, amount=1):
        """redis 计数器"""
        try:
            count = self.con.incr(key, amount)
        except Exception as e:
            raise e
        finally:
            self.con.close()
        return count

```

​                                            **incr( redis**                                        

​                                                                           

Redis Incr命令

Redis Incr 命令将 key 中储存的数字值增一。

如果 key 不存在，那么 key 的值会先被初始化为 0 ，然后再执行 INCR 操作。

如果值包含错误的类型，或字符串类型的值不能表示为数字，那么返回一个错误。

本操作的值限制在 64 位(bit)有符号数字表示之内。

语法

redis Incr 命令基本语法如下：

redis127.0.0.1:6379>INCR KEY_NAME

可用版本

\>= 1.0.0

返回值

执行 INCR 命令之后 key 的值。

实例

redis> SET page_view 20

OK

redis> INCR page_view

(integer) 21

redis> GET page_view  # 数字值在 Redis 中以字符串的形式保存

"21"

序号

相关信息

2

GET key

获取指定 key 的值

4

GETSET key value

将给定 key 的值设为 value ，并返回 key 的旧值(old value)

5

GETBIT key offset

对 key 所储存的字符串值，获取指定偏移量上的位(bit)

6

MGET key1 [key2]

获取所有(一个或多个)给定 key 的值

7

SETBIT key offset value

对 key 所储存的字符串值，设置或清除指定偏移量上的位(bit)

8

SETEX key seconds value

将值 value 关联到 key ，并将 key 的过期时间设为 seconds (以秒为单位)

9

SETNX key value

只有在 key 不存在时设置 key 的值

10

SETRANGE key offset value

用 value 参数覆写给定 key 所储存的字符串值，从偏移量 offset 开始

11

STRLEN key

返回 key 所储存的字符串值的长度

12

MSET key value

同时设置一个或多个 key-value 对

13

MSETNX key value

同时设置一个或多个 key-value 对，当且仅当所有给定 key 都不存在

14

PSETEX key milliseconds value

这个命令和 SETEX 命令相似，但它以毫秒为单位设置 key 的生存时间，而不是像 SETEX 命令那样，以秒为单位

15

INCRBY key increment

将 key 所储存的值加上给定的增量值(increment)

16

INCRBYFLOAT key increment

将 key 所储存的值加上给定的浮点增量值(increment)

17

DECR key

将 key 中储存的数字值减一

18

DECRBY key decrement

key 所储存的值减去给定的减量值(decrement)

19

APPEND key value

如果 key 已经存在并且是一个字符串， APPEND 命令将 value 追加到 key 原来的值的末尾

#### 知识点6：熟悉后端技术组件，比如消息队列，缓存，数据库，框架









